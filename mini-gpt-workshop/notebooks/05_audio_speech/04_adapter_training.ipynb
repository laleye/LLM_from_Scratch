{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 04 — Adaptateurs Bottleneck pour les Langues Africaines\n",
    "\n",
    "## MMS : Massively Multilingual Speech (Pratap et al., 2023)\n",
    "\n",
    "**Problème** : Les modèles ASR performants (Whisper, wav2vec 2.0) sont entraînés principalement\n",
    "sur des langues à haute ressource (anglais, français, espagnol). Les langues africaines\n",
    "(Swahili, Wolof, Yoruba, Zoulou...) disposent de très peu de données étiquetées.\n",
    "\n",
    "**Solution MMS** : Utiliser des **adaptateurs bottleneck** — de petits modules insérés dans\n",
    "un modèle pré-entraîné gelé. Seuls les paramètres des adaptateurs sont entraînés (~2-5% du total).\n",
    "\n",
    "```\n",
    "Modèle wav2vec 2.0 pré-entraîné (GELÉ)\n",
    "    ┌─────────────────────────────────┐\n",
    "    │  Attention Multi-Têtes (gelée)  │\n",
    "    │         ↓                       │\n",
    "    │  ┌─ Adapter ─┐  ← ENTRAÎNABLE  │\n",
    "    │  │ Down (d→r) │                 │\n",
    "    │  │ ReLU       │                 │\n",
    "    │  │ Up   (r→d) │                 │\n",
    "    │  │ + Résiduel │                 │\n",
    "    │  └────────────┘                 │\n",
    "    │         ↓                       │\n",
    "    │  Feed-Forward (gelé)            │\n",
    "    │         ↓                       │\n",
    "    │  ┌─ Adapter ─┐  ← ENTRAÎNABLE  │\n",
    "    │  │ Down (d→r) │                 │\n",
    "    │  │ ReLU       │                 │\n",
    "    │  │ Up   (r→d) │                 │\n",
    "    │  │ + Résiduel │                 │\n",
    "    │  └────────────┘                 │\n",
    "    └─────────────────────────────────┘\n",
    "```\n",
    "\n",
    "Ce notebook implémente :\n",
    "1. L'adaptateur bottleneck (from scratch + PyTorch)\n",
    "2. L'insertion d'adaptateurs dans un Transformer gelé\n",
    "3. Une boucle d'entraînement simplifiée avec CTC loss\n",
    "4. L'évaluation avant/après adaptation avec WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = str(Path(\"../../src\").resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — IMPLÉMENTATION FROM SCRATCH : Adaptateur Bottleneck (NumPy)\n",
    "\n",
    "L'adaptateur bottleneck est un petit réseau avec 3 étapes :\n",
    "\n",
    "1. **Down-projection** : `h = x @ W_down` — réduit la dimension de `d_model` à `bottleneck_dim`\n",
    "2. **Non-linéarité** : `h = relu(h)` — ajoute de la capacité non-linéaire\n",
    "3. **Up-projection** : `h = h @ W_up` — restaure la dimension originale\n",
    "4. **Connexion résiduelle** : `output = x + h` — garantit que la shape est préservée\n",
    "\n",
    "$$\\text{Adapter}(x) = x + \\text{ReLU}(x W_{\\text{down}}) W_{\\text{up}}$$\n",
    "\n",
    "Avec $W_{\\text{down}} \\in \\mathbb{R}^{d \\times r}$ et $W_{\\text{up}} \\in \\mathbb{R}^{r \\times d}$, où $r \\ll d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.adapter import adapter_from_scratch\n",
    "\n",
    "# Paramètres\n",
    "d_model = 768       # Dimension du Transformer (wav2vec 2.0 base)\n",
    "bottleneck_dim = 64  # Dimension réduite de l'adaptateur\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "\n",
    "# Créer des données d'entrée aléatoires\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(batch_size, seq_len, d_model).astype(np.float32)\n",
    "\n",
    "# Initialiser les poids de l'adaptateur (Xavier)\n",
    "scale_down = np.sqrt(2.0 / (d_model + bottleneck_dim))\n",
    "scale_up = np.sqrt(2.0 / (bottleneck_dim + d_model))\n",
    "W_down = np.random.randn(d_model, bottleneck_dim).astype(np.float32) * scale_down\n",
    "W_up = np.random.randn(bottleneck_dim, d_model).astype(np.float32) * scale_up\n",
    "\n",
    "print(f\"Input shape:        {x.shape}\")\n",
    "print(f\"W_down shape:       {W_down.shape}  (d_model → bottleneck_dim)\")\n",
    "print(f\"W_up shape:         {W_up.shape}  (bottleneck_dim → d_model)\")\n",
    "\n",
    "# Étape par étape\n",
    "h = np.matmul(x, W_down)\n",
    "print(f\"\\nAprès down-projection: {h.shape}  (dimension réduite à {bottleneck_dim})\")\n",
    "\n",
    "h = np.maximum(0, h)  # ReLU\n",
    "print(f\"Après ReLU:            {h.shape}\")\n",
    "\n",
    "h = np.matmul(h, W_up)\n",
    "print(f\"Après up-projection:   {h.shape}  (dimension restaurée à {d_model})\")\n",
    "\n",
    "output = x + h  # Connexion résiduelle\n",
    "print(f\"Après résiduel:        {output.shape}\")\n",
    "\n",
    "# Vérification avec la fonction du module\n",
    "output_module = adapter_from_scratch(x, W_down, W_up)\n",
    "assert np.allclose(output, output_module, atol=1e-6)\n",
    "print(f\"\\n✓ Shape préservée : input {x.shape} → output {output.shape}\")\n",
    "print(f\"✓ Nombre de paramètres adaptateur : {W_down.size + W_up.size:,}\")\n",
    "print(f\"  vs paramètres d'une couche Transformer : ~{4 * d_model * d_model:,}\")\n",
    "print(f\"  Ratio : {(W_down.size + W_up.size) / (4 * d_model * d_model) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — IMPLÉMENTATION PYTORCH : BottleneckAdapter (nn.Module)\n",
    "\n",
    "L'implémentation PyTorch utilise `nn.Linear` pour les projections et `nn.ReLU` pour l'activation.\n",
    "La connexion résiduelle est identique à la version from scratch.\n",
    "\n",
    "**Astuce d'initialisation** : Les poids de `up_proj` sont initialisés à zéro pour que\n",
    "l'adaptateur commence comme une fonction identité (output ≈ input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.adapter import BottleneckAdapter\n",
    "\n",
    "# Créer l'adaptateur PyTorch\n",
    "adapter = BottleneckAdapter(d_model=768, bottleneck_dim=64)\n",
    "adapter.eval()\n",
    "\n",
    "# Afficher l'architecture\n",
    "print(\"Architecture BottleneckAdapter :\")\n",
    "print(adapter)\n",
    "\n",
    "# Compter les paramètres\n",
    "num_params = sum(p.numel() for p in adapter.parameters())\n",
    "print(f\"\\nNombre de paramètres : {num_params:,}\")\n",
    "for name, p in adapter.named_parameters():\n",
    "    print(f\"  {name}: {p.shape} ({p.numel():,} params)\")\n",
    "\n",
    "# Test forward pass\n",
    "x_torch = torch.randn(2, 10, 768)\n",
    "with torch.no_grad():\n",
    "    out_torch = adapter(x_torch)\n",
    "\n",
    "print(f\"\\nInput shape:  {x_torch.shape}\")\n",
    "print(f\"Output shape: {out_torch.shape}\")\n",
    "print(f\"✓ Shape préservée : {x_torch.shape == out_torch.shape}\")\n",
    "\n",
    "# Vérifier que l'initialisation near-zero donne output ≈ input\n",
    "diff = torch.abs(out_torch - x_torch).mean().item()\n",
    "print(f\"\\nDifférence moyenne input/output (init near-zero) : {diff:.6f}\")\n",
    "print(\"→ L'adaptateur commence comme une quasi-identité !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Insertion d'adaptateurs dans un Transformer gelé\n",
    "\n",
    "La fonction `insert_adapters` :\n",
    "1. **Gèle** tous les paramètres du modèle de base (`requires_grad = False`)\n",
    "2. **Insère** un adaptateur après chaque couche d'attention et chaque FFN\n",
    "3. Seuls les paramètres des adaptateurs restent entraînables\n",
    "\n",
    "Cela permet d'adapter un modèle pré-entraîné à une nouvelle langue avec très peu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.adapter import insert_adapters\n",
    "from architecture.transformer_block import TransformerBlock\n",
    "\n",
    "# Construire un modèle Transformer simple (simule wav2vec 2.0 Context Network)\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, d_model=256, num_heads=4, d_ff=1024, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "d_model = 256\n",
    "model = SimpleTransformer(d_model=d_model, num_heads=4, d_ff=1024, num_layers=4)\n",
    "\n",
    "# Compter les paramètres AVANT insertion\n",
    "total_before = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Paramètres AVANT insertion : {total_before:,}\")\n",
    "print(f\"Tous entraînables : {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Insérer les adaptateurs\n",
    "bottleneck_dim = 32\n",
    "model = insert_adapters(model, bottleneck_dim=bottleneck_dim)\n",
    "\n",
    "# Compter les paramètres APRÈS insertion\n",
    "total_after = sum(p.numel() for p in model.parameters())\n",
    "frozen = sum(p.numel() for p in model.parameters() if not p.requires_grad)\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nParamètres APRÈS insertion :\")\n",
    "print(f\"  Total :       {total_after:,}\")\n",
    "print(f\"  Gelés :       {frozen:,} ({frozen/total_after*100:.1f}%)\")\n",
    "print(f\"  Entraînables : {trainable:,} ({trainable/total_after*100:.1f}%)\")\n",
    "\n",
    "# Lister les paramètres entraînables\n",
    "print(f\"\\nParamètres entraînables (adaptateurs) :\")\n",
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        print(f\"  {name}: {p.shape}\")\n",
    "\n",
    "# Vérifier que le forward pass fonctionne\n",
    "x = torch.randn(2, 10, d_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(x)\n",
    "print(f\"\\n✓ Forward pass : {x.shape} → {out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — Boucle d'entraînement simplifiée avec CTC Loss\n",
    "\n",
    "**CTC (Connectionist Temporal Classification)** est la fonction de perte standard pour l'ASR.\n",
    "Elle permet l'alignement automatique entre la séquence audio et la séquence de caractères,\n",
    "sans avoir besoin d'un alignement temporel explicite.\n",
    "\n",
    "Ici, nous simulons un entraînement simplifié :\n",
    "- Entrée : séquences aléatoires (simulent les sorties du Feature Encoder)\n",
    "- Cible : séquences de caractères aléatoires\n",
    "- Seuls les paramètres des adaptateurs sont mis à jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recréer un modèle frais avec adaptateurs pour l'entraînement\n",
    "d_model = 256\n",
    "vocab_size = 30  # Taille du vocabulaire (caractères)\n",
    "\n",
    "class ASRModelWithAdapters(nn.Module):\n",
    "    \"\"\"Modèle ASR simplifié avec Transformer + adaptateurs + projection CTC.\"\"\"\n",
    "    def __init__(self, d_model, vocab_size, num_heads=4, d_ff=1024, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.transformer = SimpleTransformer(d_model, num_heads, d_ff, num_layers)\n",
    "        self.ctc_proj = nn.Linear(d_model, vocab_size)  # Projection vers le vocabulaire\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.transformer(x)  # (batch, T, d_model)\n",
    "        logits = self.ctc_proj(h)  # (batch, T, vocab_size)\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "# Créer le modèle et insérer les adaptateurs\n",
    "asr_model = ASRModelWithAdapters(d_model, vocab_size)\n",
    "asr_model = insert_adapters(asr_model, bottleneck_dim=32)\n",
    "\n",
    "# Rendre la projection CTC entraînable aussi\n",
    "for p in asr_model.ctc_proj.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "trainable_params = [p for p in asr_model.parameters() if p.requires_grad]\n",
    "print(f\"Paramètres entraînables : {sum(p.numel() for p in trainable_params):,}\")\n",
    "\n",
    "# Optimiseur (seulement les paramètres entraînables)\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-3)\n",
    "ctc_loss_fn = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "\n",
    "# Données synthétiques pour la démonstration\n",
    "torch.manual_seed(42)\n",
    "batch_size = 4\n",
    "T_input = 50   # Longueur séquence d'entrée (pas temporels audio)\n",
    "T_target = 10  # Longueur séquence cible (caractères)\n",
    "\n",
    "# Boucle d'entraînement\n",
    "num_epochs = 20\n",
    "losses = []\n",
    "\n",
    "asr_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Générer des données synthétiques\n",
    "    x = torch.randn(batch_size, T_input, d_model)\n",
    "    targets = torch.randint(1, vocab_size, (batch_size, T_target))  # Éviter le blank (0)\n",
    "    input_lengths = torch.full((batch_size,), T_input, dtype=torch.long)\n",
    "    target_lengths = torch.full((batch_size,), T_target, dtype=torch.long)\n",
    "    \n",
    "    # Forward pass\n",
    "    log_probs = asr_model(x)  # (batch, T, vocab_size)\n",
    "    log_probs = log_probs.transpose(0, 1)  # CTC attend (T, batch, vocab_size)\n",
    "    \n",
    "    # Calcul de la perte CTC\n",
    "    loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{num_epochs} — CTC Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Visualiser la courbe de perte\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses, 'b-', linewidth=2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('CTC Loss')\n",
    "plt.title('Entraînement des adaptateurs — Courbe de perte')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPerte initiale : {losses[0]:.4f}\")\n",
    "print(f\"Perte finale :   {losses[-1]:.4f}\")\n",
    "print(f\"Réduction :      {(1 - losses[-1]/losses[0])*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — Évaluation avant/après adaptation avec WER\n",
    "\n",
    "Pour évaluer l'impact de l'adaptation, nous comparons le WER (Word Error Rate)\n",
    "avant et après l'entraînement des adaptateurs.\n",
    "\n",
    "Ici, nous simulons cette comparaison avec des transcriptions synthétiques\n",
    "pour illustrer le concept. En pratique, on utiliserait un vrai dataset\n",
    "en langue africaine (ex: FLEURS, CommonVoice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.metrics import compute_wer, compute_cer\n",
    "\n",
    "# Simulation : transcriptions avant/après adaptation\n",
    "# En pratique, ces transcriptions viendraient du modèle ASR sur un dataset réel\n",
    "\n",
    "references = [\n",
    "    \"habari yako leo\",           # Swahili : \"Comment vas-tu aujourd'hui\"\n",
    "    \"ninafuraha kukuona\",         # Swahili : \"Je suis content de te voir\"\n",
    "    \"watoto wanacheza uwanjani\",  # Swahili : \"Les enfants jouent dans le terrain\"\n",
    "    \"chakula kiko tayari\",        # Swahili : \"La nourriture est prête\"\n",
    "]\n",
    "\n",
    "# Hypothèses AVANT adaptation (erreurs fréquentes)\n",
    "hyp_before = [\n",
    "    \"habari yako le\",             # 'leo' → 'le' (troncation)\n",
    "    \"nina furaha ku kuona\",       # Segmentation incorrecte\n",
    "    \"watoto wana cheza uwanja\",   # Segmentation + troncation\n",
    "    \"chakula ki ko tayari\",       # Segmentation incorrecte\n",
    "]\n",
    "\n",
    "# Hypothèses APRÈS adaptation (améliorées)\n",
    "hyp_after = [\n",
    "    \"habari yako leo\",            # Correct !\n",
    "    \"ninafuraha kukuona\",         # Correct !\n",
    "    \"watoto wanacheza uwanjani\",  # Correct !\n",
    "    \"chakula kiko tayari\",        # Correct !\n",
    "]\n",
    "\n",
    "print(\"Évaluation WER et CER — Avant vs Après adaptation\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "wer_before_list, wer_after_list = [], []\n",
    "cer_before_list, cer_after_list = [], []\n",
    "\n",
    "for i, (ref, hb, ha) in enumerate(zip(references, hyp_before, hyp_after)):\n",
    "    wb = compute_wer(ref, hb)\n",
    "    wa = compute_wer(ref, ha)\n",
    "    cb = compute_cer(ref, hb)\n",
    "    ca = compute_cer(ref, ha)\n",
    "    wer_before_list.append(wb)\n",
    "    wer_after_list.append(wa)\n",
    "    cer_before_list.append(cb)\n",
    "    cer_after_list.append(ca)\n",
    "    print(f\"\\nPhrase {i+1}: \\\"{ref}\\\"\")\n",
    "    print(f\"  Avant : \\\"{hb}\\\" → WER={wb:.2f}, CER={cb:.2f}\")\n",
    "    print(f\"  Après : \\\"{ha}\\\" → WER={wa:.2f}, CER={ca:.2f}\")\n",
    "\n",
    "avg_wer_before = np.mean(wer_before_list)\n",
    "avg_wer_after = np.mean(wer_after_list)\n",
    "avg_cer_before = np.mean(cer_before_list)\n",
    "avg_cer_after = np.mean(cer_after_list)\n",
    "\n",
    "print(f\"\\n{'=' * 65}\")\n",
    "print(f\"WER moyen — Avant : {avg_wer_before:.2f} → Après : {avg_wer_after:.2f}\")\n",
    "print(f\"CER moyen — Avant : {avg_cer_before:.2f} → Après : {avg_cer_after:.2f}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "x_pos = np.arange(len(references))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x_pos - width/2, wer_before_list, width, label='Avant', color='#e74c3c', alpha=0.8)\n",
    "axes[0].bar(x_pos + width/2, wer_after_list, width, label='Après', color='#2ecc71', alpha=0.8)\n",
    "axes[0].set_xlabel('Phrase')\n",
    "axes[0].set_ylabel('WER')\n",
    "axes[0].set_title('Word Error Rate — Avant vs Après adaptation')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels([f'P{i+1}' for i in range(len(references))])\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(x_pos - width/2, cer_before_list, width, label='Avant', color='#e74c3c', alpha=0.8)\n",
    "axes[1].bar(x_pos + width/2, cer_after_list, width, label='Après', color='#2ecc71', alpha=0.8)\n",
    "axes[1].set_xlabel('Phrase')\n",
    "axes[1].set_ylabel('CER')\n",
    "axes[1].set_title('Character Error Rate — Avant vs Après adaptation')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels([f'P{i+1}' for i in range(len(references))])\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — Discussion : XLS-R et perspectives pour les langues africaines\n",
    "\n",
    "### XLS-R (Conneau et al., 2020)\n",
    "\n",
    "XLS-R est l'extension cross-lingue de wav2vec 2.0 :\n",
    "- **Pré-entraîné** sur 436 000 heures d'audio dans 128 langues\n",
    "- **Architecture** : wav2vec 2.0 avec des données multilingues\n",
    "- **Concept clé** : Inventaire phonétique universel — les représentations apprises\n",
    "  capturent des patterns acoustiques partagés entre langues\n",
    "\n",
    "### MMS : Massively Multilingual Speech\n",
    "\n",
    "| Aspect | Détail |\n",
    "|--------|--------|\n",
    "| Langues couvertes | 1 100+ |\n",
    "| Source de données | Textes religieux (Bible, Nouveau Testament) |\n",
    "| Modèle de base | XLS-R (wav2vec 2.0 multilingue) |\n",
    "| Adaptation | Adaptateurs bottleneck par langue |\n",
    "| Paramètres par langue | ~2-5% du modèle total |\n",
    "\n",
    "### Défis pour les langues africaines\n",
    "\n",
    "1. **Tonalité** (Yoruba, Igbo) : La hauteur de la voix change le sens lexical\n",
    "   - Exemple Yoruba : *owó* (argent) vs *owò* (respect) vs *ọwọ́* (main)\n",
    "   - Métriques adaptées : TER (Tone Error Rate), FER (F0 Error Rate)\n",
    "\n",
    "2. **Morphologie agglutinante** (Swahili, Zoulou) : Un mot = plusieurs morphèmes\n",
    "   - Exemple Swahili : *ninakupenda* = ni-na-ku-penda (je-présent-toi-aimer)\n",
    "   - Impact : Le WER au niveau mot est très sévère\n",
    "\n",
    "3. **Code-switching** (Swahili-Anglais, Wolof-Français) : Alternance de langues\n",
    "   - Nécessite des modèles multilingues capables de basculer entre langues\n",
    "\n",
    "4. **Rareté des données** : Peu de données étiquetées disponibles\n",
    "   - Solution : Transfer learning avec adaptateurs (ce notebook !)\n",
    "\n",
    "### Perspectives\n",
    "\n",
    "- **AfriSpeech** : Datasets dédiés aux langues africaines\n",
    "- **Fine-tuning multilingue** : Entraîner sur plusieurs langues africaines simultanément\n",
    "- **Adaptateurs composables** : Combiner des adaptateurs de langues proches\n",
    "- **Évaluation culturellement adaptée** : Métriques tenant compte de la tonalité et de la morphologie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
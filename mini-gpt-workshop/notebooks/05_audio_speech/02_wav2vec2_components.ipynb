{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02 — Composants wav2vec 2.0\n",
    "\n",
    "## Architecture wav2vec 2.0 : 3 blocs principaux\n",
    "\n",
    "wav2vec 2.0 (Baevski et al., 2020) est une architecture d'apprentissage auto-supervisé pour la parole.\n",
    "\n",
    "```\n",
    "Audio brut (16kHz)  →  Feature Encoder (CNN 1D)  →  Représentations latentes z\n",
    "                              ↓                              ↓\n",
    "                    Quantization Module          Context Network (Transformer)\n",
    "                    (Gumbel-Softmax)             (self-attention du chapitre 1)\n",
    "                              ↓                              ↓\n",
    "                    Pseudo-tokens q              Représentations contextuelles c\n",
    "                              └──── Perte Contrastive L_m ────┘\n",
    "```\n",
    "\n",
    "**Rappel chapitre 1** : Le Context Network réutilise le self-attention (Scaled Dot-Product + Multi-Head).\n",
    "\n",
    "Ce notebook implémente les 3 composants clés :\n",
    "1. **Feature Encoder** — CNN 1D, réduction temporelle ~320×\n",
    "2. **Quantization Module** — Codebook + Gumbel-Softmax\n",
    "3. **Perte Contrastive** — Distinguer la cible parmi les distracteurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "src_path = str(Path(\"../../src\").resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 — IMPLÉMENTATION FROM SCRATCH : Conv1D manuelle\n",
    "\n",
    "Avant d'utiliser PyTorch, comprenons la convolution 1D avec une boucle explicite.\n",
    "\n",
    "**Principe du stride** : Le stride contrôle la réduction temporelle.\n",
    "Avec stride=2, la sortie a 2× moins de pas temporels que l'entrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.feature_encoder import conv1d_from_scratch\n",
    "\n",
    "# Créer un signal simple : 1 canal, 20 échantillons\n",
    "np.random.seed(42)\n",
    "signal = np.random.randn(1, 20)  # (channels_in=1, length=20)\n",
    "print(f\"Signal d'entrée : shape={signal.shape}\")\n",
    "\n",
    "# Noyau de convolution : 4 filtres, 1 canal d'entrée, taille 5\n",
    "kernel = np.random.randn(4, 1, 5) * 0.1  # (channels_out=4, channels_in=1, kernel_size=5)\n",
    "print(f\"Kernel : shape={kernel.shape}\")\n",
    "\n",
    "# --- Stride = 1 (pas de réduction) ---\n",
    "out_s1 = conv1d_from_scratch(signal, kernel, stride=1)\n",
    "print(f\"\\nStride=1 : output shape={out_s1.shape}\")\n",
    "print(f\"  Longueur attendue : (20 - 5) // 1 + 1 = {(20 - 5) // 1 + 1}\")\n",
    "\n",
    "# --- Stride = 2 (réduction 2×) ---\n",
    "out_s2 = conv1d_from_scratch(signal, kernel, stride=2)\n",
    "print(f\"\\nStride=2 : output shape={out_s2.shape}\")\n",
    "print(f\"  Longueur attendue : (20 - 5) // 2 + 1 = {(20 - 5) // 2 + 1}\")\n",
    "\n",
    "# --- Stride = 5 (réduction 5×, comme la couche 1 de wav2vec 2.0) ---\n",
    "kernel_big = np.random.randn(4, 1, 10) * 0.1  # kernel_size=10\n",
    "out_s5 = conv1d_from_scratch(signal, kernel_big, stride=5)\n",
    "print(f\"\\nStride=5, kernel=10 : output shape={out_s5.shape}\")\n",
    "print(f\"  Longueur attendue : (20 - 10) // 5 + 1 = {(20 - 10) // 5 + 1}\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 3))\n",
    "axes[0].plot(signal[0], 'b-o', markersize=3)\n",
    "axes[0].set_title(f'Entrée ({signal.shape[1]} points)')\n",
    "axes[1].plot(out_s1[0], 'r-o', markersize=3)\n",
    "axes[1].set_title(f'Stride=1 ({out_s1.shape[1]} points)')\n",
    "axes[2].plot(out_s2[0], 'g-o', markersize=3)\n",
    "axes[2].set_title(f'Stride=2 ({out_s2.shape[1]} points)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 — IMPLÉMENTATION PYTORCH : FeatureEncoder complet\n",
    "\n",
    "Le FeatureEncoder de wav2vec 2.0 utilise 7 couches Conv1D avec des strides progressifs.\n",
    "Le stride total est 5 × 2⁶ = 320, réduisant 16 000 échantillons/s à ~50 vecteurs/s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.feature_encoder import FeatureEncoder\n",
    "\n",
    "# Créer le Feature Encoder\n",
    "encoder = FeatureEncoder(d_model=512)\n",
    "print(f\"Stride total : {encoder.total_stride}\")\n",
    "print(f\"Nombre de paramètres : {sum(p.numel() for p in encoder.parameters()):,}\")\n",
    "\n",
    "# Simuler 1 seconde d'audio à 16kHz\n",
    "batch_size = 2\n",
    "num_samples = 16000  # 1 seconde\n",
    "waveform = torch.randn(batch_size, 1, num_samples)\n",
    "print(f\"\\nEntrée : {waveform.shape} (batch={batch_size}, channels=1, samples={num_samples})\")\n",
    "\n",
    "# Forward pass avec shape checks à chaque couche\n",
    "x = waveform\n",
    "for i, layer in enumerate(encoder.layers):\n",
    "    x = layer(x)\n",
    "    k, s = encoder.LAYER_CONFIGS[i]\n",
    "    print(f\"  Couche {i+1} (kernel={k}, stride={s}) → {x.shape}\")\n",
    "\n",
    "# Forward pass complet (avec LayerNorm)\n",
    "output = encoder(waveform)\n",
    "print(f\"\\nSortie finale : {output.shape}\")\n",
    "print(f\"Réduction temporelle : {num_samples} → {output.shape[2]} ({num_samples / output.shape[2]:.0f}×)\")\n",
    "print(f\"Longueur calculée : {encoder.compute_output_length(num_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 — Comparaison from-scratch vs PyTorch\n",
    "\n",
    "Vérifions que les deux implémentations produisent des shapes cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.feature_encoder import feature_encoder_from_scratch\n",
    "\n",
    "# Petit signal pour la version from-scratch (plus lent)\n",
    "np.random.seed(42)\n",
    "small_waveform = np.random.randn(3200).astype(np.float32)  # 0.2 seconde\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FROM SCRATCH (NumPy)\")\n",
    "print(\"=\" * 60)\n",
    "output_scratch = feature_encoder_from_scratch(small_waveform)\n",
    "\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "waveform_pt = torch.tensor(small_waveform).unsqueeze(0).unsqueeze(0)  # (1, 1, 3200)\n",
    "with torch.no_grad():\n",
    "    output_pt = encoder(waveform_pt)\n",
    "print(f\"  Output shape: {output_pt.shape}\")\n",
    "\n",
    "print(f\"\\nComparaison des shapes :\")\n",
    "print(f\"  From scratch : {output_scratch.shape} (d_model, T)\")\n",
    "print(f\"  PyTorch      : {output_pt.shape} (batch, d_model, T)\")\n",
    "print(f\"  T from scratch = {output_scratch.shape[1]}, T PyTorch = {output_pt.shape[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 — IMPLÉMENTATION FROM SCRATCH : Quantification par argmin\n",
    "\n",
    "La quantification vectorielle remplace chaque vecteur latent par l'entrée\n",
    "la plus proche dans un codebook (livre de codes). C'est l'équivalent audio\n",
    "du lookup dans un vocabulaire de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.quantization import quantize_from_scratch\n",
    "\n",
    "# Créer des vecteurs latents simulés et un codebook\n",
    "np.random.seed(42)\n",
    "T, d = 10, 8  # 10 pas temporels, dimension 8\n",
    "V = 16  # 16 entrées dans le codebook\n",
    "\n",
    "x = np.random.randn(T, d).astype(np.float32)\n",
    "codebook = np.random.randn(V, d).astype(np.float32)\n",
    "\n",
    "print(f\"Vecteurs d'entrée : shape={x.shape}\")\n",
    "print(f\"Codebook : shape={codebook.shape} ({V} entrées de dimension {d})\")\n",
    "\n",
    "# Quantification\n",
    "quantized, indices = quantize_from_scratch(x, codebook)\n",
    "print(f\"\\nVecteurs quantifiés : shape={quantized.shape}\")\n",
    "print(f\"Indices sélectionnés : {indices}\")\n",
    "\n",
    "# Vérifier que chaque vecteur quantifié est bien une entrée du codebook\n",
    "for t in range(T):\n",
    "    assert np.allclose(quantized[t], codebook[indices[t]]), f\"Mismatch at t={t}\"\n",
    "print(\"\\n✓ Chaque vecteur quantifié correspond à une entrée du codebook\")\n",
    "\n",
    "# Visualisation des clusters\n",
    "from sklearn.decomposition import PCA\n",
    "try:\n",
    "    pca = PCA(n_components=2)\n",
    "    all_points = np.vstack([x, codebook])\n",
    "    projected = pca.fit_transform(all_points)\n",
    "    x_proj = projected[:T]\n",
    "    cb_proj = projected[T:]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(cb_proj[:, 0], cb_proj[:, 1], c='red', marker='x', s=100, label='Codebook')\n",
    "    plt.scatter(x_proj[:, 0], x_proj[:, 1], c='blue', marker='o', s=60, label='Input')\n",
    "    for t in range(T):\n",
    "        plt.plot([x_proj[t, 0], cb_proj[indices[t], 0]],\n",
    "                 [x_proj[t, 1], cb_proj[indices[t], 1]], 'k--', alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.title('Quantification vectorielle : chaque point → entrée codebook la plus proche')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"sklearn non disponible, visualisation PCA ignorée\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 — IMPLÉMENTATION PYTORCH : GumbelVectorQuantizer\n",
    "\n",
    "Le problème de l'argmin : il n'est **pas différentiable** (pas de gradient).\n",
    "\n",
    "Solution : **Gumbel-Softmax** — une approximation différentiable de la sélection discrète.\n",
    "- En forward : sélection hard (one-hot)\n",
    "- En backward : gradient doux (softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.quantization import GumbelVectorQuantizer\n",
    "\n",
    "# Créer le quantizer\n",
    "input_dim = 512\n",
    "num_groups = 2\n",
    "num_vars = 320\n",
    "quantizer = GumbelVectorQuantizer(\n",
    "    input_dim=input_dim, num_groups=num_groups, num_vars=num_vars\n",
    ")\n",
    "print(f\"Configuration :\")\n",
    "print(f\"  input_dim={input_dim}, num_groups={num_groups}, num_vars={num_vars}\")\n",
    "print(f\"  dim_per_group={quantizer.dim_per_group}\")\n",
    "print(f\"  Nombre de paramètres : {sum(p.numel() for p in quantizer.parameters()):,}\")\n",
    "\n",
    "# Simuler des vecteurs latents\n",
    "batch, T = 2, 50\n",
    "z = torch.randn(batch, T, input_dim)\n",
    "print(f\"\\nEntrée : {z.shape}\")\n",
    "\n",
    "# Quantification (mode entraînement)\n",
    "quantizer.train()\n",
    "q_train, perplexity = quantizer(z)\n",
    "print(f\"Sortie (train) : {q_train.shape}\")\n",
    "print(f\"Perplexité : shape={perplexity.shape}, mean={perplexity.mean():.4f}\")\n",
    "print(f\"Température courante : {quantizer.current_temp:.4f}\")\n",
    "\n",
    "# Quantification (mode inférence)\n",
    "quantizer.eval()\n",
    "with torch.no_grad():\n",
    "    q_eval, perp_eval = quantizer(z)\n",
    "print(f\"\\nSortie (eval) : {q_eval.shape}\")\n",
    "print(f\"Shape préservée ? {q_train.shape == z.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 — IMPLÉMENTATION FROM SCRATCH : Perte contrastive\n",
    "\n",
    "La perte contrastive force le modèle à distinguer la cible quantifiée correcte\n",
    "parmi des distracteurs négatifs.\n",
    "\n",
    "$$L_m = -\\log \\frac{\\exp(\\text{sim}(c_t, q_t) / \\kappa)}{\\exp(\\text{sim}(c_t, q_t) / \\kappa) + \\sum_{\\tilde{q}} \\exp(\\text{sim}(c_t, \\tilde{q}) / \\kappa)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.contrastive_loss import (\n",
    "    cosine_similarity_from_scratch,\n",
    "    contrastive_loss_from_scratch,\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "d = 8\n",
    "\n",
    "# Vecteur contexte et cible positive\n",
    "context = np.random.randn(d)\n",
    "target = context + np.random.randn(d) * 0.1  # Proche du contexte\n",
    "\n",
    "# Distracteurs négatifs (éloignés)\n",
    "num_negatives = 5\n",
    "negatives = np.random.randn(num_negatives, d)\n",
    "\n",
    "# Similarité cosinus\n",
    "sim_pos = cosine_similarity_from_scratch(context, target)\n",
    "print(f\"Similarité (contexte, cible positive) : {sim_pos:.4f}\")\n",
    "for i in range(num_negatives):\n",
    "    sim_neg = cosine_similarity_from_scratch(context, negatives[i])\n",
    "    print(f\"Similarité (contexte, négatif {i}) : {sim_neg:.4f}\")\n",
    "\n",
    "# Perte contrastive\n",
    "loss = contrastive_loss_from_scratch(context, target, negatives, temperature=0.1)\n",
    "print(f\"\\nPerte contrastive L_m = {loss:.4f}\")\n",
    "print(f\"  (Plus la cible est similaire au contexte, plus la perte est faible)\")\n",
    "\n",
    "# Démonstration : perte avec cible identique au contexte\n",
    "loss_perfect = contrastive_loss_from_scratch(context, context, negatives, temperature=0.1)\n",
    "print(f\"\\nPerte avec cible = contexte : {loss_perfect:.4f} (devrait être très faible)\")\n",
    "\n",
    "# Perte avec cible aléatoire\n",
    "loss_random = contrastive_loss_from_scratch(context, np.random.randn(d), negatives, temperature=0.1)\n",
    "print(f\"Perte avec cible aléatoire : {loss_random:.4f} (devrait être plus élevée)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 — IMPLÉMENTATION PYTORCH : Perte contrastive vectorisée + diversité\n",
    "\n",
    "Version vectorisée pour l'entraînement efficace, plus la perte de diversité\n",
    "$L_d$ qui empêche le mode collapse du codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.contrastive_loss import (\n",
    "    contrastive_loss,\n",
    "    sample_negatives,\n",
    "    diversity_loss,\n",
    ")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch, T, d = 2, 20, 64\n",
    "num_neg = 10\n",
    "\n",
    "# Simuler des vecteurs contexte et quantifiés\n",
    "context_vecs = torch.randn(batch, T, d)\n",
    "quantized_vecs = context_vecs + torch.randn(batch, T, d) * 0.3  # Proches\n",
    "\n",
    "# Échantillonner des négatifs\n",
    "negs = sample_negatives(quantized_vecs, num_negatives=num_neg)\n",
    "print(f\"Context : {context_vecs.shape}\")\n",
    "print(f\"Quantized : {quantized_vecs.shape}\")\n",
    "print(f\"Negatives : {negs.shape}\")\n",
    "\n",
    "# Perte contrastive\n",
    "L_m = contrastive_loss(context_vecs, quantized_vecs, negs, temperature=0.1)\n",
    "print(f\"\\nPerte contrastive L_m = {L_m.item():.4f}\")\n",
    "\n",
    "# Perte de diversité\n",
    "# Simuler des probabilités d'utilisation du codebook\n",
    "G, V = 2, 320\n",
    "# Cas 1 : utilisation uniforme (bonne)\n",
    "uniform_probs = torch.ones(G, V) / V\n",
    "L_d_uniform = diversity_loss(uniform_probs, num_groups=G, num_vars=V)\n",
    "print(f\"\\nL_d (utilisation uniforme) = {L_d_uniform.item():.4f} (devrait être ~0)\")\n",
    "\n",
    "# Cas 2 : mode collapse (mauvais)\n",
    "collapse_probs = torch.zeros(G, V)\n",
    "collapse_probs[:, 0] = 1.0  # Toujours la même entrée\n",
    "L_d_collapse = diversity_loss(collapse_probs, num_groups=G, num_vars=V)\n",
    "print(f\"L_d (mode collapse) = {L_d_collapse.item():.4f} (devrait être ~1)\")\n",
    "\n",
    "# Perte totale\n",
    "alpha = 0.1\n",
    "L_total = L_m + alpha * L_d_uniform\n",
    "print(f\"\\nPerte totale L = L_m + {alpha} × L_d = {L_total.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 — Pipeline complet PyTorch : audio → encoder → quantize → mask → loss\n",
    "\n",
    "Assemblons tous les composants pour simuler un pas d'entraînement wav2vec 2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.feature_encoder import FeatureEncoder\n",
    "from audio.quantization import GumbelVectorQuantizer\n",
    "from audio.contrastive_loss import contrastive_loss, sample_negatives, diversity_loss\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Configuration ---\n",
    "d_model = 512\n",
    "num_groups = 2\n",
    "num_vars = 320\n",
    "mask_prob = 0.065  # Probabilité de masquage\n",
    "mask_length = 10   # Longueur des spans masqués\n",
    "num_negatives = 100\n",
    "alpha = 0.1  # Poids de la perte de diversité\n",
    "\n",
    "# --- Modèle ---\n",
    "encoder = FeatureEncoder(d_model=d_model)\n",
    "quantizer = GumbelVectorQuantizer(input_dim=d_model, num_groups=num_groups, num_vars=num_vars)\n",
    "\n",
    "# --- Données ---\n",
    "batch_size = 2\n",
    "waveform = torch.randn(batch_size, 1, 16000)  # 1 seconde\n",
    "print(f\"1. Audio brut : {waveform.shape}\")\n",
    "\n",
    "# --- Étape 1 : Feature Encoder ---\n",
    "z = encoder(waveform)  # (batch, d_model, T)\n",
    "print(f\"2. Feature Encoder → z : {z.shape}\")\n",
    "\n",
    "# Transposer pour le quantizer : (batch, T, d_model)\n",
    "z = z.transpose(1, 2)\n",
    "T = z.shape[1]\n",
    "\n",
    "# --- Étape 2 : Quantification ---\n",
    "q, perplexity = quantizer(z)  # (batch, T, d_model)\n",
    "print(f\"3. Quantification → q : {q.shape}\")\n",
    "\n",
    "# --- Étape 3 : Masquage ---\n",
    "# Créer un masque aléatoire (simplification du masquage par spans)\n",
    "mask = torch.rand(batch_size, T) < mask_prob\n",
    "num_masked = mask.sum().item()\n",
    "print(f\"4. Masquage : {num_masked} positions masquées sur {batch_size * T}\")\n",
    "\n",
    "# Simuler le Context Network (ici on utilise z directement comme contexte)\n",
    "# Dans le vrai wav2vec 2.0, z passe par un Transformer\n",
    "c = z  # Simplification : contexte = latent\n",
    "\n",
    "# --- Étape 4 : Échantillonner les négatifs ---\n",
    "negs = sample_negatives(q, num_negatives=min(num_negatives, T - 1))\n",
    "print(f\"5. Négatifs échantillonnés : {negs.shape}\")\n",
    "\n",
    "# --- Étape 5 : Perte contrastive ---\n",
    "L_m = contrastive_loss(c, q, negs, temperature=0.1)\n",
    "L_d = diversity_loss(perplexity, num_groups=num_groups, num_vars=num_vars)\n",
    "L = L_m + alpha * L_d\n",
    "\n",
    "print(f\"\\n--- Pertes ---\")\n",
    "print(f\"  L_m (contrastive) = {L_m.item():.4f}\")\n",
    "print(f\"  L_d (diversité)   = {L_d.item():.4f}\")\n",
    "print(f\"  L (totale)        = {L.item():.4f}\")\n",
    "\n",
    "# --- Backward pass (vérifier que les gradients passent) ---\n",
    "L.backward()\n",
    "print(f\"\\n✓ Backward pass réussi !\")\n",
    "print(f\"  Gradient encoder couche 1 : {encoder.layers[0][0].weight.grad is not None}\")\n",
    "print(f\"  Gradient codebook : {quantizer.codebook.grad is not None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 — Visualisation des représentations latentes\n",
    "\n",
    "Visualisons les représentations à différentes étapes du pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculer sans gradients pour la visualisation\n",
    "encoder.eval()\n",
    "quantizer.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    waveform_viz = torch.randn(1, 1, 16000)\n",
    "    z_viz = encoder(waveform_viz).transpose(1, 2)  # (1, T, d_model)\n",
    "    q_viz, _ = quantizer(z_viz)\n",
    "\n",
    "z_np = z_viz[0].numpy()  # (T, d_model)\n",
    "q_np = q_viz[0].numpy()  # (T, d_model)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# 1. Waveform\n",
    "axes[0, 0].plot(waveform_viz[0, 0].numpy(), linewidth=0.5)\n",
    "axes[0, 0].set_title(f'Waveform ({waveform_viz.shape[2]} échantillons)')\n",
    "axes[0, 0].set_xlabel('Échantillon')\n",
    "\n",
    "# 2. Représentations latentes z (heatmap)\n",
    "im1 = axes[0, 1].imshow(z_np.T[:64], aspect='auto', cmap='viridis')\n",
    "axes[0, 1].set_title(f'Latent z ({z_np.shape[0]} frames × {z_np.shape[1]} dims)')\n",
    "axes[0, 1].set_xlabel('Frame')\n",
    "axes[0, 1].set_ylabel('Dimension')\n",
    "plt.colorbar(im1, ax=axes[0, 1])\n",
    "\n",
    "# 3. Représentations quantifiées q (heatmap)\n",
    "im2 = axes[1, 0].imshow(q_np.T[:64], aspect='auto', cmap='viridis')\n",
    "axes[1, 0].set_title(f'Quantized q ({q_np.shape[0]} frames × {q_np.shape[1]} dims)')\n",
    "axes[1, 0].set_xlabel('Frame')\n",
    "axes[1, 0].set_ylabel('Dimension')\n",
    "plt.colorbar(im2, ax=axes[1, 0])\n",
    "\n",
    "# 4. Différence z - q\n",
    "diff = np.abs(z_np - q_np)\n",
    "im3 = axes[1, 1].imshow(diff.T[:64], aspect='auto', cmap='hot')\n",
    "axes[1, 1].set_title('|z - q| (erreur de quantification)')\n",
    "axes[1, 1].set_xlabel('Frame')\n",
    "axes[1, 1].set_ylabel('Dimension')\n",
    "plt.colorbar(im3, ax=axes[1, 1])\n",
    "\n",
    "plt.suptitle('Pipeline wav2vec 2.0 : Waveform → Latent z → Quantized q', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Erreur de quantification moyenne : {diff.mean():.4f}\")\n",
    "print(f\"Réduction temporelle : {waveform_viz.shape[2]} → {z_np.shape[0]} ({waveform_viz.shape[2] / z_np.shape[0]:.0f}×)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
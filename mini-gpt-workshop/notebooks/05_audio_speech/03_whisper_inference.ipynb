{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03 — Whisper : Inférence et Évaluation ASR sur les Langues Africaines\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Dans le notebook précédent, nous avons implémenté les composants de **wav2vec 2.0**, une architecture **encodeur-seul** qui apprend des représentations audio par apprentissage auto-supervisé contrastif.\n",
    "\n",
    "**Whisper** (Radford et al., 2022) adopte une approche radicalement différente :\n",
    "\n",
    "| | wav2vec 2.0 | Whisper |\n",
    "|---|---|---|\n",
    "| **Architecture** | Encodeur seul | Encodeur-Décodeur |\n",
    "| **Pré-entraînement** | Auto-supervisé (contrastif) | Supervisé faiblement (680k heures) |\n",
    "| **Entrée** | Waveform brute → Feature Encoder CNN | Log-Mel spectrogram (80/128 canaux) |\n",
    "| **Sortie** | Représentations latentes | Texte (tokens) |\n",
    "| **Décodage** | Nécessite un head CTC/seq2seq | Auto-régressif avec tokens spéciaux |\n",
    "| **Multitâche** | Non | Oui (transcription, traduction, détection de langue) |\n",
    "\n",
    "Whisper utilise un **Transformer encodeur-décodeur** classique, conditionné par des **tokens spéciaux** qui contrôlent la tâche (transcription vs traduction), la langue, et les timestamps.\n",
    "\n",
    "Ce notebook couvre :\n",
    "1. Chargement d'un modèle Whisper pré-entraîné via HuggingFace\n",
    "2. Inférence sur un échantillon audio\n",
    "3. Implémentation from scratch des métriques WER/CER (distance de Levenshtein)\n",
    "4. Comparaison des performances sur langues africaines vs langues à haute ressource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Ajouter src/ au path\n",
    "src_path = str(Path(\"../../src\").resolve())\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 4)\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2 — Chargement du modèle Whisper pré-entraîné\n",
    "\n",
    "Nous utilisons la bibliothèque `transformers` de HuggingFace pour charger Whisper.\n",
    "\n",
    "Whisper est disponible en plusieurs tailles :\n",
    "- `whisper-tiny` (39M paramètres)\n",
    "- `whisper-base` (74M)\n",
    "- `whisper-small` (244M)\n",
    "- `whisper-medium` (769M)\n",
    "- `whisper-large-v3` (1.5B)\n",
    "\n",
    "Pour ce notebook, nous utilisons `whisper-small` comme bon compromis taille/performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Charger le processeur (tokenizer + feature extractor) et le modèle\n",
    "model_name = \"openai/whisper-small\"\n",
    "\n",
    "try:\n",
    "    processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    print(f\"Modèle chargé : {model_name}\")\n",
    "    print(f\"Nombre de paramètres : {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"\\nArchitecture :\")\n",
    "    print(f\"  Encodeur : {model.config.encoder_layers} couches, d_model={model.config.d_model}\")\n",
    "    print(f\"  Décodeur : {model.config.decoder_layers} couches\")\n",
    "    print(f\"  Attention heads : {model.config.encoder_attention_heads}\")\n",
    "    print(f\"  Vocabulaire : {model.config.vocab_size} tokens\")\n",
    "except OSError as e:\n",
    "    print(f\"Erreur de chargement (vérifiez votre connexion) : {e}\")\n",
    "    print(\"Vous pouvez télécharger le modèle manuellement avec :\")\n",
    "    print(f\"  huggingface-cli download {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3 — Inférence sur un échantillon audio\n",
    "\n",
    "Le pipeline Whisper :\n",
    "1. **Prétraitement** : Audio → Log-Mel spectrogram (80 canaux, fenêtres 25ms, pas 10ms)\n",
    "2. **Encodeur** : Log-Mel → représentations contextuelles\n",
    "3. **Décodeur** : Génération auto-régressive de tokens avec Cross-Attention\n",
    "\n",
    "Les **tokens spéciaux** contrôlent le comportement :\n",
    "```\n",
    "<|startoftranscript|> <|fr|> <|transcribe|> <|notimestamps|> ... texte ... <|endoftext|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.preprocessing import load_audio\n",
    "\n",
    "# Charger l'audio d'exemple\n",
    "audio_path = \"../../data/raw/sample_audio.wav\"\n",
    "waveform, sr = load_audio(audio_path, target_sr=16000)\n",
    "\n",
    "print(f\"Audio chargé : {len(waveform)/sr:.2f}s à {sr} Hz\")\n",
    "\n",
    "# Prétraitement : le processor convertit l'audio en Log-Mel spectrogram\n",
    "input_features = processor(\n",
    "    waveform, \n",
    "    sampling_rate=sr, \n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "print(f\"Shape Log-Mel : {input_features.shape}\")  # (1, 80, 3000) pour 30s max\n",
    "\n",
    "# Inférence : génération auto-régressive\n",
    "with torch.no_grad():\n",
    "    # forced_decoder_ids force la langue et la tâche\n",
    "    predicted_ids = model.generate(\n",
    "        input_features,\n",
    "        language=\"fr\",\n",
    "        task=\"transcribe\",\n",
    "    )\n",
    "\n",
    "# Décodage des tokens en texte\n",
    "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(f\"\\nTranscription : {transcription}\")\n",
    "\n",
    "# Afficher les tokens spéciaux\n",
    "tokens_with_special = processor.batch_decode(predicted_ids, skip_special_tokens=False)[0]\n",
    "print(f\"\\nTokens avec spéciaux : {tokens_with_special}\")\n",
    "\n",
    "# Décomposer les token IDs\n",
    "print(f\"\\nToken IDs ({len(predicted_ids[0])} tokens) :\")\n",
    "for i, tid in enumerate(predicted_ids[0][:10]):\n",
    "    token_str = processor.tokenizer.decode([tid])\n",
    "    print(f\"  [{i}] ID={tid.item():5d} → '{token_str}'\")\n",
    "if len(predicted_ids[0]) > 10:\n",
    "    print(f\"  ... ({len(predicted_ids[0]) - 10} tokens restants)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4 — IMPLÉMENTATION FROM SCRATCH : Distance de Levenshtein et WER/CER\n",
    "\n",
    "Pour évaluer un système ASR, on compare la transcription produite (hypothèse) au texte de référence.\n",
    "\n",
    "### Distance d'édition de Levenshtein\n",
    "\n",
    "La distance de Levenshtein entre deux séquences est le nombre minimal d'opérations élémentaires pour transformer l'une en l'autre :\n",
    "- **Insertion** d'un élément\n",
    "- **Suppression** d'un élément  \n",
    "- **Substitution** d'un élément par un autre\n",
    "\n",
    "On la calcule par **programmation dynamique** avec une matrice `(n+1) × (m+1)` :\n",
    "\n",
    "$$dp[i][j] = \\begin{cases} i & \\text{si } j = 0 \\\\ j & \\text{si } i = 0 \\\\ dp[i-1][j-1] & \\text{si } ref[i] = hyp[j] \\\\ 1 + \\min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) & \\text{sinon} \\end{cases}$$\n",
    "\n",
    "### Métriques\n",
    "- **WER** (Word Error Rate) = `levenshtein(ref_mots, hyp_mots) / len(ref_mots)`\n",
    "- **CER** (Character Error Rate) = `levenshtein(ref_chars, hyp_chars) / len(ref_chars)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.metrics import (\n",
    "    levenshtein_distance_from_scratch,\n",
    "    compute_wer_from_scratch,\n",
    "    compute_cer_from_scratch,\n",
    ")\n",
    "\n",
    "# Exemple concret\n",
    "reference = \"le chat est assis sur le tapis\"\n",
    "hypothesis = \"le chat assis sur le tapi\"\n",
    "\n",
    "# --- Distance de Levenshtein au niveau des mots ---\n",
    "ref_words = reference.split()\n",
    "hyp_words = hypothesis.split()\n",
    "print(f\"Référence  : {ref_words}\")\n",
    "print(f\"Hypothèse  : {hyp_words}\")\n",
    "\n",
    "dist_words = levenshtein_distance_from_scratch(ref_words, hyp_words)\n",
    "print(f\"\\nDistance de Levenshtein (mots) : {dist_words}\")\n",
    "print(f\"  → 'est' supprimé, 'tapis'→'tapi' substitué = 2 opérations\")\n",
    "\n",
    "# --- WER ---\n",
    "wer = compute_wer_from_scratch(reference, hypothesis)\n",
    "print(f\"\\nWER = {dist_words} / {len(ref_words)} = {wer:.4f} ({wer*100:.1f}%)\")\n",
    "\n",
    "# --- CER ---\n",
    "cer = compute_cer_from_scratch(reference, hypothesis)\n",
    "ref_chars = list(reference)\n",
    "hyp_chars = list(hypothesis)\n",
    "dist_chars = levenshtein_distance_from_scratch(ref_chars, hyp_chars)\n",
    "print(f\"CER = {dist_chars} / {len(ref_chars)} = {cer:.4f} ({cer*100:.1f}%)\")\n",
    "\n",
    "# --- Visualisation de la matrice DP ---\n",
    "print(\"\\n--- Matrice de programmation dynamique (mots) ---\")\n",
    "n, m = len(ref_words), len(hyp_words)\n",
    "dp = [[0] * (m + 1) for _ in range(n + 1)]\n",
    "for i in range(n + 1):\n",
    "    dp[i][0] = i\n",
    "for j in range(m + 1):\n",
    "    dp[0][j] = j\n",
    "for i in range(1, n + 1):\n",
    "    for j in range(1, m + 1):\n",
    "        if ref_words[i-1] == hyp_words[j-1]:\n",
    "            dp[i][j] = dp[i-1][j-1]\n",
    "        else:\n",
    "            dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])\n",
    "\n",
    "# Affichage formaté\n",
    "header = [''] + ['∅'] + hyp_words\n",
    "print(f\"{'':>10s}\", end='')\n",
    "for h in ['∅'] + hyp_words:\n",
    "    print(f\"{h:>10s}\", end='')\n",
    "print()\n",
    "for i, row in enumerate(dp):\n",
    "    label = '∅' if i == 0 else ref_words[i-1]\n",
    "    print(f\"{label:>10s}\", end='')\n",
    "    for val in row:\n",
    "        print(f\"{val:>10d}\", end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5 — IMPLÉMENTATION JIWER : Calcul optimisé WER/CER\n",
    "\n",
    "La bibliothèque **jiwer** fournit les mêmes métriques en 2 lignes, avec des optimisations et des transformations de normalisation intégrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio.metrics import compute_wer, compute_cer\n",
    "\n",
    "reference = \"le chat est assis sur le tapis\"\n",
    "hypothesis = \"le chat assis sur le tapi\"\n",
    "\n",
    "# Calcul via jiwer\n",
    "wer_jiwer = compute_wer(reference, hypothesis)\n",
    "cer_jiwer = compute_cer(reference, hypothesis)\n",
    "\n",
    "# Calcul from scratch pour comparaison\n",
    "wer_scratch = compute_wer_from_scratch(reference, hypothesis)\n",
    "cer_scratch = compute_cer_from_scratch(reference, hypothesis)\n",
    "\n",
    "print(\"Comparaison des implémentations :\")\n",
    "print(f\"{'Métrique':<10s} {'From Scratch':>15s} {'jiwer':>15s} {'Match':>10s}\")\n",
    "print(\"-\" * 55)\n",
    "print(f\"{'WER':<10s} {wer_scratch:>15.4f} {wer_jiwer:>15.4f} {'✓' if abs(wer_scratch - wer_jiwer) < 1e-6 else '✗':>10s}\")\n",
    "print(f\"{'CER':<10s} {cer_scratch:>15.4f} {cer_jiwer:>15.4f} {'✓' if abs(cer_scratch - cer_jiwer) < 1e-6 else '✗':>10s}\")\n",
    "\n",
    "# Test d'identité\n",
    "print(f\"\\nTest d'identité (ref == hyp) :\")\n",
    "print(f\"  WER = {compute_wer(reference, reference):.4f} (attendu : 0.0)\")\n",
    "print(f\"  CER = {compute_cer(reference, reference):.4f} (attendu : 0.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6 — Tableau comparatif : Performances Whisper sur langues africaines\n",
    "\n",
    "Whisper a été entraîné principalement sur des données en anglais (~60%) et en langues européennes. Les performances sur les langues africaines sont significativement plus faibles, illustrant le défi des **langues peu dotées** (low-resource languages).\n",
    "\n",
    "Les chiffres ci-dessous sont issus de la littérature (Radford et al. 2022, Whisper paper) et de benchmarks communautaires (OpenASR Leaderboard, Fleurs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Données de performance Whisper (approximatives, basées sur Fleurs benchmark)\n",
    "# Sources : Radford et al. 2022, OpenASR Leaderboard\n",
    "performance_data = {\n",
    "    \"Langue\": [\n",
    "        \"Anglais\", \"Français\",\n",
    "        \"Swahili\", \"Wolof\", \"Yoruba\",\n",
    "    ],\n",
    "    \"Code ISO\": [\"en\", \"fr\", \"sw\", \"wo\", \"yo\"],\n",
    "    \"Famille\": [\n",
    "        \"Germanique\", \"Romane\",\n",
    "        \"Bantoue\", \"Atlantique\", \"Volta-Niger\",\n",
    "    ],\n",
    "    \"Heures données (approx.)\": [\n",
    "        \"~438k\", \"~85k\",\n",
    "        \"~2k\", \"<100\", \"<500\",\n",
    "    ],\n",
    "    \"WER Whisper-small (%)\": [\n",
    "        8.5, 12.3,\n",
    "        28.7, 62.4, 48.1,\n",
    "    ],\n",
    "    \"WER Whisper-large-v3 (%)\": [\n",
    "        4.2, 7.1,\n",
    "        18.5, 45.2, 32.6,\n",
    "    ],\n",
    "    \"Défis spécifiques\": [\n",
    "        \"Référence (haute ressource)\",\n",
    "        \"Référence (haute ressource)\",\n",
    "        \"Morphologie agglutinante, code-switching\",\n",
    "        \"Tonalité, très peu de données\",\n",
    "        \"Tonalité (3 niveaux), diacritiques\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(performance_data)\n",
    "print(\"=\" * 100)\n",
    "print(\"TABLEAU COMPARATIF — Performances Whisper par langue\")\n",
    "print(\"=\" * 100)\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nSources : Radford et al. 2022, Fleurs benchmark, OpenASR Leaderboard\")\n",
    "print(\"Note : Les valeurs sont approximatives et varient selon le benchmark utilisé.\")\n",
    "\n",
    "# Visualisation\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, df[\"WER Whisper-small (%)\"], width, \n",
    "               label=\"Whisper-small\", color=\"#4C72B0\", alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, df[\"WER Whisper-large-v3 (%)\"], width,\n",
    "               label=\"Whisper-large-v3\", color=\"#DD8452\", alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"WER (%)\")\n",
    "ax.set_title(\"Word Error Rate par langue — Whisper small vs large-v3\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(df[\"Langue\"])\n",
    "ax.legend()\n",
    "ax.axhline(y=20, color='gray', linestyle='--', alpha=0.5, label='Seuil usable (20%)')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "for bar in bars2:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "            f'{bar.get_height():.1f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Ratio de dégradation\n",
    "print(\"\\nRatio de dégradation par rapport à l'anglais (Whisper-small) :\")\n",
    "en_wer = df[df[\"Langue\"] == \"Anglais\"][\"WER Whisper-small (%)\"].values[0]\n",
    "for _, row in df.iterrows():\n",
    "    ratio = row[\"WER Whisper-small (%)\"] / en_wer\n",
    "    print(f\"  {row['Langue']:>10s} : WER = {row['WER Whisper-small (%)']:5.1f}% → ×{ratio:.1f} vs anglais\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7 — Limitations et Hallucinations de Whisper\n",
    "\n",
    "### Problèmes connus\n",
    "\n",
    "1. **Hallucinations sur les langues peu dotées**\n",
    "   - Whisper peut générer du texte fluide mais complètement inventé quand l'audio est dans une langue peu représentée\n",
    "   - Le modèle « hallucine » en produisant du texte dans une langue à haute ressource (souvent l'anglais)\n",
    "   - Ce phénomène est particulièrement fréquent pour le Wolof, le Yoruba et d'autres langues africaines\n",
    "\n",
    "2. **Biais de données d'entraînement**\n",
    "   - ~60% des données sont en anglais\n",
    "   - Les langues africaines représentent < 1% du corpus total\n",
    "   - Les accents et variétés dialectales sont sous-représentés\n",
    "\n",
    "3. **Défis linguistiques spécifiques**\n",
    "   - **Tonalité** : Whisper ne capture pas bien les distinctions tonales (Yoruba : owó=argent vs owo=respect)\n",
    "   - **Morphologie agglutinante** : Les mots longs en Swahili/Zoulou sont mal segmentés par le tokenizer BPE\n",
    "   - **Code-switching** : L'alternance Swahili-Anglais ou Yoruba-Anglais perturbe la détection de langue\n",
    "\n",
    "4. **Heuristiques anti-hallucination**\n",
    "   - **Temperature Fallback** : Si la log-probabilité moyenne est trop basse, on augmente la température et on ré-échantillonne\n",
    "   - **Compression Gzip** : Si le ratio de compression du texte généré est anormalement élevé, c'est un signe de répétition/hallucination\n",
    "   - **Seuil de log-probabilité** : Les segments avec une confiance trop faible sont marqués comme incertains\n",
    "\n",
    "### Pistes d'amélioration\n",
    "\n",
    "- **Fine-tuning** sur des données spécifiques à la langue cible (même quelques heures aident)\n",
    "- **Adaptateurs** (voir Notebook 04) : Ajouter des modules légers sans modifier le modèle de base\n",
    "- **MMS** (Meta) : Architecture spécialement conçue pour 1100+ langues avec des adaptateurs par langue\n",
    "- **Augmentation de données** : Synthèse vocale (TTS) pour générer des données d'entraînement supplémentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résumé des points clés\n",
    "print(\"=\" * 70)\n",
    "print(\"RÉSUMÉ — Points clés du Notebook 03\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"1. Whisper = Transformer encodeur-décodeur supervisé (680k heures)\")\n",
    "print(\"2. Entrée : Log-Mel spectrogram → Sortie : tokens texte\")\n",
    "print(\"3. Tokens spéciaux contrôlent langue, tâche, timestamps\")\n",
    "print(\"4. WER = distance d'édition (mots) / nombre de mots référence\")\n",
    "print(\"5. CER = distance d'édition (caractères) / nombre de caractères\")\n",
    "print(\"6. Performance dégradée sur langues africaines (×3 à ×7 vs anglais)\")\n",
    "print(\"7. Hallucinations fréquentes sur langues peu dotées\")\n",
    "print(\"8. Solutions : fine-tuning, adaptateurs (MMS), augmentation de données\")\n",
    "print()\n",
    "print(\"→ Notebook suivant : Implémentation d'adaptateurs pour langues africaines\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

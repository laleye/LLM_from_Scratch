{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Embeddings: Token et Positional\n",
    "\n",
    "## Objectifs P\u00e9dagogiques\n",
    "\n",
    "Dans ce notebook, nous allons:\n",
    "1. **Comprendre les embeddings de tokens** - Comment convertir des IDs discrets en vecteurs denses\n",
    "2. **Impl\u00e9menter l'encodage positionnel** - Comment encoder la position des tokens dans une s\u00e9quence\n",
    "3. **Combiner les deux types d'embeddings** - Comment cr\u00e9er les embeddings d'entr\u00e9e finaux\n",
    "4. **Visualiser les patterns** - Observer les structures dans l'espace d'embedding\n",
    "\n",
    "## Pourquoi les Embeddings?\n",
    "\n",
    "Les transformers ne peuvent pas travailler directement avec des tokens discrets (mots, sous-mots). Ils ont besoin de repr\u00e9sentations continues (vecteurs) qui capturent:\n",
    "- **Le contenu s\u00e9mantique** (via les token embeddings)\n",
    "- **La position dans la s\u00e9quence** (via les positional embeddings)\n",
    "\n",
    "## Structure du Notebook\n",
    "\n",
    "**Partie 1**: Token Embeddings (From Scratch + PyTorch)\n",
    "**Partie 2**: Positional Embeddings (From Scratch + PyTorch)\n",
    "**Partie 3**: Combinaison et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports n\u00e9cessaires\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Ajouter le chemin vers les modules src\n",
    "sys.path.append('../../src')\n",
    "\n",
    "# Import des impl\u00e9mentations\n",
    "from embeddings.token_embedding import (\n",
    "    create_token_embedding_matrix,\n",
    "    token_embedding_lookup,\n",
    "    TokenEmbedding,\n",
    "    validate_embedding_dimensions,\n",
    "    check_embedding_shape\n",
    ")\n",
    "from embeddings.positional_embedding import (\n",
    "    create_sinusoidal_positional_encoding,\n",
    "    get_positional_encoding,\n",
    "    PositionalEmbedding,\n",
    "    create_position_ids,\n",
    "    combine_token_and_positional_embeddings,\n",
    "    validate_positional_encoding_dimensions\n",
    ")\n",
    "\n",
    "# Configuration pour les visualisations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\u2713 Imports r\u00e9ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 1: Token Embeddings\n",
    "\n",
    "## Concept\n",
    "\n",
    "Un **token embedding** convertit un ID de token (entier) en un vecteur dense de dimension `d_model`.\n",
    "\n",
    "### Formule Math\u00e9matique\n",
    "\n",
    "Soit $E \\in \\mathbb{R}^{\\text{vocab\\_size} \\times d_{\\text{model}}}$ la matrice d'embedding.\n",
    "\n",
    "Pour un token avec ID $i$, l'embedding est simplement la ligne $i$ de la matrice:\n",
    "\n",
    "$$\\text{embedding}(i) = E[i] \\in \\mathbb{R}^{d_{\\text{model}}}$$\n",
    "\n",
    "Pour une s\u00e9quence de tokens $[i_1, i_2, ..., i_n]$, on obtient:\n",
    "\n",
    "$$\\text{embeddings} = [E[i_1], E[i_2], ..., E[i_n]] \\in \\mathbb{R}^{n \\times d_{\\text{model}}}$$\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **Input**: Token IDs de shape `(batch_size, seq_len)`\n",
    "- **Matrice d'embedding**: Shape `(vocab_size, d_model)`\n",
    "- **Output**: Embeddings de shape `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPL\u00c9MENTATION 1: From Scratch (NumPy)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKEN EMBEDDINGS - FROM SCRATCH (NumPy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "vocab_size = 1000  # Vocabulaire de 1000 tokens\n",
    "d_model = 128      # Vecteurs de dimension 128\n",
    "batch_size = 4     # Batch de 4 s\u00e9quences\n",
    "seq_len = 10       # S\u00e9quences de longueur 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Vocabulaire: {vocab_size} tokens\")\n",
    "print(f\"  - Dimension d'embedding: {d_model}\")\n",
    "print(f\"  - Taille du batch: {batch_size}\")\n",
    "print(f\"  - Longueur de s\u00e9quence: {seq_len}\")\n",
    "\n",
    "# \u00c9tape 1: Cr\u00e9er la matrice d'embedding\n",
    "print(f\"\\n1. Cr\u00e9ation de la matrice d'embedding:\")\n",
    "embedding_matrix = create_token_embedding_matrix(vocab_size, d_model)\n",
    "print(f\"   \u2713 Shape: {embedding_matrix.shape}\")\n",
    "print(f\"   \u2713 Nombre de param\u00e8tres: {vocab_size * d_model:,}\")\n",
    "\n",
    "# \u00c9tape 2: Cr\u00e9er des token IDs al\u00e9atoires\n",
    "print(f\"\\n2. Cr\u00e9ation de token IDs:\")\n",
    "token_ids = np.random.randint(0, vocab_size, size=(batch_size, seq_len))\n",
    "print(f\"   \u2713 Shape: {token_ids.shape}\")\n",
    "print(f\"   \u2713 Exemple (premi\u00e8re s\u00e9quence): {token_ids[0]}\")\n",
    "\n",
    "# \u00c9tape 3: Lookup des embeddings\n",
    "print(f\"\\n3. Lookup des embeddings:\")\n",
    "embeddings = token_embedding_lookup(token_ids, embedding_matrix)\n",
    "print(f\"   \u2713 Shape: {embeddings.shape}\")\n",
    "print(f\"   \u2713 V\u00e9rification: {embeddings.shape} == ({batch_size}, {seq_len}, {d_model})\")\n",
    "\n",
    "# \u00c9tape 4: Inspection d'un embedding\n",
    "print(f\"\\n4. Inspection d'un embedding:\")\n",
    "token_id = token_ids[0, 0]\n",
    "embedding_vector = embeddings[0, 0]\n",
    "print(f\"   Token ID: {token_id}\")\n",
    "print(f\"   Embedding (5 premiers \u00e9l\u00e9ments): {embedding_vector[:5]}\")\n",
    "print(f\"   Norme L2: {np.linalg.norm(embedding_vector):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Impl\u00e9mentation NumPy termin\u00e9e!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl\u00e9mentation PyTorch: nn.Embedding\n",
    "\n",
    "PyTorch fournit `nn.Embedding`, une couche optimis\u00e9e qui:\n",
    "- Stocke la matrice d'embedding comme param\u00e8tres apprenables\n",
    "- Effectue des lookups efficaces sur GPU\n",
    "- Supporte la diff\u00e9rentiation automatique\n",
    "\n",
    "### M\u00e9thodes PyTorch Utilis\u00e9es\n",
    "\n",
    "- **`nn.Embedding(num_embeddings, embedding_dim)`**: Cr\u00e9e une couche d'embedding\n",
    "  - `num_embeddings`: Taille du vocabulaire\n",
    "  - `embedding_dim`: Dimension des vecteurs\n",
    "  - Attribut `weight`: Matrice d'embedding de shape `(vocab_size, d_model)`\n",
    "\n",
    "- **Forward pass**: Input `(batch_size, seq_len)` \u2192 Output `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPL\u00c9MENTATION 2: PyTorch (nn.Module)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKEN EMBEDDINGS - PYTORCH (nn.Embedding)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration (m\u00eame que NumPy pour comparaison)\n",
    "vocab_size = 1000\n",
    "d_model = 128\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Vocabulaire: {vocab_size} tokens\")\n",
    "print(f\"  - Dimension d'embedding: {d_model}\")\n",
    "\n",
    "# \u00c9tape 1: Cr\u00e9er la couche TokenEmbedding\n",
    "print(f\"\\n1. Cr\u00e9ation de la couche TokenEmbedding:\")\n",
    "token_emb = TokenEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "print(f\"   \u2713 Couche cr\u00e9\u00e9e: {token_emb}\")\n",
    "\n",
    "# Compter les param\u00e8tres\n",
    "num_params = sum(p.numel() for p in token_emb.parameters())\n",
    "print(f\"   \u2713 Nombre de param\u00e8tres: {num_params:,}\")\n",
    "\n",
    "# \u00c9tape 2: Cr\u00e9er des token IDs\n",
    "print(f\"\\n2. Cr\u00e9ation de token IDs:\")\n",
    "token_ids_torch = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"   \u2713 Shape: {token_ids_torch.shape}\")\n",
    "print(f\"   \u2713 Type: {token_ids_torch.dtype}\")\n",
    "print(f\"   \u2713 Exemple: {token_ids_torch[0]}\")\n",
    "\n",
    "# \u00c9tape 3: Forward pass\n",
    "print(f\"\\n3. Forward pass:\")\n",
    "embeddings_torch = token_emb(token_ids_torch)\n",
    "print(f\"   \u2713 Shape: {embeddings_torch.shape}\")\n",
    "print(f\"   \u2713 Type: {embeddings_torch.dtype}\")\n",
    "print(f\"   \u2713 Device: {embeddings_torch.device}\")\n",
    "\n",
    "# \u00c9tape 4: V\u00e9rification des dimensions\n",
    "print(f\"\\n4. V\u00e9rification des dimensions:\")\n",
    "check_embedding_shape(token_ids_torch, embeddings_torch, d_model)\n",
    "\n",
    "# \u00c9tape 5: Statistiques\n",
    "print(f\"5. Statistiques des embeddings:\")\n",
    "print(f\"   - Moyenne: {embeddings_torch.mean().item():.6f}\")\n",
    "print(f\"   - \u00c9cart-type: {embeddings_torch.std().item():.6f}\")\n",
    "print(f\"   - Min: {embeddings_torch.min().item():.6f}\")\n",
    "print(f\"   - Max: {embeddings_torch.max().item():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Impl\u00e9mentation PyTorch termin\u00e9e!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 2: Positional Embeddings\n",
    "\n",
    "## Probl\u00e8me\n",
    "\n",
    "Le m\u00e9canisme d'attention est **position-agnostic**: il traite tous les tokens de la m\u00eame mani\u00e8re, peu importe leur position. Pour que le mod\u00e8le comprenne l'ordre des mots, nous devons ajouter des **encodages positionnels**.\n",
    "\n",
    "## Solution: Encodage Sinuso\u00efdal\n",
    "\n",
    "Vaswani et al. (2017) ont propos\u00e9 d'utiliser des fonctions sinuso\u00efdales:\n",
    "\n",
    "### Formules Math\u00e9matiques\n",
    "\n",
    "Pour une position $\\text{pos}$ et une dimension $i$:\n",
    "\n",
    "$$PE_{(\\text{pos}, 2i)} = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "$$PE_{(\\text{pos}, 2i+1)} = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "o\u00f9:\n",
    "- $\\text{pos}$ est la position dans la s\u00e9quence (0, 1, 2, ...)\n",
    "- $i$ est l'indice de la dimension (0, 1, 2, ..., $d_{\\text{model}}/2 - 1$)\n",
    "- Les dimensions **paires** (0, 2, 4, ...) utilisent $\\sin$\n",
    "- Les dimensions **impaires** (1, 3, 5, ...) utilisent $\\cos$\n",
    "\n",
    "### Intuition\n",
    "\n",
    "- **Fr\u00e9quences vari\u00e9es**: Les dimensions basses ont des fr\u00e9quences \u00e9lev\u00e9es (changent rapidement), les dimensions hautes ont des fr\u00e9quences basses (changent lentement)\n",
    "- **Positions relatives**: Le mod\u00e8le peut apprendre \u00e0 d\u00e9tecter les positions relatives gr\u00e2ce aux patterns sinuso\u00efdaux\n",
    "- **Extrapolation**: Peut g\u00e9n\u00e9raliser \u00e0 des s\u00e9quences plus longues que celles vues pendant l'entra\u00eenement\n",
    "\n",
    "### Dimensions\n",
    "\n",
    "- **Matrice d'encodage**: Shape `(max_seq_len, d_model)`\n",
    "- **Input**: Positions de shape `(batch_size, seq_len)`\n",
    "- **Output**: Encodages de shape `(batch_size, seq_len, d_model)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPL\u00c9MENTATION 1: From Scratch (NumPy)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITIONAL ENCODINGS - FROM SCRATCH (NumPy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "max_seq_len = 100  # S\u00e9quences jusqu'\u00e0 100 tokens\n",
    "d_model = 128      # Dimension (doit \u00eatre pair)\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Longueur max: {max_seq_len}\")\n",
    "print(f\"  - Dimension: {d_model}\")\n",
    "print(f\"  - Batch: {batch_size}\")\n",
    "print(f\"  - S\u00e9quence: {seq_len}\")\n",
    "\n",
    "# \u00c9tape 1: Cr\u00e9er la matrice d'encodage positionnel\n",
    "print(f\"\\n1. Cr\u00e9ation de la matrice d'encodage positionnel:\")\n",
    "pe_matrix = create_sinusoidal_positional_encoding(max_seq_len, d_model)\n",
    "print(f\"   \u2713 Shape: {pe_matrix.shape}\")\n",
    "print(f\"   \u2713 Type: Sinuso\u00efdal (sin/cos)\")\n",
    "\n",
    "# \u00c9tape 2: Cr\u00e9er des IDs de positions\n",
    "print(f\"\\n2. Cr\u00e9ation des IDs de positions:\")\n",
    "position_ids = create_position_ids(batch_size, seq_len)\n",
    "print(f\"   \u2713 Shape: {position_ids.shape}\")\n",
    "print(f\"   \u2713 Exemple: {position_ids[0]}\")\n",
    "\n",
    "# \u00c9tape 3: R\u00e9cup\u00e9rer les encodages\n",
    "print(f\"\\n3. R\u00e9cup\u00e9ration des encodages:\")\n",
    "pos_encodings = get_positional_encoding(position_ids, pe_matrix)\n",
    "print(f\"   \u2713 Shape: {pos_encodings.shape}\")\n",
    "print(f\"   \u2713 V\u00e9rification: {pos_encodings.shape} == ({batch_size}, {seq_len}, {d_model})\")\n",
    "\n",
    "# \u00c9tape 4: Propri\u00e9t\u00e9s sinuso\u00efdales\n",
    "print(f\"\\n4. Propri\u00e9t\u00e9s sinuso\u00efdales:\")\n",
    "print(f\"   - Valeurs dans [-1, 1]: {np.all(np.abs(pe_matrix) <= 1.0)}\")\n",
    "print(f\"   - Moyenne: {np.mean(pe_matrix):.6f}\")\n",
    "print(f\"   - \u00c9cart-type: {np.std(pe_matrix):.6f}\")\n",
    "\n",
    "# \u00c9tape 5: Inspection d'un encodage\n",
    "print(f\"\\n5. Inspection d'un encodage (position 0):\")\n",
    "pos_0 = pe_matrix[0]\n",
    "print(f\"   Dimensions paires (sin): {pos_0[0::2][:5]}\")\n",
    "print(f\"   Dimensions impaires (cos): {pos_0[1::2][:5]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Impl\u00e9mentation NumPy termin\u00e9e!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation: Heatmap des Encodages Positionnels\n",
    "\n",
    "Visualisons les patterns sinuso\u00efdaux dans l'encodage positionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la matrice d'encodage positionnel\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Afficher les 50 premi\u00e8res positions et toutes les dimensions\n",
    "num_positions_to_show = 50\n",
    "pe_to_plot = pe_matrix[:num_positions_to_show, :]\n",
    "\n",
    "# Cr\u00e9er le heatmap\n",
    "im = ax.imshow(pe_to_plot, cmap='RdBu', aspect='auto', vmin=-1, vmax=1)\n",
    "\n",
    "# Labels et titre\n",
    "ax.set_xlabel('Dimension', fontsize=12)\n",
    "ax.set_ylabel('Position', fontsize=12)\n",
    "ax.set_title('Encodage Positionnel Sinuso\u00efdal\\n(Heatmap des 50 premi\u00e8res positions)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Valeur', rotation=270, labelpad=20)\n",
    "\n",
    "# Annotations\n",
    "ax.text(0.02, 0.98, 'Dimensions paires: sin\\nDimensions impaires: cos',\n",
    "        transform=ax.transAxes, fontsize=10,\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Observations:\")\n",
    "print(\"  - Les dimensions basses (gauche) changent rapidement avec la position\")\n",
    "print(\"  - Les dimensions hautes (droite) changent lentement\")\n",
    "print(\"  - Patterns altern\u00e9s sin/cos cr\u00e9ent des structures uniques pour chaque position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impl\u00e9mentation PyTorch: PositionalEmbedding\n",
    "\n",
    "PyTorch permet de stocker les encodages positionnels comme un **buffer** (non-entra\u00eenable).\n",
    "\n",
    "### M\u00e9thodes PyTorch Utilis\u00e9es\n",
    "\n",
    "- **`register_buffer()`**: Enregistre un tensor comme partie du module mais non-entra\u00eenable\n",
    "  - Le buffer est sauvegard\u00e9/charg\u00e9 avec le mod\u00e8le\n",
    "  - D\u00e9plac\u00e9 automatiquement sur GPU avec le mod\u00e8le\n",
    "  - Pas de gradients calcul\u00e9s\n",
    "\n",
    "- **`torch.arange()`**: Cr\u00e9e une s\u00e9quence [0, 1, 2, ..., n-1]\n",
    "- **`torch.sin()` / `torch.cos()`**: Fonctions trigonom\u00e9triques\n",
    "\n",
    "### Diff\u00e9rence Cl\u00e9\n",
    "\n",
    "- **TokenEmbedding**: Poids **apprenables** (mis \u00e0 jour pendant l'entra\u00eenement)\n",
    "- **PositionalEmbedding**: Poids **fixes** (calcul\u00e9s une fois, jamais modifi\u00e9s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPL\u00c9MENTATION 2: PyTorch (nn.Module)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POSITIONAL ENCODINGS - PYTORCH\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "max_seq_len = 100\n",
    "d_model = 128\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Longueur max: {max_seq_len}\")\n",
    "print(f\"  - Dimension: {d_model}\")\n",
    "\n",
    "# \u00c9tape 1: Cr\u00e9er la couche PositionalEmbedding\n",
    "print(f\"\\n1. Cr\u00e9ation de la couche PositionalEmbedding:\")\n",
    "pos_emb = PositionalEmbedding(max_seq_len=max_seq_len, d_model=d_model)\n",
    "print(f\"   \u2713 Couche cr\u00e9\u00e9e: {pos_emb}\")\n",
    "\n",
    "# V\u00e9rifier que les poids ne sont pas entra\u00eenables\n",
    "num_params = sum(p.numel() for p in pos_emb.parameters())\n",
    "print(f\"   \u2713 Param\u00e8tres entra\u00eenables: {num_params}\")\n",
    "print(f\"   \u2713 Note: Les encodages sont FIXES (non-entra\u00eenables)\")\n",
    "\n",
    "# \u00c9tape 2: Cr\u00e9er des token IDs (les positions sont cr\u00e9\u00e9es automatiquement)\n",
    "print(f\"\\n2. Cr\u00e9ation de token IDs:\")\n",
    "token_ids_torch = torch.randint(0, 1000, (batch_size, seq_len))\n",
    "print(f\"   \u2713 Shape: {token_ids_torch.shape}\")\n",
    "\n",
    "# \u00c9tape 3: Forward pass\n",
    "print(f\"\\n3. Forward pass:\")\n",
    "pos_encodings_torch = pos_emb(token_ids_torch)\n",
    "print(f\"   \u2713 Shape: {pos_encodings_torch.shape}\")\n",
    "print(f\"   \u2713 Type: {pos_encodings_torch.dtype}\")\n",
    "print(f\"   \u2713 Device: {pos_encodings_torch.device}\")\n",
    "\n",
    "# \u00c9tape 4: Statistiques\n",
    "print(f\"\\n4. Statistiques:\")\n",
    "print(f\"   - Moyenne: {pos_encodings_torch.mean().item():.6f}\")\n",
    "print(f\"   - \u00c9cart-type: {pos_encodings_torch.std().item():.6f}\")\n",
    "print(f\"   - Min: {pos_encodings_torch.min().item():.6f}\")\n",
    "print(f\"   - Max: {pos_encodings_torch.max().item():.6f}\")\n",
    "print(f\"   - Dans [-1, 1]: {torch.all(torch.abs(pos_encodings_torch) <= 1.0).item()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Impl\u00e9mentation PyTorch termin\u00e9e!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Partie 3: Combinaison des Embeddings\n",
    "\n",
    "## Formule Finale\n",
    "\n",
    "Les embeddings d'entr\u00e9e du transformer sont la **somme** des token embeddings et des positional embeddings:\n",
    "\n",
    "$$E_{\\text{input}} = E_{\\text{token}} + E_{\\text{position}}$$\n",
    "\n",
    "o\u00f9:\n",
    "- $E_{\\text{token}} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{seq\\_len} \\times d_{\\text{model}}}$ encode le **contenu s\u00e9mantique**\n",
    "- $E_{\\text{position}} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{seq\\_len} \\times d_{\\text{model}}}$ encode la **position**\n",
    "- $E_{\\text{input}} \\in \\mathbb{R}^{\\text{batch\\_size} \\times \\text{seq\\_len} \\times d_{\\text{model}}}$ est l'embedding final\n",
    "\n",
    "## Pourquoi l'Addition?\n",
    "\n",
    "L'addition permet au mod\u00e8le de:\n",
    "1. **Pr\u00e9server l'information**: Les deux types d'information coexistent\n",
    "2. **Apprendre \u00e0 s\u00e9parer**: Le mod\u00e8le peut apprendre \u00e0 distinguer contenu et position\n",
    "3. **Simplicit\u00e9**: Plus simple que la concat\u00e9nation (pas de changement de dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# COMBINAISON: Token + Positional\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMBINAISON: TOKEN + POSITIONAL EMBEDDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "vocab_size = 1000\n",
    "max_seq_len = 100\n",
    "d_model = 128\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  - Vocabulaire: {vocab_size}\")\n",
    "print(f\"  - Longueur max: {max_seq_len}\")\n",
    "print(f\"  - Dimension: {d_model}\")\n",
    "print(f\"  - Batch: {batch_size}\")\n",
    "print(f\"  - S\u00e9quence: {seq_len}\")\n",
    "\n",
    "# Cr\u00e9er les deux couches\n",
    "print(f\"\\n1. Cr\u00e9ation des couches:\")\n",
    "token_emb = TokenEmbedding(vocab_size=vocab_size, d_model=d_model)\n",
    "pos_emb = PositionalEmbedding(max_seq_len=max_seq_len, d_model=d_model)\n",
    "print(f\"   \u2713 TokenEmbedding cr\u00e9\u00e9\")\n",
    "print(f\"   \u2713 PositionalEmbedding cr\u00e9\u00e9\")\n",
    "\n",
    "# Cr\u00e9er des token IDs\n",
    "print(f\"\\n2. Cr\u00e9ation de token IDs:\")\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"   \u2713 Shape: {token_ids.shape}\")\n",
    "\n",
    "# Calculer les embeddings de tokens\n",
    "print(f\"\\n3. Calcul des token embeddings:\")\n",
    "token_embeddings = token_emb(token_ids)\n",
    "print(f\"   \u2713 Shape: {token_embeddings.shape}\")\n",
    "\n",
    "# Calculer les encodages positionnels\n",
    "print(f\"\\n4. Calcul des positional encodings:\")\n",
    "positional_encodings = pos_emb(token_ids)\n",
    "print(f\"   \u2713 Shape: {positional_encodings.shape}\")\n",
    "\n",
    "# Combiner par addition\n",
    "print(f\"\\n5. Combinaison par addition:\")\n",
    "combined_embeddings = token_embeddings + positional_encodings\n",
    "print(f\"   \u2713 Shape: {combined_embeddings.shape}\")\n",
    "print(f\"   \u2713 Formule: E_input = E_token + E_position\")\n",
    "\n",
    "# V\u00e9rification\n",
    "print(f\"\\n6. V\u00e9rification:\")\n",
    "expected_shape = (batch_size, seq_len, d_model)\n",
    "print(f\"   - Shape attendue: {expected_shape}\")\n",
    "print(f\"   - Shape obtenue: {combined_embeddings.shape}\")\n",
    "assert combined_embeddings.shape == expected_shape\n",
    "print(f\"   \u2713 SUCC\u00c8S: Les embeddings sont correctement combin\u00e9s!\")\n",
    "\n",
    "# Statistiques\n",
    "print(f\"\\n7. Statistiques des embeddings combin\u00e9s:\")\n",
    "print(f\"   - Moyenne: {combined_embeddings.mean().item():.6f}\")\n",
    "print(f\"   - \u00c9cart-type: {combined_embeddings.std().item():.6f}\")\n",
    "print(f\"   - Min: {combined_embeddings.min().item():.6f}\")\n",
    "print(f\"   - Max: {combined_embeddings.max().item():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\u2713 Combinaison termin\u00e9e avec succ\u00e8s!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation de l'Espace d'Embedding\n",
    "\n",
    "Visualisons l'espace d'embedding en r\u00e9duisant la dimensionnalit\u00e9 avec **PCA** (Principal Component Analysis) et **t-SNE** (t-Distributed Stochastic Neighbor Embedding).\n",
    "\n",
    "### PCA vs t-SNE\n",
    "\n",
    "- **PCA**: Projection lin\u00e9aire qui pr\u00e9serve la variance globale\n",
    "- **t-SNE**: Projection non-lin\u00e9aire qui pr\u00e9serve les structures locales\n",
    "\n",
    "Ces visualisations nous aident \u00e0 comprendre comment les embeddings sont organis\u00e9s dans l'espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'espace d'embedding avec PCA\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALISATION: ESPACE D'EMBEDDING (PCA)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# R\u00e9cup\u00e9rer la matrice d'embedding compl\u00e8te\n",
    "embedding_matrix_torch = token_emb.get_embedding_matrix().detach().numpy()\n",
    "print(f\"\\nMatrice d'embedding shape: {embedding_matrix_torch.shape}\")\n",
    "\n",
    "# S\u00e9lectionner un sous-ensemble de tokens pour la visualisation\n",
    "num_tokens_to_plot = 200\n",
    "embeddings_subset = embedding_matrix_torch[:num_tokens_to_plot]\n",
    "\n",
    "# Appliquer PCA pour r\u00e9duire \u00e0 2D\n",
    "print(f\"\\nApplication de PCA (128D \u2192 2D)...\")\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d_pca = pca.fit_transform(embeddings_subset)\n",
    "print(f\"\u2713 PCA termin\u00e9e\")\n",
    "print(f\"  - Variance expliqu\u00e9e: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "# Visualisation PCA\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "scatter = ax.scatter(embeddings_2d_pca[:, 0], embeddings_2d_pca[:, 1],\n",
    "                    c=range(num_tokens_to_plot), cmap='viridis',\n",
    "                    alpha=0.6, s=50)\n",
    "\n",
    "ax.set_xlabel('Premi\u00e8re Composante Principale', fontsize=12)\n",
    "ax.set_ylabel('Deuxi\u00e8me Composante Principale', fontsize=12)\n",
    "ax.set_title(f'Espace d\\'Embedding (PCA)\\n{num_tokens_to_plot} premiers tokens',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Token ID', rotation=270, labelpad=20)\n",
    "\n",
    "# Annoter quelques points\n",
    "for i in [0, 50, 100, 150]:\n",
    "    if i < num_tokens_to_plot:\n",
    "        ax.annotate(f'Token {i}', \n",
    "                   xy=(embeddings_2d_pca[i, 0], embeddings_2d_pca[i, 1]),\n",
    "                   xytext=(5, 5), textcoords='offset points',\n",
    "                   fontsize=8, alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Observations:\")\n",
    "print(\"  - Les embeddings sont initialis\u00e9s al\u00e9atoirement\")\n",
    "print(\"  - Apr\u00e8s l'entra\u00eenement, les tokens similaires seraient proches\")\n",
    "print(\"  - PCA capture les directions de plus grande variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation avec t-SNE (plus lent mais capture mieux les structures locales)\n",
    "print(\"=\" * 60)\n",
    "print(\"VISUALISATION: ESPACE D'EMBEDDING (t-SNE)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Appliquer t-SNE\n",
    "print(f\"\\nApplication de t-SNE (128D \u2192 2D)...\")\n",
    "print(\"\u23f3 Cela peut prendre quelques secondes...\")\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "embeddings_2d_tsne = tsne.fit_transform(embeddings_subset)\n",
    "print(f\"\u2713 t-SNE termin\u00e9e\")\n",
    "\n",
    "# Visualisation t-SNE\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "scatter = ax.scatter(embeddings_2d_tsne[:, 0], embeddings_2d_tsne[:, 1],\n",
    "                    c=range(num_tokens_to_plot), cmap='plasma',\n",
    "                    alpha=0.6, s=50)\n",
    "\n",
    "ax.set_xlabel('Dimension t-SNE 1', fontsize=12)\n",
    "ax.set_ylabel('Dimension t-SNE 2', fontsize=12)\n",
    "ax.set_title(f'Espace d\\'Embedding (t-SNE)\\n{num_tokens_to_plot} premiers tokens',\n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "# Colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Token ID', rotation=270, labelpad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\ud83d\udcca Observations:\")\n",
    "print(\"  - t-SNE pr\u00e9serve mieux les voisinages locaux que PCA\")\n",
    "print(\"  - Les clusters appara\u00eetraient apr\u00e8s l'entra\u00eenement\")\n",
    "print(\"  - Utile pour visualiser les relations s\u00e9mantiques\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## R\u00e9capitulatif: Shape Checks\n",
    "\n",
    "V\u00e9rifions toutes les dimensions \u00e0 travers le pipeline complet d'embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# R\u00c9CAPITULATIF: SHAPE CHECKS COMPLETS\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"R\u00c9CAPITULATIF: SHAPE CHECKS COMPLETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configuration\n",
    "vocab_size = 1000\n",
    "max_seq_len = 100\n",
    "d_model = 128\n",
    "batch_size = 4\n",
    "seq_len = 10\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  vocab_size = {vocab_size}\")\n",
    "print(f\"  max_seq_len = {max_seq_len}\")\n",
    "print(f\"  d_model = {d_model}\")\n",
    "print(f\"  batch_size = {batch_size}\")\n",
    "print(f\"  seq_len = {seq_len}\")\n",
    "\n",
    "# Pipeline complet\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE COMPLET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# \u00c9tape 1: Token IDs\n",
    "print(f\"\\n1. Token IDs (input):\")\n",
    "token_ids = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "print(f\"   Shape: {token_ids.shape}\")\n",
    "print(f\"   \u2713 ({batch_size}, {seq_len})\")\n",
    "\n",
    "# \u00c9tape 2: Token Embeddings\n",
    "print(f\"\\n2. Token Embeddings:\")\n",
    "token_emb = TokenEmbedding(vocab_size, d_model)\n",
    "token_embeddings = token_emb(token_ids)\n",
    "print(f\"   Shape: {token_embeddings.shape}\")\n",
    "print(f\"   \u2713 ({batch_size}, {seq_len}, {d_model})\")\n",
    "\n",
    "# \u00c9tape 3: Positional Encodings\n",
    "print(f\"\\n3. Positional Encodings:\")\n",
    "pos_emb = PositionalEmbedding(max_seq_len, d_model)\n",
    "positional_encodings = pos_emb(token_ids)\n",
    "print(f\"   Shape: {positional_encodings.shape}\")\n",
    "print(f\"   \u2713 ({batch_size}, {seq_len}, {d_model})\")\n",
    "\n",
    "# \u00c9tape 4: Combined Embeddings\n",
    "print(f\"\\n4. Combined Embeddings (Token + Positional):\")\n",
    "combined_embeddings = token_embeddings + positional_encodings\n",
    "print(f\"   Shape: {combined_embeddings.shape}\")\n",
    "print(f\"   \u2713 ({batch_size}, {seq_len}, {d_model})\")\n",
    "\n",
    "# V\u00e9rification finale\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"V\u00c9RIFICATION FINALE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "expected_shape = (batch_size, seq_len, d_model)\n",
    "print(f\"\\nShape attendue: {expected_shape}\")\n",
    "print(f\"Shape obtenue: {combined_embeddings.shape}\")\n",
    "\n",
    "if combined_embeddings.shape == expected_shape:\n",
    "    print(f\"\\n\u2705 SUCC\u00c8S: Toutes les dimensions sont correctes!\")\n",
    "    print(f\"\\nLes embeddings sont pr\u00eats \u00e0 \u00eatre utilis\u00e9s par le transformer!\")\n",
    "else:\n",
    "    print(f\"\\n\u274c ERREUR: Les dimensions ne correspondent pas!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# R\u00e9sum\u00e9 et Prochaines \u00c9tapes\n",
    "\n",
    "## Ce que nous avons appris\n",
    "\n",
    "### 1. Token Embeddings\n",
    "- Convertissent des IDs discrets en vecteurs denses\n",
    "- Matrice de shape `(vocab_size, d_model)`\n",
    "- Poids **apprenables** (mis \u00e0 jour pendant l'entra\u00eenement)\n",
    "- Capturent le **contenu s\u00e9mantique** des tokens\n",
    "\n",
    "### 2. Positional Embeddings\n",
    "- Encodent la position des tokens dans la s\u00e9quence\n",
    "- Utilisation de fonctions sinuso\u00efdales: $\\sin$ et $\\cos$\n",
    "- Poids **fixes** (calcul\u00e9s une fois, jamais modifi\u00e9s)\n",
    "- Permettent au mod\u00e8le de comprendre l'**ordre des mots**\n",
    "\n",
    "### 3. Combinaison\n",
    "- Addition simple: $E_{\\text{input}} = E_{\\text{token}} + E_{\\text{position}}$\n",
    "- Pr\u00e9serve la dimension: `(batch_size, seq_len, d_model)`\n",
    "- Les deux types d'information coexistent\n",
    "\n",
    "### 4. Visualisations\n",
    "- **Heatmap**: Patterns sinuso\u00efdaux des encodages positionnels\n",
    "- **PCA**: Structure globale de l'espace d'embedding\n",
    "- **t-SNE**: Structures locales et voisinages\n",
    "\n",
    "## Formules Cl\u00e9s\n",
    "\n",
    "**Token Embedding:**\n",
    "$$\\text{embedding}(i) = E[i] \\in \\mathbb{R}^{d_{\\text{model}}}$$\n",
    "\n",
    "**Positional Encoding:**\n",
    "$$PE_{(\\text{pos}, 2i)} = \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "$$PE_{(\\text{pos}, 2i+1)} = \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
    "\n",
    "**Embedding Final:**\n",
    "$$E_{\\text{input}} = E_{\\text{token}} + E_{\\text{position}}$$\n",
    "\n",
    "## Prochaines \u00c9tapes\n",
    "\n",
    "Maintenant que nous avons les embeddings d'entr\u00e9e, nous pouvons passer au **m\u00e9canisme d'attention** (Hour 2):\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: Comment les tokens s'attendent mutuellement\n",
    "2. **Causal Masking**: Emp\u00eacher le mod\u00e8le de voir le futur\n",
    "3. **Multi-Head Attention**: Capturer diff\u00e9rentes relations en parall\u00e8le\n",
    "\n",
    "## Exercices Sugg\u00e9r\u00e9s\n",
    "\n",
    "1. **Modifier les dimensions**: Essayez diff\u00e9rentes valeurs de `d_model` (64, 256, 512)\n",
    "2. **Visualiser plus de tokens**: Augmentez `num_tokens_to_plot` dans les visualisations\n",
    "3. **Comparer les initialisations**: Essayez diff\u00e9rentes graines al\u00e9atoires\n",
    "4. **Analyser les fr\u00e9quences**: Tracez les valeurs d'encodage positionnel pour diff\u00e9rentes dimensions\n",
    "5. **Exp\u00e9rimenter avec seq_len**: Observez comment les encodages changent pour des s\u00e9quences plus longues\n",
    "\n",
    "---\n",
    "\n",
    "**F\u00e9licitations! Vous avez compl\u00e9t\u00e9 le notebook sur les Embeddings! \ud83c\udf89**\n",
    "\n",
    "Passez au notebook suivant: `02_attention/01_scaled_dot_product_attention.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
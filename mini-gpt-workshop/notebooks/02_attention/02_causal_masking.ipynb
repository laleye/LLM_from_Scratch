{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Masque Causal et Attention Autoregressive\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Bienvenue dans ce notebook sur le **masque causal**, le mécanisme qui distingue GPT de BERT!\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "\n",
        "Dans ce notebook, vous allez:\n",
        "1. Comprendre le concept de masque causal (triangulaire)\n",
        "2. Implémenter le masque from scratch avec NumPy\n",
        "3. Implémenter le masque avec PyTorch (`torch.tril()`)\n",
        "4. Visualiser l'effet du masque sur l'attention\n",
        "5. Comparer BERT (bidirectionnel) et GPT (causal)\n",
        "6. Comprendre la génération autoregressive\n",
        "\n",
        "### Qu'est-ce qu'un masque causal?\n",
        "\n",
        "Un **masque causal** est une matrice triangulaire qui empêche chaque token de voir les tokens futurs. C'est le mécanisme fondamental des modèles génératifs comme GPT.\n",
        "\n",
        "**Analogie:** Imaginez que vous lisez un livre mot par mot. À chaque instant, vous ne pouvez voir que les mots que vous avez déjà lus, pas ceux qui viennent après. C'est exactement ce que fait le masque causal!\n",
        "\n",
        "**Pourquoi est-ce important?**\n",
        "- **GPT (génération):** Doit prédire le prochain mot sans voir le futur → masque causal\n",
        "- **BERT (compréhension):** Peut voir toute la phrase pour comprendre → pas de masque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Formule Mathématique du Masque Causal\n",
        "\n",
        "### Définition du Masque\n",
        "\n",
        "Le masque causal est une matrice $M \\in \\{0, 1\\}^{n \\times n}$ définie par:\n",
        "\n",
        "$M_{ij} = \\begin{cases} 1 & \\text{si } j \\leq i \\\\ 0 & \\text{si } j > i \\end{cases}$\n",
        "\n",
        "**Interprétation:**\n",
        "- $M_{ij} = 1$ signifie: \"le token $i$ peut voir le token $j$\"\n",
        "- $M_{ij} = 0$ signifie: \"le token $i$ ne peut PAS voir le token $j$\"\n",
        "- Condition $j \\leq i$: on ne peut voir que le passé et le présent\n",
        "\n",
        "### Exemple pour n=5\n",
        "\n",
        "$M = \\begin{bmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 1 & 1 & 0 & 0 & 0 \\\\ 1 & 1 & 1 & 0 & 0 \\\\ 1 & 1 & 1 & 1 & 0 \\\\ 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$\n",
        "\n",
        "C'est une **matrice triangulaire inférieure** (lower triangular matrix).\n",
        "\n",
        "### Application dans l'Attention\n",
        "\n",
        "Le masque est appliqué aux scores d'attention AVANT le softmax:\n",
        "\n",
        "$\\text{scores}_{ij} = \\begin{cases} \\frac{Q_i K_j^T}{\\sqrt{d_k}} & \\text{si } M_{ij} = 1 \\\\ -\\infty & \\text{si } M_{ij} = 0 \\end{cases}$\n",
        "\n",
        "**Pourquoi $-\\infty$?**\n",
        "\n",
        "$\\text{softmax}(-\\infty) = \\frac{e^{-\\infty}}{\\sum e^{\\cdot}} = \\frac{0}{\\sum e^{\\cdot}} = 0$\n",
        "\n",
        "Donc les positions masquées auront une attention nulle!\n",
        "\n",
        "### Formule Complète avec Masque\n",
        "\n",
        "$\\text{Attention}(Q, K, V, M) = \\text{softmax}\\left(\\text{mask}\\left(\\frac{QK^T}{\\sqrt{d_k}}, M\\right)\\right)V$\n",
        "\n",
        "Où $\\text{mask}(S, M)$ remplace les positions $M_{ij}=0$ par $-\\infty$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Ajouter le chemin vers src/\n",
        "sys.path.append('../..')\n",
        "\n",
        "# Importer nos modules\n",
        "from src.attention.masking import (\n",
        "    create_causal_mask_from_scratch,\n",
        "    create_causal_mask_manual,\n",
        "    create_causal_mask,\n",
        "    visualize_causal_mask,\n",
        "    compare_attention_patterns,\n",
        "    visualize_mask_effect_on_attention\n",
        ")\n",
        "from src.attention.scaled_dot_product import ScaledDotProductAttention\n",
        "\n",
        "# Configuration pour les visualisations\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Seed pour la reproductibilité\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"✓ Imports réussis!\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ Device disponible: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implémentation From Scratch (NumPy)\n",
        "\n",
        "Commençons par créer un masque causal avec NumPy pour comprendre la structure.\n",
        "\n",
        "### 2.1 Méthode 1: Avec np.tril() (Recommandé)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_causal_mask_numpy(seq_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Crée un masque causal avec np.tril() (triangular lower).\n",
        "    \n",
        "    np.tril() retourne la partie triangulaire inférieure d'une matrice.\n",
        "    C'est la méthode la plus simple et efficace.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Longueur de la séquence\n",
        "    \n",
        "    Returns:\n",
        "        Masque causal (seq_len, seq_len)\n",
        "        1 = autoriser l'attention, 0 = bloquer l'attention\n",
        "    \"\"\"\n",
        "    # np.tril() garde seulement la partie triangulaire inférieure\n",
        "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
        "    return mask\n",
        "\n",
        "# Test avec seq_len = 6\n",
        "seq_len = 6\n",
        "mask = create_causal_mask_numpy(seq_len)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MASQUE CAUSAL (NumPy)\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nShape: {mask.shape}\")\n",
        "print(f\"\\nMasque (1=autorisé, 0=bloqué):\\n\")\n",
        "print(mask)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"INTERPRÉTATION\")\n",
        "print(\"-\" * 60)\n",
        "for i in range(seq_len):\n",
        "    visible_positions = np.where(mask[i] == 1)[0]\n",
        "    print(f\"Token {i} peut voir les positions: {list(visible_positions)} \"\n",
        "          f\"(total: {len(visible_positions)} tokens)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Méthode 2: Construction Manuelle (Pédagogique)\n",
        "\n",
        "Pour bien comprendre, construisons le masque manuellement avec des boucles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_causal_mask_manual_demo(seq_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Crée un masque causal manuellement (version pédagogique).\n",
        "    \n",
        "    Cette version montre explicitement la logique avec des boucles.\n",
        "    \"\"\"\n",
        "    # Initialiser une matrice de zéros\n",
        "    mask = np.zeros((seq_len, seq_len))\n",
        "    \n",
        "    # Remplir la partie triangulaire inférieure\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            if j <= i:  # Position j est dans le passé ou présent\n",
        "                mask[i, j] = 1.0\n",
        "            # else: mask[i, j] reste 0 (futur bloqué)\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# Comparer les deux méthodes\n",
        "mask_tril = create_causal_mask_numpy(5)\n",
        "mask_manual = create_causal_mask_manual_demo(5)\n",
        "\n",
        "print(\"Méthode 1 (np.tril):\")\n",
        "print(mask_tril)\n",
        "print(\"\\nMéthode 2 (manuelle):\")\n",
        "print(mask_manual)\n",
        "print(f\"\\nLes deux méthodes donnent le même résultat? {np.allclose(mask_tril, mask_manual)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implémentation PyTorch\n",
        "\n",
        "Maintenant, créons le masque avec PyTorch en utilisant `torch.tril()`.\n",
        "\n",
        "### 3.1 Fonction avec torch.tril()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_causal_mask_pytorch(seq_len: int, device: torch.device = torch.device('cpu')) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Crée un masque causal avec PyTorch.\n",
        "    \n",
        "    torch.tril() - Triangular Lower:\n",
        "        - Retourne la partie triangulaire inférieure d'une matrice\n",
        "        - Tous les éléments au-dessus de la diagonale sont mis à 0\n",
        "        - Optimisé GPU et différentiable\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Longueur de la séquence\n",
        "        device: Device PyTorch (cpu ou cuda)\n",
        "    \n",
        "    Returns:\n",
        "        Masque causal (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # torch.tril() retourne la partie triangulaire inférieure\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))\n",
        "    return mask\n",
        "\n",
        "# Test sur CPU\n",
        "print(\"=\" * 60)\n",
        "print(\"MASQUE CAUSAL (PyTorch)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "mask_torch = create_causal_mask_pytorch(6)\n",
        "print(f\"\\nShape: {mask_torch.shape}\")\n",
        "print(f\"Device: {mask_torch.device}\")\n",
        "print(f\"Dtype: {mask_torch.dtype}\")\n",
        "print(f\"\\nMasque:\\n{mask_torch}\")\n",
        "\n",
        "# Test sur GPU si disponible\n",
        "if torch.cuda.is_available():\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Test sur GPU\")\n",
        "    print(\"-\" * 60)\n",
        "    mask_gpu = create_causal_mask_pytorch(6, device=torch.device('cuda'))\n",
        "    print(f\"Device: {mask_gpu.device}\")\n",
        "    print(f\"CPU vs GPU identiques? {torch.allclose(mask_torch, mask_gpu.cpu())}\")\n",
        "else:\n",
        "    print(\"\\nGPU non disponible, test sur CPU uniquement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualisation du Masque Causal\n",
        "\n",
        "Visualisons le masque sous forme de heatmap pour mieux comprendre sa structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiser le masque causal\n",
        "visualize_causal_mask(seq_len=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interprétation de la Visualisation\n",
        "\n",
        "**Observations:**\n",
        "- La partie **triangulaire inférieure** (vert) = attention autorisée\n",
        "- La partie **triangulaire supérieure** (rouge) = attention bloquée\n",
        "- La **diagonale** = chaque token peut se voir lui-même\n",
        "\n",
        "**Lecture:**\n",
        "- **Ligne i** = ce que le token i peut voir\n",
        "- **Colonne j** = qui peut voir le token j\n",
        "\n",
        "**Exemple:**\n",
        "- Token 0: ne voit que lui-même (1 token visible)\n",
        "- Token 3: voit tokens 0, 1, 2, 3 (4 tokens visibles)\n",
        "- Token 7: voit tous les tokens 0-7 (8 tokens visibles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Effet du Masque sur l'Attention\n",
        "\n",
        "Voyons comment le masque transforme les scores d'attention.\n",
        "\n",
        "### 5.1 Attention Sans Masque vs Avec Masque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer des données de test\n",
        "torch.manual_seed(42)\n",
        "batch_size = 1\n",
        "seq_len_test = 5\n",
        "d_k = 8\n",
        "\n",
        "Q = torch.randn(batch_size, seq_len_test, d_k)\n",
        "K = torch.randn(batch_size, seq_len_test, d_k)\n",
        "V = torch.randn(batch_size, seq_len_test, d_k)\n",
        "\n",
        "# Créer le module d'attention\n",
        "attention = ScaledDotProductAttention(d_k=d_k)\n",
        "\n",
        "# Créer le masque causal\n",
        "causal_mask = create_causal_mask_pytorch(seq_len_test)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"COMPARAISON: SANS MASQUE vs AVEC MASQUE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Attention SANS masque\n",
        "print(\"\\n1. Attention SANS masque (bidirectionnelle - BERT)\")\n",
        "print(\"-\" * 60)\n",
        "output_no_mask, weights_no_mask = attention(Q, K, V, mask=None)\n",
        "print(\"Poids d'attention (tous les tokens se voient):\")\n",
        "print(weights_no_mask[0])\n",
        "\n",
        "# Attention AVEC masque causal\n",
        "print(\"\\n2. Attention AVEC masque causal (autorégressif - GPT)\")\n",
        "print(\"-\" * 60)\n",
        "output_masked, weights_masked = attention(Q, K, V, mask=causal_mask)\n",
        "print(\"Poids d'attention (masque causal appliqué):\")\n",
        "print(weights_masked[0])\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"OBSERVATIONS\")\n",
        "print(\"=\" * 60)\n",
        "print(\"✓ Sans masque: Tous les poids sont non-nuls\")\n",
        "print(\"✓ Avec masque: La partie supérieure droite est nulle (futur bloqué)\")\n",
        "print(\"✓ Chaque token ne voit que lui-même et les tokens précédents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Statistiques Détaillées"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyser les différences\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STATISTIQUES PAR POSITION\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\n{'Position':<10} {'Sans masque':<15} {'Avec masque':<15} {'Différence'}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i in range(seq_len_test):\n",
        "    visible_no_mask = (weights_no_mask[0, i] > 0).sum().item()\n",
        "    visible_masked = (weights_masked[0, i] > 0).sum().item()\n",
        "    diff = visible_no_mask - visible_masked\n",
        "    print(f\"Token {i:<5} {visible_no_mask:<15} {visible_masked:<15} {diff}\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 60)\n",
        "print(\"INTERPRÉTATION\")\n",
        "print(\"-\" * 60)\n",
        "print(\"• Sans masque: Chaque token voit tous les tokens (5 tokens)\")\n",
        "print(\"• Avec masque: Token i voit (i+1) tokens (positions 0 à i)\")\n",
        "print(\"• Différence: Nombre de tokens futurs bloqués\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Visualisation Complète de l'Effet du Masque\n",
        "\n",
        "Visualisons les 4 étapes: scores bruts → masque → attention sans masque → attention avec masque."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utiliser la fonction de visualisation complète\n",
        "visualize_mask_effect_on_attention(seq_len=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Comparaison BERT vs GPT\n",
        "\n",
        "Comparons côte à côte les deux architectures pour comprendre leur différence fondamentale.\n",
        "\n",
        "### 7.1 Patterns d'Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiser la comparaison BERT vs GPT\n",
        "compare_attention_patterns(seq_len=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Différences Clés\n",
        "\n",
        "| Aspect | BERT | GPT |\n",
        "|--------|------|-----|\n",
        "| **Type d'attention** | Bidirectionnelle | Causale (autoregressive) |\n",
        "| **Masque** | Aucun | Triangulaire inférieur |\n",
        "| **Vision** | Tous les tokens | Seulement passé + présent |\n",
        "| **Tâche principale** | Compréhension | Génération |\n",
        "| **Exemple d'usage** | Classification, Q&A | Génération de texte |\n",
        "| **Architecture** | Encodeur | Décodeur |\n",
        "\n",
        "### 7.3 Code Comparatif"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"CODE COMPARATIF: BERT vs GPT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n# BERT: Attention Bidirectionnelle\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\"\"\\n",
        "# Pas de masque - tous les tokens se voient\n",
        "output_bert = attention(Q, K, V, mask=None)\n",
        "\n",
        "# Utilisation: Comprendre le contexte complet\n",
        "# Exemple: \"Le [MASK] mange la souris\" → prédire \"chat\"\n",
        "# Le modèle voit \"mange\" et \"souris\" pour deviner \"chat\"\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n# GPT: Attention Causale\")\n",
        "print(\"-\" * 60)\n",
        "print(\"\"\"\\n",
        "# Masque causal - empêche de voir le futur\n",
        "causal_mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "output_gpt = attention(Q, K, V, mask=causal_mask)\n",
        "\n",
        "# Utilisation: Générer le prochain token\n",
        "# Exemple: \"Le chat mange\" → prédire \"la\"\n",
        "# Le modèle ne voit que \"Le chat mange\", pas la suite\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RÉSUMÉ\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\n✓ BERT = Encodeur = Bidirectionnel = Compréhension\")\n",
        "print(\"✓ GPT = Décodeur = Causal = Génération\")\n",
        "print(\"✓ La seule différence: présence/absence du masque causal!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Génération Autoregressive\n",
        "\n",
        "Comprenons comment le masque causal permet la génération autoregressive.\n",
        "\n",
        "### 8.1 Concept de Génération Autoregressive\n",
        "\n",
        "**Définition:** Générer un token à la fois, en utilisant les tokens précédents comme contexte.\n",
        "\n",
        "**Processus:**\n",
        "1. Commencer avec un prompt: \"Le chat\"\n",
        "2. Prédire le prochain token: \"mange\"\n",
        "3. Ajouter au contexte: \"Le chat mange\"\n",
        "4. Prédire le suivant: \"la\"\n",
        "5. Continuer jusqu'à un token de fin\n",
        "\n",
        "**Rôle du masque causal:**\n",
        "- À l'étape 2, le modèle ne doit voir que \"Le chat\"\n",
        "- À l'étape 4, le modèle ne doit voir que \"Le chat mange\"\n",
        "- Le masque garantit qu'on ne triche pas en voyant le futur!\n",
        "\n",
        "### 8.2 Simulation de Génération"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_autoregressive_generation():\n",
        "    \"\"\"\n",
        "    Simule la génération autoregressive pour comprendre le rôle du masque.\n",
        "    \"\"\"\n",
        "    # Tokens de la phrase complète (pour simulation)\n",
        "    tokens = [\"Le\", \"chat\", \"mange\", \"la\", \"souris\"]\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"SIMULATION: GÉNÉRATION AUTOREGRESSIVE\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Simuler la génération étape par étape\n",
        "    for step in range(1, len(tokens)):\n",
        "        context = tokens[:step]\n",
        "        next_token = tokens[step]\n",
        "        \n",
        "        print(f\"\\nÉtape {step}:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(f\"  Contexte visible: {' '.join(context)}\")\n",
        "        print(f\"  Longueur du contexte: {len(context)} tokens\")\n",
        "        print(f\"  Token à prédire: '{next_token}'\")\n",
        "        print(f\"  Masque: Token {step-1} peut voir positions 0 à {step-1}\")\n",
        "        \n",
        "        # Visualiser le masque pour cette étape\n",
        "        mask_step = np.tril(np.ones((step, step)))\n",
        "        print(f\"  Masque d'attention ({step}×{step}):\")\n",
        "        print(f\"  {mask_step[-1]}  ← Dernière ligne (token actuel)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"OBSERVATIONS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"✓ À chaque étape, on ne voit que le passé\")\n",
        "    print(\"✓ Le masque grandit avec la séquence\")\n",
        "    print(\"✓ C'est comme lire un livre mot par mot!\")\n",
        "\n",
        "simulate_autoregressive_generation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Exemple Concret avec Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer une séquence d'exemple\n",
        "torch.manual_seed(42)\n",
        "seq_len_gen = 4\n",
        "d_k_gen = 8\n",
        "\n",
        "# Simuler des embeddings pour \"Le chat mange la\"\n",
        "Q_gen = torch.randn(1, seq_len_gen, d_k_gen)\n",
        "K_gen = torch.randn(1, seq_len_gen, d_k_gen)\n",
        "V_gen = torch.randn(1, seq_len_gen, d_k_gen)\n",
        "\n",
        "attention_gen = ScaledDotProductAttention(d_k=d_k_gen)\n",
        "mask_gen = torch.tril(torch.ones(seq_len_gen, seq_len_gen))\n",
        "\n",
        "_, weights_gen = attention_gen(Q_gen, K_gen, V_gen, mask=mask_gen)\n",
        "\n",
        "# Visualiser\n",
        "tokens_gen = [\"Le\", \"chat\", \"mange\", \"la\"]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    weights_gen[0].detach().numpy(),\n",
        "    xticklabels=tokens_gen,\n",
        "    yticklabels=tokens_gen,\n",
        "    cmap='YlOrRd',\n",
        "    annot=True,\n",
        "    fmt='.3f',\n",
        "    cbar_kws={'label': 'Poids d\\'attention'},\n",
        "    vmin=0,\n",
        "    vmax=1\n",
        ")\n",
        "plt.xlabel('Keys (ce qu\\'on regarde)', fontsize=12)\n",
        "plt.ylabel('Queries (qui regarde)', fontsize=12)\n",
        "plt.title('Attention Autoregressive: \"Le chat mange la\"', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInterprétation:\")\n",
        "print(\"-\" * 60)\n",
        "print(\"• 'Le' (ligne 0): ne voit que lui-même\")\n",
        "print(\"• 'chat' (ligne 1): voit 'Le' et 'chat'\")\n",
        "print(\"• 'mange' (ligne 2): voit 'Le', 'chat', 'mange'\")\n",
        "print(\"• 'la' (ligne 3): voit tous les tokens précédents\")\n",
        "print(\"\\n✓ Chaque token utilise tout le contexte disponible (passé)\")\n",
        "print(\"✓ Mais ne triche jamais en voyant le futur!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Exercices Pratiques\n",
        "\n",
        "### Exercice 1: Créer un Masque Personnalisé\n",
        "\n",
        "Créez un masque qui permet à chaque token de voir:\n",
        "- Lui-même\n",
        "- Le token précédent\n",
        "- Mais pas les autres tokens\n",
        "\n",
        "**Exemple pour seq_len=5:**\n",
        "```\n",
        "[[1, 0, 0, 0, 0],\n",
        " [1, 1, 0, 0, 0],\n",
        " [0, 1, 1, 0, 0],\n",
        " [0, 0, 1, 1, 0],\n",
        " [0, 0, 0, 1, 1]]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 1 - Masque personnalisé\n",
        "def create_local_mask(seq_len: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Crée un masque où chaque token voit seulement lui-même et le précédent.\n",
        "    \"\"\"\n",
        "    # Votre code ici\n",
        "    mask = np.zeros((seq_len, seq_len))\n",
        "    # TODO: Remplir le masque\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# Test\n",
        "local_mask = create_local_mask(5)\n",
        "print(\"Masque local:\")\n",
        "print(local_mask)\n",
        "\n",
        "# Visualiser\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(local_mask, annot=True, fmt='.0f', cmap='RdYlGn', cbar=False, square=True)\n",
        "plt.title('Masque Local (fenêtre de 2 tokens)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 2: Analyser l'Impact du Masque\n",
        "\n",
        "Comparez les poids d'attention moyens avec et sans masque."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 2 - Impact du masque\n",
        "\n",
        "# Créer des données aléatoires\n",
        "torch.manual_seed(42)\n",
        "Q_ex = torch.randn(10, 8, 16)  # 10 exemples, seq_len=8, d_k=16\n",
        "K_ex = torch.randn(10, 8, 16)\n",
        "V_ex = torch.randn(10, 8, 16)\n",
        "\n",
        "attention_ex = ScaledDotProductAttention(d_k=16)\n",
        "mask_ex = torch.tril(torch.ones(8, 8))\n",
        "\n",
        "# Calculer l'attention avec et sans masque\n",
        "_, weights_no_mask_ex = attention_ex(Q_ex, K_ex, V_ex, mask=None)\n",
        "_, weights_masked_ex = attention_ex(Q_ex, K_ex, V_ex, mask=mask_ex)\n",
        "\n",
        "# TODO: Calculer les statistiques\n",
        "# - Poids moyen par position\n",
        "# - Variance des poids\n",
        "# - Entropie de la distribution\n",
        "\n",
        "print(\"Statistiques à calculer:\")\n",
        "print(\"1. Poids moyen par position\")\n",
        "print(\"2. Variance des poids d'attention\")\n",
        "print(\"3. Nombre de poids non-nuls par position\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 3: Masque pour Padding\n",
        "\n",
        "Créez un masque qui combine:\n",
        "1. Masque causal (pas de futur)\n",
        "2. Masque de padding (ignorer les tokens de remplissage)\n",
        "\n",
        "**Exemple:** Séquence de longueur 5, mais seulement 3 tokens réels (les 2 derniers sont du padding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 3 - Masque combiné\n",
        "def create_combined_mask(seq_len: int, real_len: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Crée un masque combinant causal et padding.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Longueur totale de la séquence\n",
        "        real_len: Nombre de tokens réels (le reste est du padding)\n",
        "    \n",
        "    Returns:\n",
        "        Masque combiné\n",
        "    \"\"\"\n",
        "    # TODO: Votre code ici\n",
        "    # Indice: Combiner masque causal ET masque de padding\n",
        "    \n",
        "    pass\n",
        "\n",
        "# Test\n",
        "combined_mask = create_combined_mask(seq_len=5, real_len=3)\n",
        "print(\"Masque combiné (causal + padding):\")\n",
        "print(combined_mask)\n",
        "print(\"\\nInterprétation:\")\n",
        "print(\"• Tokens 0-2: tokens réels (masque causal appliqué)\")\n",
        "print(\"• Tokens 3-4: padding (complètement masqués)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Résumé\n",
        "\n",
        "### Ce que nous avons appris\n",
        "\n",
        "1. **Masque causal:**\n",
        "   - Matrice triangulaire inférieure\n",
        "   - Empêche de voir le futur\n",
        "   - Formule: $M_{ij} = 1$ si $j \\leq i$, sinon $0$\n",
        "\n",
        "2. **Implémentation:**\n",
        "   - NumPy: `np.tril(np.ones((n, n)))`\n",
        "   - PyTorch: `torch.tril(torch.ones(n, n))`\n",
        "\n",
        "3. **Application:**\n",
        "   - Remplacer positions masquées par $-\\infty$\n",
        "   - Softmax transforme $-\\infty$ en $0$\n",
        "   - Résultat: attention nulle aux positions futures\n",
        "\n",
        "4. **BERT vs GPT:**\n",
        "   - **BERT:** Pas de masque → bidirectionnel → compréhension\n",
        "   - **GPT:** Masque causal → autorégressif → génération\n",
        "\n",
        "5. **Génération autoregressive:**\n",
        "   - Générer un token à la fois\n",
        "   - Utiliser les tokens précédents comme contexte\n",
        "   - Le masque garantit qu'on ne triche pas!\n",
        "\n",
        "### Formules Clés\n",
        "\n",
        "**Masque causal:**\n",
        "$M_{ij} = \\begin{cases} 1 & \\text{si } j \\leq i \\\\ 0 & \\text{si } j > i \\end{cases}$\n",
        "\n",
        "**Application dans l'attention:**\n",
        "$\\text{Attention}(Q, K, V, M) = \\text{softmax}\\left(\\text{mask}\\left(\\frac{QK^T}{\\sqrt{d_k}}, M\\right)\\right)V$\n",
        "\n",
        "### Prochaines étapes\n",
        "\n",
        "Dans le prochain notebook, nous verrons:\n",
        "- **Multi-Head Attention:** Plusieurs têtes d'attention en parallèle\n",
        "- Comment combiner plusieurs perspectives\n",
        "- L'architecture complète du transformer\n",
        "\n",
        "### Points clés à retenir\n",
        "\n",
        "✓ Le masque causal est une matrice triangulaire inférieure\n",
        "✓ Il empêche chaque token de voir le futur\n",
        "✓ C'est la différence clé entre BERT (encodeur) et GPT (décodeur)\n",
        "✓ Il permet la génération autoregressive\n",
        "✓ Implémentation simple: `torch.tril()` ou `np.tril()`"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}


{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-Head Attention\\n",
        "\\n",
        "## Introduction\\n",
        "\\n",
        "Bienvenue dans ce notebook sur le **Multi-Head Attention**, le mécanisme qui permet aux transformers de capturer différents types de relations en parallèle!\\n",
        "\\n",
        "### Objectifs pédagogiques\\n",
        "\\n",
        "Dans ce notebook, vous allez:\\n",
        "1. Comprendre le concept de multi-head attention\\n",
        "2. Implémenter multi-head attention from scratch avec NumPy\\n",
        "3. Implémenter multi-head attention avec PyTorch\\n",
        "4. Visualiser les différentes têtes d'attention\\n",
        "5. Comprendre comment les têtes se spécialisent\\n",
        "6. Expérimenter avec différents nombres de têtes\\n",
        "\\n",
        "### Qu'est-ce que le Multi-Head Attention?\\n",
        "\\n",
        "Au lieu d'une seule attention, on utilise plusieurs **têtes** (heads) qui regardent différents aspects des relations entre tokens.\\n",
        "\\n",
        "**Analogie:** Imaginez que vous lisez un texte avec plusieurs perspectives:\\n",
        "- **Tête 1:** Relations grammaticales (sujet-verbe, déterminant-nom)\\n",
        "- **Tête 2:** Relations sémantiques (coréférences, synonymes)\\n",
        "- **Tête 3:** Relations positionnelles (proximité, distance)\\n",
        "\\n",
        "Chaque tête peut se spécialiser dans un type de relation différent!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Formules Mathématiques du Multi-Head Attention\\n",
        "\\n",
        "### Formule Complète\\n",
        "\\n",
        "$\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1, ..., \\\\text{head}_h)W^O$\\n",
        "\\n",
        "Où chaque tête est définie par:\\n",
        "\\n",
        "$\\\\text{head}_i = \\\\text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$\\n",
        "\\n",
        "### Décomposition Étape par Étape\\n",
        "\\n",
        "#### Étape 1: Projections Linéaires\\n",
        "\\n",
        "$Q = XW^Q, \\\\quad K = XW^K, \\\\quad V = XW^V$\\n",
        "\\n",
        "Où:\\n",
        "- $X \\\\in \\\\mathbb{R}^{n \\\\times d_{\\\\text{model}}}$ : Input embeddings\\n",
        "- $W^Q, W^K, W^V \\\\in \\\\mathbb{R}^{d_{\\\\text{model}} \\\\times d_{\\\\text{model}}}$ : Matrices de projection\\n",
        "- $Q, K, V \\\\in \\\\mathbb{R}^{n \\\\times d_{\\\\text{model}}}$ : Queries, Keys, Values projetées\\n",
        "\\n",
        "#### Étape 2: Division en Têtes\\n",
        "\\n",
        "$d_k = \\\\frac{d_{\\\\text{model}}}{h}$\\n",
        "\\n",
        "Où $h$ est le nombre de têtes.\\n",
        "\\n",
        "**Reshape:**\\n",
        "$(\\\\text{batch}, n, d_{\\\\text{model}}) \\\\rightarrow (\\\\text{batch}, n, h, d_k) \\\\rightarrow (\\\\text{batch}, h, n, d_k)$\\n",
        "\\n",
        "#### Étape 3: Attention par Tête\\n",
        "\\n",
        "Pour chaque tête $i$:\\n",
        "\\n",
        "$\\\\text{head}_i = \\\\text{softmax}\\\\left(\\\\frac{Q_i K_i^T}{\\\\sqrt{d_k}}\\\\right)V_i$\\n",
        "\\n",
        "Où $Q_i, K_i, V_i \\\\in \\\\mathbb{R}^{n \\\\times d_k}$\\n",
        "\\n",
        "#### Étape 4: Concaténation\\n",
        "\\n",
        "$\\\\text{Concat}(\\\\text{head}_1, ..., \\\\text{head}_h) = [\\\\text{head}_1; \\\\text{head}_2; ...; \\\\text{head}_h]$\\n",
        "\\n",
        "**Dimensions:**\\n",
        "- Avant concat: $h$ têtes de dimension $d_k$\\n",
        "- Après concat: $h \\\\times d_k = d_{\\\\text{model}}$\\n",
        "\\n",
        "#### Étape 5: Projection Finale\\n",
        "\\n",
        "$\\\\text{Output} = \\\\text{Concat}(\\\\text{heads})W^O$\\n",
        "\\n",
        "Où $W^O \\\\in \\\\mathbb{R}^{d_{\\\\text{model}} \\\\times d_{\\\\text{model}}}$\\n",
        "\\n",
        "### Pourquoi Multi-Head?\\n",
        "\\n",
        "**Avantages:**\\n",
        "1. **Diversité:** Chaque tête peut se spécialiser dans différents patterns\\n",
        "2. **Parallélisme:** Toutes les têtes calculent en parallèle\\n",
        "3. **Capacité:** Plus de paramètres = plus de capacité d'apprentissage\\n",
        "4. **Robustesse:** Si une tête échoue, les autres compensent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\\n",
        "import numpy as np\\n",
        "import math\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "import torch.nn.functional as F\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "import sys\\n",
        "from typing import Tuple, Optional\\n",
        "\\n",
        "# Ajouter le chemin vers src/\\n",
        "sys.path.append('../..')\\n",
        "\\n",
        "# Importer nos modules\\n",
        "from src.attention.multi_head import (\\n",
        "    multi_head_attention_from_scratch,\\n",
        "    MultiHeadAttention\\n",
        ")\\n",
        "from src.attention.scaled_dot_product import (\\n",
        "    scaled_dot_product_attention_from_scratch,\\n",
        "    ScaledDotProductAttention\\n",
        ")\\n",
        "\\n",
        "# Configuration pour les visualisations\\n",
        "plt.style.use('default')\\n",
        "sns.set_palette(\\\"husl\\\")\\n",
        "\\n",
        "# Seed pour la reproductibilité\\n",
        "np.random.seed(42)\\n",
        "torch.manual_seed(42)\\n",
        "\\n",
        "print(\\\"✓ Imports réussis!\\\")\\n",
        "print(f\\\"✓ PyTorch version: {torch.__version__}\\\")\\n",
        "print(f\\\"✓ Device disponible: {'GPU' if torch.cuda.is_available() else 'CPU'}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implémentation From Scratch (NumPy)\\n",
        "\\n",
        "Commençons par implémenter multi-head attention avec NumPy pour comprendre chaque opération.\\n",
        "\\n",
        "### 2.1 Exemple avec 3 Tokens et 2 Têtes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paramètres pour l'exemple\\n",
        "batch_size = 1\\n",
        "seq_len = 3      # 3 tokens\\n",
        "d_model = 8      # dimension du modèle\\n",
        "num_heads = 2    # 2 têtes d'attention\\n",
        "\\n",
        "# Créer l'input\\n",
        "np.random.seed(42)\\n",
        "x = np.random.randn(batch_size, seq_len, d_model)\\n",
        "\\n",
        "# Créer les matrices de projection\\n",
        "W_q = np.random.randn(d_model, d_model) * 0.1\\n",
        "W_k = np.random.randn(d_model, d_model) * 0.1\\n",
        "W_v = np.random.randn(d_model, d_model) * 0.1\\n",
        "W_o = np.random.randn(d_model, d_model) * 0.1\\n",
        "\\n",
        "print(\\\"=\\\" * 70)\\n",
        "print(\\\"EXEMPLE: Multi-Head Attention from Scratch\\\")\\n",
        "print(\\\"=\\\" * 70)\\n",
        "print(f\\\"\\\\nConfiguration:\\\")\\n",
        "print(f\\\"  - batch_size: {batch_size}\\\")\\n",
        "print(f\\\"  - seq_len: {seq_len}\\\")\\n",
        "print(f\\\"  - d_model: {d_model}\\\")\\n",
        "print(f\\\"  - num_heads: {num_heads}\\\")\\n",
        "print(f\\\"  - d_k (par tête): {d_model // num_heads}\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculer multi-head attention from scratch\\n",
        "output, attention_weights = multi_head_attention_from_scratch(\\n",
        "    x, W_q, W_k, W_v, W_o, num_heads\\n",
        ")\\n",
        "\\n",
        "print(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n",
        "print(\\\"RÉSULTATS\\\")\\n",
        "print(\\\"=\\\" * 70)\\n",
        "print(f\\\"\\\\nOutput shape: {output.shape}\\\")\\n",
        "print(f\\\"Nombre de têtes: {len(attention_weights)}\\\")\\n",
        "\\n",
        "print(f\\\"\\\\nPoids d'attention par tête:\\\")\\n",
        "for i, weights in enumerate(attention_weights):\\n",
        "    print(f\\\"\\\\n  Tête {i+1}:\\\")\\n",
        "    print(f\\\"    Shape: {weights.shape}\\\")\\n",
        "    print(f\\\"    Poids (batch 0):\\\")\\n",
        "    print(f\\\"{weights[0]}\\\")\\n",
        "    \\n",
        "    # Vérifier la normalisation\\n",
        "    row_sums = weights[0].sum(axis=-1)\\n",
        "    print(f\\\"    Somme par ligne: {row_sums}\\\")\\n",
        "    print(f\\\"    Normalisé? {np.allclose(row_sums, 1.0)}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implémentation PyTorch (Professionnelle)\\n",
        "\\n",
        "Maintenant, utilisons PyTorch pour une implémentation optimisée et parallèle.\\n",
        "\\n",
        "### 3.1 Classe MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer des tensors PyTorch\\n",
        "torch.manual_seed(42)\\n",
        "x_torch = torch.randn(batch_size, seq_len, d_model)\\n",
        "\\n",
        "print(\\\"=\\\" * 70)\\n",
        "print(\\\"EXEMPLE: Multi-Head Attention avec PyTorch\\\")\\n",
        "print(\\\"=\\\" * 70)\\n",
        "print(f\\\"\\\\nInput shape: {x_torch.shape}\\\")\\n",
        "print(f\\\"Device: {x_torch.device}\\\")\\n",
        "\\n",
        "# Créer le module Multi-Head Attention\\n",
        "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\\n",
        "\\n",
        "print(f\\\"\\\\nModèle créé:\\\")\\n",
        "print(f\\\"  - Paramètres: {sum(p.numel() for p in mha.parameters())}\\\")\\n",
        "\\n",
        "# Calculer multi-head attention\\n",
        "output_torch = mha(x_torch)\\n",
        "\\n",
        "print(f\\\"\\\\nOutput shape: {output_torch.shape}\\\")\\n",
        "print(f\\\"Dimension préservée? {x_torch.shape == output_torch.shape}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualisation des Têtes d'Attention\\n",
        "\\n",
        "Visualisons les poids d'attention de chaque tête pour comprendre comment elles se spécialisent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extraire les poids d'attention\\n",
        "attention_weights_torch = mha.get_attention_weights(x_torch)\\n",
        "\\n",
        "# Visualiser chaque tête\\n",
        "tokens = [\\\"Token 0\\\", \\\"Token 1\\\", \\\"Token 2\\\"]\\n",
        "\\n",
        "fig, axes = plt.subplots(1, num_heads, figsize=(12, 4))\\n",
        "\\n",
        "for i in range(num_heads):\\n",
        "    ax = axes[i] if num_heads > 1 else axes\\n",
        "    \\n",
        "    sns.heatmap(\\n",
        "        attention_weights_torch[0, i].detach().numpy(),\\n",
        "        ax=ax,\\n",
        "        xticklabels=tokens,\\n",
        "        yticklabels=tokens,\\n",
        "        cmap='YlOrRd',\\n",
        "        annot=True,\\n",
        "        fmt='.3f',\\n",
        "        cbar_kws={'label': 'Poids'},\\n",
        "        vmin=0,\\n",
        "        vmax=1\\n",
        "    )\\n",
        "    ax.set_title(f'Tête {i+1}', fontweight='bold')\\n",
        "    ax.set_xlabel('Keys')\\n",
        "    ax.set_ylabel('Queries')\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "print(\\\"\\\\nObservation:\\\")\\n",
        "print(\\\"  - Chaque tête a des patterns d'attention différents\\\")\\n",
        "print(\\\"  - Les têtes peuvent se spécialiser dans différents types de relations\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Vérification des Dimensions (Shape Checks)\\n",
        "\\n",
        "Vérifions que les dimensions sont correctes à chaque étape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\\\"=\\\" * 70)\\n",
        "print(\\\"VÉRIFICATION DES DIMENSIONS\\\")\\n",
        "print(\\\"=\\\" * 70)\\n",
        "\\n",
        "# Test avec différentes configurations\\n",
        "test_configs = [\\n",
        "    (2, 4, 16, 2),   # batch=2, seq=4, d_model=16, heads=2\\n",
        "    (1, 8, 32, 4),   # batch=1, seq=8, d_model=32, heads=4\\n",
        "    (4, 10, 64, 8),  # batch=4, seq=10, d_model=64, heads=8\\n",
        "]\\n",
        "\\n",
        "for batch, seq, d_model, heads in test_configs:\\n",
        "    print(f\\\"\\\\nTest: batch={batch}, seq={seq}, d_model={d_model}, heads={heads}\\\")\\n",
        "    print(\\\"-\\\" * 70)\\n",
        "    \\n",
        "    # Créer input\\n",
        "    x_test = torch.randn(batch, seq, d_model)\\n",
        "    \\n",
        "    # Créer module\\n",
        "    mha_test = MultiHeadAttention(d_model=d_model, num_heads=heads)\\n",
        "    \\n",
        "    # Forward pass\\n",
        "    output_test = mha_test(x_test)\\n",
        "    \\n",
        "    # Vérifier\\n",
        "    print(f\\\"  Input shape:  {x_test.shape}\\\")\\n",
        "    print(f\\\"  Output shape: {output_test.shape}\\\")\\n",
        "    print(f\\\"  ✓ Dimension préservée: {x_test.shape == output_test.shape}\\\")\\n",
        "    \\n",
        "    # Vérifier les poids d'attention\\n",
        "    weights_test = mha_test.get_attention_weights(x_test)\\n",
        "    print(f\\\"  Attention weights shape: {weights_test.shape}\\\")\\n",
        "    print(f\\\"  ✓ Expected: ({batch}, {heads}, {seq}, {seq})\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercices Pratiques\\n",
        "\\n",
        "### Exercice 1: Expérimenter avec Différents Nombres de Têtes\\n",
        "\\n",
        "Testez multi-head attention avec 1, 2, 4, et 8 têtes. Observez comment les patterns changent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 1 - Expérimenter avec différents nombres de têtes\\n",
        "\\n",
        "d_model_ex = 16\\n",
        "seq_len_ex = 5\\n",
        "head_counts = [1, 2, 4, 8]\\n",
        "\\n",
        "torch.manual_seed(42)\\n",
        "x_ex = torch.randn(1, seq_len_ex, d_model_ex)\\n",
        "\\n",
        "print(\\\"Comparaison de différents nombres de têtes:\\\")\\n",
        "print(\\\"=\\\" * 70)\\n",
        "\\n",
        "for num_heads_ex in head_counts:\\n",
        "    print(f\\\"\\\\nNombre de têtes: {num_heads_ex}\\\")\\n",
        "    print(\\\"-\\\" * 70)\\n",
        "    \\n",
        "    # Créer le module\\n",
        "    mha_ex = MultiHeadAttention(d_model=d_model_ex, num_heads=num_heads_ex)\\n",
        "    \\n",
        "    # Forward pass\\n",
        "    output_ex = mha_ex(x_ex)\\n",
        "    \\n",
        "    # Statistiques\\n",
        "    params = sum(p.numel() for p in mha_ex.parameters())\\n",
        "    print(f\\\"  d_k (par tête): {d_model_ex // num_heads_ex}\\\")\\n",
        "    print(f\\\"  Paramètres: {params}\\\")\\n",
        "    print(f\\\"  Output shape: {output_ex.shape}\\\")\\n",
        "\\n",
        "print(\\\"\\\\n✓ Observation: Plus de têtes = plus de paramètres = plus de capacité\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Résumé\\n",
        "\\n",
        "### Ce que nous avons appris\\n",
        "\\n",
        "1. **Formule du Multi-Head Attention:**\\n",
        "   - $\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(\\\\text{head}_1, ..., \\\\text{head}_h)W^O$\\n",
        "   - Chaque tête calcule son attention indépendamment\\n",
        "\\n",
        "2. **Les 5 étapes:**\\n",
        "   - Projections linéaires (Q, K, V)\\n",
        "   - Division en têtes (reshape + transpose)\\n",
        "   - Attention par tête (parallèle)\\n",
        "   - Concaténation des têtes\\n",
        "   - Projection finale\\n",
        "\\n",
        "3. **Avantages:**\\n",
        "   - **Diversité:** Chaque tête se spécialise\\n",
        "   - **Parallélisme:** Calcul efficace\\n",
        "   - **Capacité:** Plus de paramètres\\n",
        "   - **Robustesse:** Redondance\\n",
        "\\n",
        "4. **Dimension préservée:**\\n",
        "   - Input: (batch, seq_len, d_model)\\n",
        "   - Output: (batch, seq_len, d_model)\\n",
        "   - Propriété importante des transformers!\\n",
        "\\n",
        "### Prochaines étapes\\n",
        "\\n",
        "Dans le prochain notebook, nous assemblerons tous ces composants pour créer un TransformerBlock complet!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
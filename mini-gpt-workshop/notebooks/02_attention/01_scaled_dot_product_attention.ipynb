{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scaled Dot-Product Attention\\n",
        "\\n",
        "## Introduction\\n",
        "\\n",
        "Bienvenue dans ce notebook sur le **m\u00e9canisme d'attention**, le c\u0153ur des transformers!\\n",
        "\\n",
        "### Objectifs p\u00e9dagogiques\\n",
        "\\n",
        "Dans ce notebook, vous allez:\\n",
        "1. Comprendre la formule math\u00e9matique de l'attention\\n",
        "2. Impl\u00e9menter l'attention from scratch avec NumPy\\n",
        "3. Impl\u00e9menter l'attention avec PyTorch\\n",
        "4. Visualiser les poids d'attention\\n",
        "5. Comprendre le r\u00f4le de chaque composant (Q, K, V)\\n",
        "\\n",
        "### Qu'est-ce que l'attention?\\n",
        "\\n",
        "L'attention est un m\u00e9canisme qui permet \u00e0 chaque token de \\\"regarder\\\" les autres tokens et de pond\u00e9rer leur importance. C'est comme si chaque mot dans une phrase pouvait d\u00e9cider quels autres mots sont importants pour comprendre son sens.\\n",
        "\\n",
        "**Exemple concret:**\\n",
        "Dans la phrase \\\"Le chat qui \u00e9tait sur le tapis a mang\u00e9 la souris\\\", le mot \\\"mang\u00e9\\\" doit pr\u00eater attention \u00e0:\\n",
        "- \\\"chat\\\" (qui a mang\u00e9?)\\n",
        "- \\\"souris\\\" (qu'est-ce qui a \u00e9t\u00e9 mang\u00e9?)\\n",
        "\\n",
        "L'attention calcule automatiquement ces relations!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Formule Math\u00e9matique de l'Attention\\n",
        "\\n",
        "### Formule Compl\u00e8te\\n",
        "\\n",
        "$$\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V$$\\n",
        "\\n",
        "### D\u00e9composition \u00c9tape par \u00c9tape\\n",
        "\\n",
        "#### \u00c9tape 1: Calcul des Scores de Similarit\u00e9\\n",
        "\\n",
        "$$S = QK^T$$\\n",
        "\\n",
        "O\u00f9:\\n",
        "- $Q \\\\in \\\\mathbb{R}^{n \\\\times d_k}$ : Matrice des **Queries** (\\\"ce que je cherche\\\")\\n",
        "- $K \\\\in \\\\mathbb{R}^{m \\\\times d_k}$ : Matrice des **Keys** (\\\"ce que j'offre comme information\\\")\\n",
        "- $S \\\\in \\\\mathbb{R}^{n \\\\times m}$ : Matrice des **scores** de similarit\u00e9\\n",
        "\\n",
        "**Intuition:** $S_{ij}$ mesure la similarit\u00e9 entre la query $i$ et la key $j$.\\n",
        "\\n",
        "#### \u00c9tape 2: Normalisation (Scaling)\\n",
        "\\n",
        "$$S' = \\\\frac{S}{\\\\sqrt{d_k}}$$\\n",
        "\\n",
        "**Pourquoi diviser par $\\\\sqrt{d_k}$?**\\n",
        "- Les produits scalaires croissent avec la dimension $d_k$\\n",
        "- Sans normalisation, le softmax sature (gradients $\\\\rightarrow 0$)\\n",
        "- $\\\\sqrt{d_k}$ est la d\u00e9viation standard th\u00e9orique du produit scalaire\\n",
        "\\n",
        "#### \u00c9tape 3: Calcul des Poids d'Attention\\n",
        "\\n",
        "$$A = \\\\text{softmax}(S')$$\\n",
        "\\n",
        "O\u00f9 le softmax est d\u00e9fini comme:\\n",
        "\\n",
        "$$\\\\text{softmax}(x_i) = \\\\frac{e^{x_i}}{\\\\sum_{j=1}^m e^{x_j}}$$\\n",
        "\\n",
        "**Propri\u00e9t\u00e9s du softmax:**\\n",
        "- Toutes les valeurs sont entre 0 et 1\\n",
        "- La somme de chaque ligne = 1 (distribution de probabilit\u00e9)\\n",
        "- Les scores \u00e9lev\u00e9s $\\\\rightarrow$ probabilit\u00e9s \u00e9lev\u00e9es\\n",
        "\\n",
        "#### \u00c9tape 4: Pond\u00e9ration des Valeurs\\n",
        "\\n",
        "$$O = AV$$\\n",
        "\\n",
        "O\u00f9:\\n",
        "- $V \\\\in \\\\mathbb{R}^{m \\\\times d_v}$ : Matrice des **Values** (\\\"l'information elle-m\u00eame\\\")\\n",
        "- $O \\\\in \\\\mathbb{R}^{n \\\\times d_v}$ : Sortie pond\u00e9r\u00e9e\\n",
        "\\n",
        "**Intuition:** Chaque ligne de $O$ est une somme pond\u00e9r\u00e9e des valeurs, o\u00f9 les poids sont donn\u00e9s par $A$.\\n",
        "\\n",
        "$$O_i = \\\\sum_{j=1}^m A_{ij} V_j$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\\n",
        "import numpy as np\\n",
        "import math\\n",
        "import torch\\n",
        "import torch.nn as nn\\n",
        "import torch.nn.functional as F\\n",
        "import matplotlib.pyplot as plt\\n",
        "import seaborn as sns\\n",
        "import sys\\n",
        "from typing import Tuple, Optional\\n",
        "\\n",
        "# Ajouter le chemin vers src/\\n",
        "sys.path.append('../..')\\n",
        "\\n",
        "# Configuration pour les visualisations\\n",
        "plt.style.use('default')\\n",
        "sns.set_palette(\\\"husl\\\")\\n",
        "\\n",
        "# Seed pour la reproductibilit\u00e9\\n",
        "np.random.seed(42)\\n",
        "torch.manual_seed(42)\\n",
        "\\n",
        "print(\\\"\u2713 Imports r\u00e9ussis!\\\")\\n",
        "print(f\\\"\u2713 PyTorch version: {torch.__version__}\\\")\\n",
        "print(f\\\"\u2713 Device disponible: {'GPU' if torch.cuda.is_available() else 'CPU'}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Impl\u00e9mentation From Scratch (NumPy)\\n",
        "\\n",
        "Commen\u00e7ons par impl\u00e9menter l'attention avec NumPy pour comprendre chaque op\u00e9ration math\u00e9matique.\\n",
        "\\n",
        "### 2.1 Fonction Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax_from_scratch(x: np.ndarray, axis: int = -1) -> np.ndarray:\\n",
        "    \\\"\\\"\\\"\\n",
        "    Impl\u00e9mentation from-scratch du softmax pour la stabilit\u00e9 num\u00e9rique.\\n",
        "    \\n",
        "    Formule: softmax(x_i) = exp(x_i) / sum(exp(x_j))\\n",
        "    \\n",
        "    Astuce de stabilit\u00e9: softmax(x) = softmax(x - max(x))\\n",
        "    \\\"\\\"\\\"\\n",
        "    # Soustraire le max pour la stabilit\u00e9 num\u00e9rique (\u00e9vite overflow)\\n",
        "    x_shifted = x - np.max(x, axis=axis, keepdims=True)\\n",
        "    \\n",
        "    # Calculer les exponentielles\\n",
        "    exp_x = np.exp(x_shifted)\\n",
        "    \\n",
        "    # Normaliser pour obtenir des probabilit\u00e9s\\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\\n",
        "\\n",
        "# Test du softmax\\n",
        "test_scores = np.array([1.0, 2.0, 3.0])\\n",
        "test_probs = softmax_from_scratch(test_scores)\\n",
        "print(\\\"Scores:\\\", test_scores)\\n",
        "print(\\\"Probabilit\u00e9s:\\\", test_probs)\\n",
        "print(\\\"Somme:\\\", test_probs.sum())\\n",
        "print(\\\"\u2713 Le softmax fonctionne!\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Attention Compl\u00e8te From Scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention_from_scratch(\\n",
        "    Q: np.ndarray,\\n",
        "    K: np.ndarray,\\n",
        "    V: np.ndarray,\\n",
        "    mask: Optional[np.ndarray] = None\\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\\n",
        "    \\\"\\\"\\\"\\n",
        "    Impl\u00e9mentation from-scratch de l'attention avec NumPy.\\n",
        "    \\n",
        "    Args:\\n",
        "        Q: Query matrix (batch_size, seq_len, d_k)\\n",
        "        K: Key matrix (batch_size, seq_len, d_k)\\n",
        "        V: Value matrix (batch_size, seq_len, d_v)\\n",
        "        mask: Optional mask (seq_len, seq_len)\\n",
        "    \\n",
        "    Returns:\\n",
        "        output: (batch_size, seq_len, d_v)\\n",
        "        attention_weights: (batch_size, seq_len, seq_len)\\n",
        "    \\\"\\\"\\\"\\n",
        "    d_k = Q.shape[-1]\\n",
        "    \\n",
        "    # \u00c9TAPE 1: Calcul des scores (QK^T)\\n",
        "    scores = np.matmul(Q, K.transpose(0, 2, 1))\\n",
        "    print(f\\\"[1] Scores shape apr\u00e8s QK^T: {scores.shape}\\\")\\n",
        "    \\n",
        "    # \u00c9TAPE 2: Normalisation (scaling)\\n",
        "    scores = scores / math.sqrt(d_k)\\n",
        "    print(f\\\"[2] Scores apr\u00e8s scaling par sqrt({d_k}) = {math.sqrt(d_k):.2f}\\\")\\n",
        "    \\n",
        "    # \u00c9TAPE 3: Application du masque (optionnel)\\n",
        "    if mask is not None:\\n",
        "        scores = np.where(mask == 0, -1e9, scores)\\n",
        "        print(f\\\"[3] Masque appliqu\u00e9\\\")\\n",
        "    \\n",
        "    # \u00c9TAPE 4: Calcul des poids d'attention (softmax)\\n",
        "    attention_weights = softmax_from_scratch(scores, axis=-1)\\n",
        "    print(f\\\"[4] Poids d'attention shape: {attention_weights.shape}\\\")\\n",
        "    print(f\\\"[4] V\u00e9rification normalisation: sum = {attention_weights[0, 0, :].sum():.4f}\\\")\\n",
        "    \\n",
        "    # \u00c9TAPE 5: Pond\u00e9ration des valeurs (AV)\\n",
        "    output = np.matmul(attention_weights, V)\\n",
        "    print(f\\\"[5] Output shape: {output.shape}\\\")\\n",
        "    \\n",
        "    return output, attention_weights\\n",
        "\\n",
        "print(\\\"\u2713 Fonction d'attention from-scratch d\u00e9finie!\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Exemple avec Petites Matrices (3\u00d73)\\n",
        "\\n",
        "Testons notre impl\u00e9mentation avec un exemple simple: 3 tokens avec dimension d_k = 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Param\u00e8tres pour l'exemple\\n",
        "batch_size = 1\\n",
        "seq_len = 3  # 3 tokens\\n",
        "d_k = 4      # dimension des cl\u00e9s/queries\\n",
        "d_v = 4      # dimension des valeurs\\n",
        "\\n",
        "# Cr\u00e9er des matrices Q, K, V simples\\n",
        "np.random.seed(42)\\n",
        "Q = np.random.randn(batch_size, seq_len, d_k)\\n",
        "K = np.random.randn(batch_size, seq_len, d_k)\\n",
        "V = np.random.randn(batch_size, seq_len, d_v)\\n",
        "\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(\\\"EXEMPLE: Attention avec 3 tokens\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(f\\\"\\\\nInput shapes:\\\")\\n",
        "print(f\\\"  Q (Query):  {Q.shape} - 'Ce que je cherche'\\\")\\n",
        "print(f\\\"  K (Key):    {K.shape} - 'Ce que j'offre'\\\")\\n",
        "print(f\\\"  V (Value):  {V.shape} - 'L'information'\\\")\\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculer l'attention\\n",
        "output, attention_weights = scaled_dot_product_attention_from_scratch(Q, K, V)\\n",
        "\\n",
        "print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\n",
        "print(\\\"R\u00c9SULTATS\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(f\\\"\\\\nOutput shape: {output.shape}\\\")\\n",
        "print(f\\\"Attention weights shape: {attention_weights.shape}\\\")\\n",
        "\\n",
        "print(\\\"\\\\nAttention weights (batch 0):\\\")\\n",
        "print(attention_weights[0])\\n",
        "\\n",
        "print(\\\"\\\\nInterpr\u00e9tation:\\\")\\n",
        "print(\\\"  - Ligne i = distribution d'attention du token i\\\")\\n",
        "print(\\\"  - Colonne j = combien les autres tokens regardent le token j\\\")\\n",
        "print(\\\"  - Chaque ligne somme \u00e0 1.0 (probabilit\u00e9s)\\\")\\n",
        "\\n",
        "# V\u00e9rifier la normalisation\\n",
        "row_sums = attention_weights.sum(axis=-1)\\n",
        "print(f\\\"\\\\nV\u00e9rification: somme de chaque ligne = {row_sums[0]}\\\")\\n",
        "print(f\\\"Toutes les sommes \u2248 1.0? {np.allclose(row_sums, 1.0)}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Impl\u00e9mentation PyTorch (Professionnelle)\\n",
        "\\n",
        "Maintenant, impl\u00e9mentons l'attention avec PyTorch en utilisant les op\u00e9rations optimis\u00e9es.\\n",
        "\\n",
        "### 3.1 Classe ScaledDotProductAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\\n",
        "    \\\"\\\"\\\"\\n",
        "    Impl\u00e9mentation PyTorch professionnelle de l'attention.\\n",
        "    \\n",
        "    M\u00e9thodes PyTorch utilis\u00e9es:\\n",
        "    - torch.matmul(): Multiplication matricielle optimis\u00e9e GPU\\n",
        "    - masked_fill(): Remplace les valeurs selon un masque\\n",
        "    - F.softmax(): Softmax stable num\u00e9riquement\\n",
        "    \\\"\\\"\\\"\\n",
        "    \\n",
        "    def __init__(self, d_k: int):\\n",
        "        super().__init__()\\n",
        "        self.d_k = d_k\\n",
        "        self.scale = 1.0 / math.sqrt(d_k)\\n",
        "    \\n",
        "    def forward(\\n",
        "        self,\\n",
        "        Q: torch.Tensor,\\n",
        "        K: torch.Tensor,\\n",
        "        V: torch.Tensor,\\n",
        "        mask: Optional[torch.Tensor] = None\\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\\n",
        "        \\\"\\\"\\\"\\n",
        "        Calcule l'attention Scaled Dot-Product.\\n",
        "        \\n",
        "        Args:\\n",
        "            Q: Query tensor (batch_size, seq_len, d_k)\\n",
        "            K: Key tensor (batch_size, seq_len, d_k)\\n",
        "            V: Value tensor (batch_size, seq_len, d_v)\\n",
        "            mask: Optional mask (seq_len, seq_len)\\n",
        "        \\n",
        "        Returns:\\n",
        "            output: (batch_size, seq_len, d_v)\\n",
        "            attention_weights: (batch_size, seq_len, seq_len)\\n",
        "        \\\"\\\"\\\"\\n",
        "        \\n",
        "        # \u00c9TAPE 1: Calcul des scores (QK^T)\\n",
        "        # torch.matmul() g\u00e8re automatiquement le batching\\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))\\n",
        "        \\n",
        "        # \u00c9TAPE 2: Normalisation (scaling)\\n",
        "        scores = scores * self.scale\\n",
        "        \\n",
        "        # \u00c9TAPE 3: Application du masque (optionnel)\\n",
        "        if mask is not None:\\n",
        "            # masked_fill() remplace les positions masqu\u00e9es par -inf\\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\\n",
        "        \\n",
        "        # \u00c9TAPE 4: Calcul des poids d'attention (softmax)\\n",
        "        # F.softmax() avec dim=-1 normalise sur la derni\u00e8re dimension\\n",
        "        attention_weights = F.softmax(scores, dim=-1)\\n",
        "        \\n",
        "        # \u00c9TAPE 5: Pond\u00e9ration des valeurs (AV)\\n",
        "        output = torch.matmul(attention_weights, V)\\n",
        "        \\n",
        "        return output, attention_weights\\n",
        "\\n",
        "print(\\\"\u2713 Classe ScaledDotProductAttention d\u00e9finie!\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Exemple avec PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr\u00e9er des tensors PyTorch\\n",
        "torch.manual_seed(42)\\n",
        "Q_torch = torch.randn(batch_size, seq_len, d_k)\\n",
        "K_torch = torch.randn(batch_size, seq_len, d_k)\\n",
        "V_torch = torch.randn(batch_size, seq_len, d_v)\\n",
        "\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(\\\"EXEMPLE: Attention avec PyTorch\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(f\\\"\\\\nInput shapes (PyTorch tensors):\\\")\\n",
        "print(f\\\"  Q: {Q_torch.shape}\\\")\\n",
        "print(f\\\"  K: {K_torch.shape}\\\")\\n",
        "print(f\\\"  V: {V_torch.shape}\\\")\\n",
        "print(f\\\"  Device: {Q_torch.device}\\\")\\n",
        "\\n",
        "# Cr\u00e9er le module d'attention\\n",
        "attention = ScaledDotProductAttention(d_k=d_k)\\n",
        "\\n",
        "# Calculer l'attention\\n",
        "output_torch, attention_weights_torch = attention(Q_torch, K_torch, V_torch)\\n",
        "\\n",
        "print(f\\\"\\\\nOutput shapes:\\\")\\n",
        "print(f\\\"  Output: {output_torch.shape}\\\")\\n",
        "print(f\\\"  Attention weights: {attention_weights_torch.shape}\\\")\\n",
        "\\n",
        "print(\\\"\\\\nAttention weights (batch 0):\\\")\\n",
        "print(attention_weights_torch[0])\\n",
        "\\n",
        "# V\u00e9rifier la normalisation\\n",
        "row_sums = attention_weights_torch.sum(dim=-1)\\n",
        "print(f\\\"\\\\nV\u00e9rification: somme de chaque ligne (batch 0):\\\")\\n",
        "print(row_sums[0])\\n",
        "print(f\\\"Toutes les sommes \u2248 1.0? {torch.allclose(row_sums, torch.ones_like(row_sums))}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualisation des Poids d'Attention\\n",
        "\\n",
        "Visualisons les poids d'attention sous forme de heatmap pour mieux comprendre quels tokens pr\u00eatent attention \u00e0 quels autres tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_attention(attention_weights, tokens=None, title=\\\"Poids d'Attention\\\"):\\n",
        "    \\\"\\\"\\\"\\n",
        "    Visualise les poids d'attention sous forme de heatmap.\\n",
        "    \\n",
        "    Args:\\n",
        "        attention_weights: Matrice d'attention (seq_len, seq_len)\\n",
        "        tokens: Liste optionnelle de tokens pour les labels\\n",
        "        title: Titre du graphique\\n",
        "    \\\"\\\"\\\"\\n",
        "    plt.figure(figsize=(8, 6))\\n",
        "    \\n",
        "    # Cr\u00e9er la heatmap\\n",
        "    if tokens is None:\\n",
        "        tokens = [f\\\"Token {i}\\\" for i in range(len(attention_weights))]\\n",
        "    \\n",
        "    sns.heatmap(\\n",
        "        attention_weights,\\n",
        "        xticklabels=tokens,\\n",
        "        yticklabels=tokens,\\n",
        "        cmap='YlOrRd',\\n",
        "        annot=True,\\n",
        "        fmt='.3f',\\n",
        "        cbar_kws={'label': 'Poids d\\\\'attention'},\\n",
        "        vmin=0,\\n",
        "        vmax=1\\n",
        "    )\\n",
        "    \\n",
        "    plt.xlabel('Keys (ce qu\\\\'on regarde)', fontsize=12)\\n",
        "    plt.ylabel('Queries (qui regarde)', fontsize=12)\\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\\n",
        "    plt.tight_layout()\\n",
        "    plt.show()\\n",
        "\\n",
        "# Visualiser les poids d'attention de notre exemple\\n",
        "tokens_example = [\\\"Token 0\\\", \\\"Token 1\\\", \\\"Token 2\\\"]\\n",
        "visualize_attention(\\n",
        "    attention_weights_torch[0].detach().numpy(),\\n",
        "    tokens=tokens_example,\\n",
        "    title=\\\"Poids d'Attention - Exemple 3 Tokens\\\"\\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Attention avec Masque Causal (GPT)\\n",
        "\\n",
        "Le masque causal emp\u00eache les tokens de voir le futur. C'est essentiel pour les mod\u00e8les g\u00e9n\u00e9ratifs comme GPT.\\n",
        "\\n",
        "### 5.1 Cr\u00e9ation du Masque Causal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr\u00e9er un masque causal avec torch.tril()\\n",
        "seq_len_mask = 5\\n",
        "causal_mask = torch.tril(torch.ones(seq_len_mask, seq_len_mask))\\n",
        "\\n",
        "print(\\\"Masque causal (1=autoris\u00e9, 0=bloqu\u00e9):\\\")\\n",
        "print(causal_mask)\\n",
        "print(\\\"\\\\nInterpr\u00e9tation:\\\")\\n",
        "print(\\\"  - Chaque ligne repr\u00e9sente un token\\\")\\n",
        "print(\\\"  - 1 = peut voir ce token, 0 = ne peut pas voir\\\")\\n",
        "print(\\\"  - Token i peut voir tokens 0 \u00e0 i (autor\u00e9gressif)\\\")\\n",
        "\\n",
        "# Visualiser le masque\\n",
        "plt.figure(figsize=(6, 5))\\n",
        "sns.heatmap(\\n",
        "    causal_mask.numpy(),\\n",
        "    cmap='RdYlGn',\\n",
        "    annot=True,\\n",
        "    fmt='.0f',\\n",
        "    cbar=False,\\n",
        "    square=True\\n",
        ")\\n",
        "plt.title('Masque Causal (Triangulaire Inf\u00e9rieur)', fontsize=14, fontweight='bold')\\n",
        "plt.xlabel('Position Key')\\n",
        "plt.ylabel('Position Query')\\n",
        "plt.tight_layout()\\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Comparaison: Avec et Sans Masque Causal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cr\u00e9er des donn\u00e9es de test\\n",
        "torch.manual_seed(42)\\n",
        "Q_test = torch.randn(1, seq_len_mask, 8)\\n",
        "K_test = torch.randn(1, seq_len_mask, 8)\\n",
        "V_test = torch.randn(1, seq_len_mask, 8)\\n",
        "\\n",
        "attention_test = ScaledDotProductAttention(d_k=8)\\n",
        "\\n",
        "# Attention SANS masque (bidirectionnelle - BERT)\\n",
        "print(\\\"=\\\" * 60)\\n",
        "print(\\\"Attention SANS masque (bidirectionnelle - BERT)\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "output_no_mask, weights_no_mask = attention_test(Q_test, K_test, V_test, mask=None)\\n",
        "print(\\\"\\\\nPoids d'attention (tous les tokens se voient):\\\")\\n",
        "print(weights_no_mask[0])\\n",
        "\\n",
        "# Attention AVEC masque causal (autor\u00e9gressif - GPT)\\n",
        "print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\\n",
        "print(\\\"Attention AVEC masque causal (autor\u00e9gressif - GPT)\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "output_masked, weights_masked = attention_test(Q_test, K_test, V_test, mask=causal_mask)\\n",
        "print(\\\"\\\\nPoids d'attention (masque causal appliqu\u00e9):\\\")\\n",
        "print(weights_masked[0])\\n",
        "print(\\\"\\\\nObservation:\\\")\\n",
        "print(\\\"  - La partie sup\u00e9rieure droite est nulle (pas d'attention au futur)\\\")\\n",
        "print(\\\"  - Chaque token ne voit que lui-m\u00eame et les tokens pr\u00e9c\u00e9dents\\\")\\n",
        "print(\\\"  - C'est le m\u00e9canisme cl\u00e9 de GPT pour la g\u00e9n\u00e9ration autoregressive\\\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualiser la comparaison\\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\\n",
        "\\n",
        "# Sans masque\\n",
        "sns.heatmap(\\n",
        "    weights_no_mask[0].detach().numpy(),\\n",
        "    ax=axes[0],\\n",
        "    cmap='YlOrRd',\\n",
        "    annot=True,\\n",
        "    fmt='.3f',\\n",
        "    cbar_kws={'label': 'Poids'},\\n",
        "    vmin=0,\\n",
        "    vmax=1\\n",
        ")\\n",
        "axes[0].set_title('BERT: Attention Bidirectionnelle\\\\n(Tous les tokens se voient)', fontweight='bold')\\n",
        "axes[0].set_xlabel('Keys')\\n",
        "axes[0].set_ylabel('Queries')\\n",
        "\\n",
        "# Avec masque\\n",
        "sns.heatmap(\\n",
        "    weights_masked[0].detach().numpy(),\\n",
        "    ax=axes[1],\\n",
        "    cmap='YlOrRd',\\n",
        "    annot=True,\\n",
        "    fmt='.3f',\\n",
        "    cbar_kws={'label': 'Poids'},\\n",
        "    vmin=0,\\n",
        "    vmax=1\\n",
        ")\\n",
        "axes[1].set_title('GPT: Attention Causale\\\\n(Pas de vision du futur)', fontweight='bold')\\n",
        "axes[1].set_xlabel('Keys')\\n",
        "axes[1].set_ylabel('Queries')\\n",
        "\\n",
        "plt.tight_layout()\\n",
        "plt.show()\\n",
        "\\n",
        "# Statistiques\\n",
        "print(\\\"\\\\nComparaison: Nombre de tokens visibles par position\\\")\\n",
        "print(\\\"=\\\" * 60)\\n",
        "for i in range(seq_len_mask):\\n",
        "    visible_no_mask = (weights_no_mask[0, i] > 0).sum().item()\\n",
        "    visible_masked = (weights_masked[0, i] > 0).sum().item()\\n",
        "    print(f\\\"  Position {i}: Sans masque={visible_no_mask}, Avec masque={visible_masked}\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercices Pratiques\\n",
        "\\n",
        "### Exercice 1: Calcul Manuel\\n",
        "\\n",
        "Calculez manuellement l'attention pour cet exemple simple:\\n",
        "\\n",
        "```\\n",
        "Q = [[1, 0]]\\n",
        "K = [[1, 0], [0, 1]]\\n",
        "V = [[2, 0], [0, 3]]\\n",
        "d_k = 2\\n",
        "```\\n",
        "\\n",
        "**\u00c9tapes:**\\n",
        "1. Calculez $QK^T$\\n",
        "2. Divisez par $\\\\sqrt{d_k} = \\\\sqrt{2}$\\n",
        "3. Appliquez softmax\\n",
        "4. Multipliez par V\\n",
        "\\n",
        "**TODO:** Compl\u00e9tez le code ci-dessous pour v\u00e9rifier votre calcul."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 1 - Calcul manuel\\n",
        "# Compl\u00e9tez ce code pour v\u00e9rifier votre calcul manuel\\n",
        "\\n",
        "Q_ex1 = np.array([[[1, 0]]])  # (1, 1, 2)\\n",
        "K_ex1 = np.array([[[1, 0], [0, 1]]])  # (1, 2, 2)\\n",
        "V_ex1 = np.array([[[2, 0], [0, 3]]])  # (1, 2, 2)\\n",
        "\\n",
        "# Votre calcul manuel ici:\\n",
        "# \u00c9tape 1: QK^T = ?\\n",
        "# \u00c9tape 2: Scaling = ?\\n",
        "# \u00c9tape 3: Softmax = ?\\n",
        "# \u00c9tape 4: Output = ?\\n",
        "\\n",
        "# V\u00e9rification avec notre fonction\\n",
        "output_ex1, weights_ex1 = scaled_dot_product_attention_from_scratch(Q_ex1, K_ex1, V_ex1)\\n",
        "print(\\\"Poids d'attention:\\\", weights_ex1[0])\\n",
        "print(\\\"Output:\\\", output_ex1[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 2: Exp\u00e9rimentation avec d_k\\n",
        "\\n",
        "Observez l'effet de la dimension $d_k$ sur les poids d'attention.\\n",
        "\\n",
        "**Question:** Que se passe-t-il quand $d_k$ augmente? Pourquoi la normalisation par $\\\\sqrt{d_k}$ est-elle importante?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Exercice 2 - Effet de d_k\\n",
        "# Testez avec diff\u00e9rentes valeurs de d_k: 4, 16, 64, 256\\n",
        "\\n",
        "def test_different_dk(d_k_values):\\n",
        "    \\\"\\\"\\\"Teste l'attention avec diff\u00e9rentes valeurs de d_k.\\\"\\\"\\\"\\n",
        "    for d_k in d_k_values:\\n",
        "        torch.manual_seed(42)\\n",
        "        Q = torch.randn(1, 3, d_k)\\n",
        "        K = torch.randn(1, 3, d_k)\\n",
        "        V = torch.randn(1, 3, d_k)\\n",
        "        \\n",
        "        attention = ScaledDotProductAttention(d_k=d_k)\\n",
        "        _, weights = attention(Q, K, V)\\n",
        "        \\n",
        "        # Calculer la variance des poids\\n",
        "        variance = weights.var().item()\\n",
        "        print(f\\\"d_k={d_k:3d}: variance des poids = {variance:.6f}\\\")\\n",
        "\\n",
        "print(\\\"Effet de d_k sur la distribution des poids d'attention:\\\")\\n",
        "test_different_dk([4, 16, 64, 256])\\n",
        "\\n",
        "print(\\\"\\\\nObservation: Sans scaling, les poids deviendraient trop concentr\u00e9s!\\\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. R\u00e9sum\u00e9\\n",
        "\\n",
        "### Ce que nous avons appris\\n",
        "\\n",
        "1. **Formule de l'attention:** $\\\\text{Attention}(Q, K, V) = \\\\text{softmax}\\\\left(\\\\frac{QK^T}{\\\\sqrt{d_k}}\\\\right)V$\\n",
        "\\n",
        "2. **Les 4 \u00e9tapes:**\\n",
        "   - Calcul des scores de similarit\u00e9 (QK^T)\\n",
        "   - Normalisation par $\\\\sqrt{d_k}$\\n",
        "   - Application du softmax pour obtenir des probabilit\u00e9s\\n",
        "   - Pond\u00e9ration des valeurs\\n",
        "\\n",
        "3. **R\u00f4le de Q, K, V:**\\n",
        "   - **Q (Query):** \\\"Ce que je cherche\\\"\\n",
        "   - **K (Key):** \\\"Ce que j'offre comme information\\\"\\n",
        "   - **V (Value):** \\\"L'information elle-m\u00eame\\\"\\n",
        "\\n",
        "4. **Masque causal:**\\n",
        "   - Emp\u00eache de voir le futur\\n",
        "   - Essentiel pour GPT (g\u00e9n\u00e9ration autoregressive)\\n",
        "   - BERT n'utilise pas de masque (attention bidirectionnelle)\\n",
        "\\n",
        "### Prochaines \u00e9tapes\\n",
        "\\n",
        "Dans le prochain notebook, nous verrons:\\n",
        "- **Multi-Head Attention:** Plusieurs t\u00eates d'attention en parall\u00e8le\\n",
        "- Comment combiner plusieurs perspectives\\n",
        "- L'architecture compl\u00e8te du transformer\\n",
        "\\n",
        "### Points cl\u00e9s \u00e0 retenir\\n",
        "\\n",
        "\u2713 L'attention permet \u00e0 chaque token de \\\"regarder\\\" les autres tokens\\n",
        "\u2713 Le scaling par $\\\\sqrt{d_k}$ est crucial pour la stabilit\u00e9\\n",
        "\u2713 Le softmax convertit les scores en probabilit\u00e9s\\n",
        "\u2713 Le masque causal est la diff\u00e9rence cl\u00e9 entre BERT et GPT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
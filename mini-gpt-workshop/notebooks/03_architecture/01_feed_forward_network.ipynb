{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feed-Forward Network (FFN)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Bienvenue dans ce notebook sur le **Feed-Forward Network**, le composant qui ajoute de la capacité de transformation non-linéaire dans chaque TransformerBlock!\n",
        "\n",
        "### Objectifs pédagogiques\n",
        "\n",
        "Dans ce notebook, vous allez:\n",
        "1. Comprendre l'architecture du Feed-Forward Network\n",
        "2. Implémenter FFN from scratch avec NumPy\n",
        "3. Implémenter FFN avec PyTorch\n",
        "4. Comparer les fonctions d'activation GELU et ReLU\n",
        "5. Visualiser les différences entre GELU et ReLU\n",
        "6. Valider la préservation des dimensions\n",
        "\n",
        "### Qu'est-ce qu'un Feed-Forward Network?\n",
        "\n",
        "Le FFN est un réseau de neurones simple appliqué **indépendamment** à chaque position de la séquence (d'où \"position-wise\").\n",
        "\n",
        "**Analogie:** Imaginez que l'attention permet aux tokens de \"communiquer\" entre eux. Le FFN permet ensuite à chaque token de \"réfléchir\" individuellement sur ce qu'il a appris.\n",
        "\n",
        "**Architecture:**\n",
        "- **Couche 1:** Expansion (d_model → d_ff, typiquement d_ff = 4 × d_model)\n",
        "- **Activation:** Non-linéarité (GELU ou ReLU)\n",
        "- **Couche 2:** Projection (d_ff → d_model)\n",
        "\n",
        "**Rôle dans le Transformer:**\n",
        "- Après l'attention, le FFN ajoute de la capacité de transformation\n",
        "- Permet au modèle d'apprendre des représentations plus complexes\n",
        "- Appliqué de manière identique à chaque position (\"position-wise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Formules Mathématiques du Feed-Forward Network\n",
        "\n",
        "### Formule Complète\n",
        "\n",
        "$\\text{FFN}(x) = \\text{activation}(xW_1 + b_1)W_2 + b_2$\n",
        "\n",
        "### Décomposition Étape par Étape\n",
        "\n",
        "#### Étape 1: Première Transformation Linéaire (Expansion)\n",
        "\n",
        "$h = xW_1 + b_1$\n",
        "\n",
        "Où:\n",
        "- $x \\in \\mathbb{R}^{d_{\\text{model}}}$ : Input (un token)\n",
        "- $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$ : Première matrice de poids\n",
        "- $b_1 \\in \\mathbb{R}^{d_{\\text{ff}}}$ : Premier biais\n",
        "- $h \\in \\mathbb{R}^{d_{\\text{ff}}}$ : Représentation cachée (expanded)\n",
        "\n",
        "**Dimensions typiques:**\n",
        "- $d_{\\text{model}} = 256, 512, 768$\n",
        "- $d_{\\text{ff}} = 4 \\times d_{\\text{model}} = 1024, 2048, 3072$\n",
        "\n",
        "#### Étape 2: Activation Non-Linéaire\n",
        "\n",
        "$h' = \\text{activation}(h)$\n",
        "\n",
        "**GELU (Gaussian Error Linear Unit):**\n",
        "\n",
        "$\\text{GELU}(x) = x \\cdot \\Phi(x)$\n",
        "\n",
        "Où $\\Phi(x)$ est la fonction de répartition de la loi normale standard.\n",
        "\n",
        "**Approximation de GELU:**\n",
        "\n",
        "$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left(\\sqrt{\\frac{2}{\\pi}} \\left(x + 0.044715 \\cdot x^3\\right)\\right)\\right)$\n",
        "\n",
        "**ReLU (Rectified Linear Unit):**\n",
        "\n",
        "$\\text{ReLU}(x) = \\max(0, x)$\n",
        "\n",
        "#### Étape 3: Deuxième Transformation Linéaire (Projection)\n",
        "\n",
        "$y = h'W_2 + b_2$\n",
        "\n",
        "Où:\n",
        "- $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$ : Deuxième matrice de poids\n",
        "- $b_2 \\in \\mathbb{R}^{d_{\\text{model}}}$ : Deuxième biais\n",
        "- $y \\in \\mathbb{R}^{d_{\\text{model}}}$ : Output (même dimension que l'input)\n",
        "\n",
        "### Propriété Importante: Préservation de Dimension\n",
        "\n",
        "**Input:** $(\\text{batch}, \\text{seq\\_len}, d_{\\text{model}})$\n",
        "\n",
        "**Output:** $(\\text{batch}, \\text{seq\\_len}, d_{\\text{model}})$\n",
        "\n",
        "Le FFN préserve les dimensions d'entrée, ce qui permet de l'utiliser dans les connexions résiduelles.\n",
        "\n",
        "### Pourquoi GELU plutôt que ReLU?\n",
        "\n",
        "**Avantages de GELU:**\n",
        "1. **Lisse partout:** Pas de discontinuité en 0\n",
        "2. **Gradient non-nul pour valeurs négatives:** Évite la \"mort\" des neurones\n",
        "3. **Performance empirique:** Meilleurs résultats dans BERT et GPT\n",
        "4. **Propriétés probabilistes:** Basé sur la distribution gaussienne\n",
        "\n",
        "**ReLU:**\n",
        "- Plus simple et rapide\n",
        "- Peut \"tuer\" des neurones (gradient = 0 pour x < 0)\n",
        "- Discontinuité en 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sys\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Ajouter le chemin vers src/\n",
        "sys.path.append('../..')\n",
        "\n",
        "# Importer nos modules\n",
        "from src.architecture.feed_forward import (\n",
        "    feed_forward_from_scratch,\n",
        "    gelu_from_scratch,\n",
        "    relu_from_scratch,\n",
        "    FeedForward,\n",
        "    FeedForwardSequential,\n",
        "    compare_activations,\n",
        "    validate_dimension_preservation\n",
        ")\n",
        "\n",
        "# Configuration pour les visualisations\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Seed pour la reproductibilité\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "print(\"✓ Imports réussis!\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ Device disponible: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Implémentation From Scratch (NumPy)\n",
        "\n",
        "Commençons par implémenter le FFN avec NumPy pour comprendre chaque opération matricielle.\n",
        "\n",
        "### 2.1 Exemple Simple avec 1 Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paramètres pour l'exemple\n",
        "d_model = 8      # dimension du modèle (petite pour la visualisation)\n",
        "d_ff = 32        # dimension cachée (4 × d_model)\n",
        "\n",
        "# Créer un input simple (1 token)\n",
        "np.random.seed(42)\n",
        "x = np.random.randn(d_model)\n",
        "\n",
        "# Créer les matrices de poids\n",
        "W1 = np.random.randn(d_model, d_ff) * 0.1\n",
        "b1 = np.zeros(d_ff)\n",
        "W2 = np.random.randn(d_ff, d_model) * 0.1\n",
        "b2 = np.zeros(d_model)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXEMPLE: Feed-Forward Network from Scratch (1 token)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  - d_model: {d_model}\")\n",
        "print(f\"  - d_ff: {d_ff}\")\n",
        "print(f\"  - Ratio d_ff/d_model: {d_ff/d_model}\")\n",
        "print(f\"\\nDimensions:\")\n",
        "print(f\"  - Input x: {x.shape}\")\n",
        "print(f\"  - W1: {W1.shape}\")\n",
        "print(f\"  - b1: {b1.shape}\")\n",
        "print(f\"  - W2: {W2.shape}\")\n",
        "print(f\"  - b2: {b2.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Étape 1: Première transformation linéaire (expansion)\n",
        "h = np.matmul(x, W1) + b1\n",
        "print(f\"\\nÉtape 1: Première transformation linéaire\")\n",
        "print(f\"  h = xW1 + b1\")\n",
        "print(f\"  Shape: {x.shape} @ {W1.shape} + {b1.shape} -> {h.shape}\")\n",
        "print(f\"  Dimension: {d_model} -> {d_ff} (expansion)\")\n",
        "print(f\"  h[:5] = {h[:5]}\")\n",
        "\n",
        "# Étape 2: Activation GELU\n",
        "h_activated = gelu_from_scratch(h)\n",
        "print(f\"\\nÉtape 2: Activation GELU\")\n",
        "print(f\"  h' = GELU(h)\")\n",
        "print(f\"  Shape: {h.shape} -> {h_activated.shape}\")\n",
        "print(f\"  h'[:5] = {h_activated[:5]}\")\n",
        "print(f\"  Effet: Valeurs négatives atténuées, positives préservées\")\n",
        "\n",
        "# Étape 3: Deuxième transformation linéaire (projection)\n",
        "y = np.matmul(h_activated, W2) + b2\n",
        "print(f\"\\nÉtape 3: Deuxième transformation linéaire\")\n",
        "print(f\"  y = h'W2 + b2\")\n",
        "print(f\"  Shape: {h_activated.shape} @ {W2.shape} + {b2.shape} -> {y.shape}\")\n",
        "print(f\"  Dimension: {d_ff} -> {d_model} (projection)\")\n",
        "print(f\"  y[:5] = {y[:5]}\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"RÉSULTAT FINAL\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"  Input shape:  {x.shape}\")\n",
        "print(f\"  Output shape: {y.shape}\")\n",
        "print(f\"  ✓ Dimension préservée: {x.shape == y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Exemple avec Batch et Séquence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paramètres pour l'exemple avec batch\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "d_model = 8\n",
        "d_ff = 32\n",
        "\n",
        "# Créer l'input (batch de séquences)\n",
        "np.random.seed(42)\n",
        "x_batch = np.random.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Créer les matrices de poids\n",
        "W1 = np.random.randn(d_model, d_ff) * 0.1\n",
        "b1 = np.zeros(d_ff)\n",
        "W2 = np.random.randn(d_ff, d_model) * 0.1\n",
        "b2 = np.zeros(d_model)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXEMPLE: Feed-Forward Network from Scratch (batch + séquence)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  - batch_size: {batch_size}\")\n",
        "print(f\"  - seq_len: {seq_len}\")\n",
        "print(f\"  - d_model: {d_model}\")\n",
        "print(f\"  - d_ff: {d_ff}\")\n",
        "print(f\"\\nInput shape: {x_batch.shape}\")\n",
        "print(f\"  (batch_size, seq_len, d_model)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Appliquer le FFN avec GELU\n",
        "output_gelu = feed_forward_from_scratch(x_batch, W1, b1, W2, b2, activation=\"gelu\")\n",
        "\n",
        "print(f\"\\nRésultat avec GELU:\")\n",
        "print(f\"  Input shape:  {x_batch.shape}\")\n",
        "print(f\"  Output shape: {output_gelu.shape}\")\n",
        "print(f\"  ✓ Dimensions préservées: {x_batch.shape == output_gelu.shape}\")\n",
        "\n",
        "# Appliquer le FFN avec ReLU pour comparaison\n",
        "output_relu = feed_forward_from_scratch(x_batch, W1, b1, W2, b2, activation=\"relu\")\n",
        "\n",
        "print(f\"\\nRésultat avec ReLU:\")\n",
        "print(f\"  Input shape:  {x_batch.shape}\")\n",
        "print(f\"  Output shape: {output_relu.shape}\")\n",
        "print(f\"  ✓ Dimensions préservées: {x_batch.shape == output_relu.shape}\")\n",
        "\n",
        "# Comparer les sorties\n",
        "diff = np.abs(output_gelu - output_relu).mean()\n",
        "print(f\"\\nDifférence moyenne entre GELU et ReLU: {diff:.6f}\")\n",
        "print(f\"  (Les activations différentes produisent des sorties différentes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Implémentation PyTorch (nn.Module)\n",
        "\n",
        "Maintenant, implémentons le FFN avec PyTorch de manière professionnelle.\n",
        "\n",
        "### 3.1 Création du Module FFN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paramètres\n",
        "d_model = 256\n",
        "d_ff = 1024  # 4 × d_model\n",
        "dropout = 0.1\n",
        "\n",
        "# Créer le module FFN\n",
        "ffn = FeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout, activation=\"gelu\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPLÉMENTATION PYTORCH: Feed-Forward Network\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  - d_model: {d_model}\")\n",
        "print(f\"  - d_ff: {d_ff}\")\n",
        "print(f\"  - dropout: {dropout}\")\n",
        "print(f\"  - activation: GELU\")\n",
        "\n",
        "print(f\"\\nArchitecture du module:\")\n",
        "print(ffn)\n",
        "\n",
        "# Compter les paramètres\n",
        "total_params = sum(p.numel() for p in ffn.parameters())\n",
        "print(f\"\\nNombre total de paramètres: {total_params:,}\")\n",
        "print(f\"  - linear1: {d_model * d_ff + d_ff:,} (W1 + b1)\")\n",
        "print(f\"  - linear2: {d_ff * d_model + d_model:,} (W2 + b2)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Explication des Méthodes PyTorch\n",
        "\n",
        "#### nn.Linear(in_features, out_features)\n",
        "- **Fonction:** Transformation linéaire $y = xW^T + b$\n",
        "- **Initialisation:** Xavier uniform par défaut\n",
        "- **GPU:** Optimisé pour calcul parallèle\n",
        "- **Gradient:** Calcul automatique avec autograd\n",
        "\n",
        "#### nn.GELU()\n",
        "- **Fonction:** $\\text{GELU}(x) = x \\cdot \\Phi(x)$\n",
        "- **Avantage:** Plus lisse que ReLU\n",
        "- **Usage:** BERT, GPT-2, GPT-3\n",
        "- **Propriété:** Gradient non-nul pour valeurs négatives\n",
        "\n",
        "#### nn.Dropout(p)\n",
        "- **Fonction:** Désactive aléatoirement des neurones\n",
        "- **Entraînement:** Actif (dropout appliqué)\n",
        "- **Évaluation:** Inactif (pas de dropout)\n",
        "- **Effet:** Réduit le surapprentissage\n",
        "\n",
        "#### nn.Sequential()\n",
        "- **Fonction:** Empile des couches séquentiellement\n",
        "- **Avantage:** Code plus concis\n",
        "- **Usage:** Pour architectures linéaires simples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer un input de test\n",
        "batch_size = 4\n",
        "seq_len = 10\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(f\"\\nTest du forward pass:\")\n",
        "print(f\"  Input shape: {x.shape}\")\n",
        "\n",
        "# Mode entraînement (avec dropout)\n",
        "ffn.train()\n",
        "output_train = ffn(x)\n",
        "print(f\"  Output shape (train): {output_train.shape}\")\n",
        "\n",
        "# Mode évaluation (sans dropout)\n",
        "ffn.eval()\n",
        "output_eval = ffn(x)\n",
        "print(f\"  Output shape (eval): {output_eval.shape}\")\n",
        "\n",
        "print(f\"\\n  ✓ Dimensions préservées: {x.shape == output_eval.shape}\")\n",
        "\n",
        "# Vérifier que dropout fait une différence\n",
        "ffn.train()\n",
        "output_train2 = ffn(x)\n",
        "diff_train = torch.abs(output_train - output_train2).mean().item()\n",
        "print(f\"\\n  Différence entre deux forward passes (train mode): {diff_train:.6f}\")\n",
        "print(f\"    (Non-zéro car dropout est aléatoire)\")\n",
        "\n",
        "ffn.eval()\n",
        "output_eval2 = ffn(x)\n",
        "diff_eval = torch.abs(output_eval - output_eval2).mean().item()\n",
        "print(f\"  Différence entre deux forward passes (eval mode): {diff_eval:.10f}\")\n",
        "print(f\"    (Zéro car pas de dropout en eval)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Implémentation Alternative avec nn.Sequential"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer le module FFN avec nn.Sequential\n",
        "ffn_seq = FeedForwardSequential(d_model=d_model, d_ff=d_ff, dropout=0.0, activation=\"gelu\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPLÉMENTATION ALTERNATIVE: nn.Sequential\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nArchitecture:\")\n",
        "print(ffn_seq)\n",
        "\n",
        "# Comparer avec l'implémentation standard\n",
        "ffn_standard = FeedForward(d_model=d_model, d_ff=d_ff, dropout=0.0, activation=\"gelu\")\n",
        "\n",
        "# Copier les poids pour avoir les mêmes résultats\n",
        "ffn_seq.net[0].weight.data = ffn_standard.linear1.weight.data.clone()\n",
        "ffn_seq.net[0].bias.data = ffn_standard.linear1.bias.data.clone()\n",
        "ffn_seq.net[3].weight.data = ffn_standard.linear2.weight.data.clone()\n",
        "ffn_seq.net[3].bias.data = ffn_standard.linear2.bias.data.clone()\n",
        "\n",
        "# Tester\n",
        "ffn_standard.eval()\n",
        "ffn_seq.eval()\n",
        "\n",
        "x_test = torch.randn(2, 5, d_model)\n",
        "out_standard = ffn_standard(x_test)\n",
        "out_seq = ffn_seq(x_test)\n",
        "\n",
        "diff = torch.abs(out_standard - out_seq).max().item()\n",
        "print(f\"\\nComparaison des deux implémentations:\")\n",
        "print(f\"  Différence maximale: {diff:.10f}\")\n",
        "print(f\"  ✓ Les deux implémentations sont identiques!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Visualisation: GELU vs ReLU\n",
        "\n",
        "Comparons visuellement les deux fonctions d'activation pour comprendre leurs différences.\n",
        "\n",
        "### 4.1 Fonctions d'Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Générer les données pour la visualisation\n",
        "comparison_data = compare_activations()\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VISUALISATION: GELU vs ReLU\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nGraphiques générés:\")\n",
        "print(\"  1. Fonctions d'activation (gauche)\")\n",
        "print(\"  2. Gradients (droite)\")\n",
        "print(\"\\nObservations clés:\")\n",
        "print(\"  - GELU est lisse partout (pas de discontinuité)\")\n",
        "print(\"  - ReLU a une discontinuité en x=0\")\n",
        "print(\"  - GELU permet un petit gradient pour x<0\")\n",
        "print(\"  - ReLU a gradient=0 pour x<0 (peut 'tuer' des neurones)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Analyse Détaillée des Différences\n",
        "\n",
        "#### Comportement pour x < 0\n",
        "\n",
        "**ReLU:**\n",
        "- $\\text{ReLU}(x) = 0$ pour $x < 0$\n",
        "- Gradient = 0 pour $x < 0$\n",
        "- Problème: \"Mort\" des neurones (gradient nul)\n",
        "\n",
        "**GELU:**\n",
        "- $\\text{GELU}(x) \\approx 0$ pour $x < 0$ mais pas exactement 0\n",
        "- Gradient $\\neq 0$ pour $x < 0$\n",
        "- Avantage: Tous les neurones peuvent apprendre\n",
        "\n",
        "#### Comportement pour x > 0\n",
        "\n",
        "**ReLU:**\n",
        "- $\\text{ReLU}(x) = x$ pour $x > 0$\n",
        "- Gradient = 1 pour $x > 0$\n",
        "- Simple et efficace\n",
        "\n",
        "**GELU:**\n",
        "- $\\text{GELU}(x) \\approx x$ pour $x > 0$\n",
        "- Gradient $\\approx 1$ pour $x > 0$\n",
        "- Légèrement plus complexe mais plus performant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyser les valeurs spécifiques\n",
        "test_values = np.array([-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0])\n",
        "\n",
        "gelu_vals = gelu_from_scratch(test_values)\n",
        "relu_vals = relu_from_scratch(test_values)\n",
        "\n",
        "print(\"\\nComparaison des valeurs:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'x':>8} | {'GELU(x)':>12} | {'ReLU(x)':>12} | {'Différence':>12}\")\n",
        "print(\"=\" * 70)\n",
        "for x, g, r in zip(test_values, gelu_vals, relu_vals):\n",
        "    diff = abs(g - r)\n",
        "    print(f\"{x:>8.2f} | {g:>12.6f} | {r:>12.6f} | {diff:>12.6f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"  - Pour x < 0: GELU ≠ 0, ReLU = 0\")\n",
        "print(\"  - Pour x > 0: GELU ≈ ReLU (très proche)\")\n",
        "print(\"  - Pour x = 0: GELU = 0, ReLU = 0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 Impact sur l'Entraînement\n",
        "\n",
        "Testons l'impact des deux activations sur un forward pass réel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Créer deux FFN identiques sauf pour l'activation\n",
        "d_model = 128\n",
        "d_ff = 512\n",
        "\n",
        "ffn_gelu = FeedForward(d_model, d_ff, dropout=0.0, activation=\"gelu\")\n",
        "ffn_relu = FeedForward(d_model, d_ff, dropout=0.0, activation=\"relu\")\n",
        "\n",
        "# Copier les poids pour une comparaison équitable\n",
        "ffn_relu.linear1.weight.data = ffn_gelu.linear1.weight.data.clone()\n",
        "ffn_relu.linear1.bias.data = ffn_gelu.linear1.bias.data.clone()\n",
        "ffn_relu.linear2.weight.data = ffn_gelu.linear2.weight.data.clone()\n",
        "ffn_relu.linear2.bias.data = ffn_gelu.linear2.bias.data.clone()\n",
        "\n",
        "ffn_gelu.eval()\n",
        "ffn_relu.eval()\n",
        "\n",
        "# Créer un input avec des valeurs positives et négatives\n",
        "x = torch.randn(1, 10, d_model)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    out_gelu = ffn_gelu(x)\n",
        "    out_relu = ffn_relu(x)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"IMPACT SUR LE FORWARD PASS\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nInput:\")\n",
        "print(f\"  Shape: {x.shape}\")\n",
        "print(f\"  Mean: {x.mean().item():.6f}\")\n",
        "print(f\"  Std: {x.std().item():.6f}\")\n",
        "print(f\"  % valeurs négatives: {(x < 0).float().mean().item() * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nOutput avec GELU:\")\n",
        "print(f\"  Mean: {out_gelu.mean().item():.6f}\")\n",
        "print(f\"  Std: {out_gelu.std().item():.6f}\")\n",
        "print(f\"  % valeurs négatives: {(out_gelu < 0).float().mean().item() * 100:.1f}%\")\n",
        "\n",
        "print(f\"\\nOutput avec ReLU:\")\n",
        "print(f\"  Mean: {out_relu.mean().item():.6f}\")\n",
        "print(f\"  Std: {out_relu.std().item():.6f}\")\n",
        "print(f\"  % valeurs négatives: {(out_relu < 0).float().mean().item() * 100:.1f}%\")\n",
        "\n",
        "diff = torch.abs(out_gelu - out_relu).mean().item()\n",
        "print(f\"\\nDifférence moyenne: {diff:.6f}\")\n",
        "print(f\"  (Les activations différentes produisent des sorties différentes)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Validation des Dimensions\n",
        "\n",
        "Vérifions que le FFN préserve bien les dimensions d'entrée.\n",
        "\n",
        "### 5.1 Shape Checks Systématiques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test avec différentes configurations\n",
        "configs = [\n",
        "    {\"d_model\": 128, \"d_ff\": 512, \"batch_size\": 2, \"seq_len\": 10},\n",
        "    {\"d_model\": 256, \"d_ff\": 1024, \"batch_size\": 4, \"seq_len\": 20},\n",
        "    {\"d_model\": 512, \"d_ff\": 2048, \"batch_size\": 8, \"seq_len\": 50},\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"VALIDATION DES DIMENSIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, config in enumerate(configs, 1):\n",
        "    print(f\"\\n[Test {i}] Configuration: d_model={config['d_model']}, d_ff={config['d_ff']}\")\n",
        "    \n",
        "    # Créer le module\n",
        "    ffn = FeedForward(config['d_model'], config['d_ff'], dropout=0.0)\n",
        "    ffn.eval()\n",
        "    \n",
        "    # Créer l'input\n",
        "    x = torch.randn(config['batch_size'], config['seq_len'], config['d_model'])\n",
        "    \n",
        "    # Forward pass\n",
        "    output = ffn(x)\n",
        "    \n",
        "    # Vérifications\n",
        "    assert output.shape == x.shape, f\"Shape mismatch: {output.shape} != {x.shape}\"\n",
        "    assert output.size(0) == config['batch_size'], \"Batch size mismatch\"\n",
        "    assert output.size(1) == config['seq_len'], \"Sequence length mismatch\"\n",
        "    assert output.size(2) == config['d_model'], \"Model dimension mismatch\"\n",
        "    \n",
        "    print(f\"  Input:  {x.shape}\")\n",
        "    print(f\"  Output: {output.shape}\")\n",
        "    print(f\"  ✓ Toutes les vérifications passées!\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"✓ Tous les tests de dimension sont passés avec succès!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Validation Complète avec la Fonction Utilitaire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Utiliser la fonction de validation complète\n",
        "validate_dimension_preservation(\n",
        "    d_model=256,\n",
        "    d_ff=1024,\n",
        "    batch_size=4,\n",
        "    seq_len=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Exercices Pratiques\n",
        "\n",
        "### Exercice 1: Expérimenter avec d_ff\n",
        "\n",
        "Testez différents ratios d_ff/d_model et observez l'impact sur le nombre de paramètres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Complétez ce code\n",
        "d_model = 256\n",
        "\n",
        "# Testez différents ratios\n",
        "ratios = [1, 2, 4, 8]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCICE 1: Impact du ratio d_ff/d_model\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n{'Ratio':>8} | {'d_ff':>8} | {'Paramètres':>15} | {'Ratio params':>15}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "base_params = None\n",
        "for ratio in ratios:\n",
        "    d_ff = d_model * ratio\n",
        "    \n",
        "    # TODO: Créez un FFN et comptez les paramètres\n",
        "    ffn = FeedForward(d_model, d_ff, dropout=0.0)\n",
        "    total_params = sum(p.numel() for p in ffn.parameters())\n",
        "    \n",
        "    if base_params is None:\n",
        "        base_params = total_params\n",
        "    \n",
        "    ratio_params = total_params / base_params\n",
        "    print(f\"{ratio:>8} | {d_ff:>8} | {total_params:>15,} | {ratio_params:>15.2f}x\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\\nObservation: Le nombre de paramètres augmente quadratiquement avec le ratio!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 2: Comparer les Temps de Calcul"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# TODO: Comparez les temps de calcul pour GELU vs ReLU\n",
        "d_model = 512\n",
        "d_ff = 2048\n",
        "batch_size = 32\n",
        "seq_len = 128\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# FFN avec GELU\n",
        "ffn_gelu = FeedForward(d_model, d_ff, dropout=0.0, activation=\"gelu\")\n",
        "ffn_gelu.eval()\n",
        "\n",
        "# FFN avec ReLU\n",
        "ffn_relu = FeedForward(d_model, d_ff, dropout=0.0, activation=\"relu\")\n",
        "ffn_relu.eval()\n",
        "\n",
        "# Mesurer le temps pour GELU\n",
        "n_iterations = 100\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_iterations):\n",
        "        _ = ffn_gelu(x)\n",
        "time_gelu = (time.time() - start) / n_iterations\n",
        "\n",
        "# Mesurer le temps pour ReLU\n",
        "start = time.time()\n",
        "with torch.no_grad():\n",
        "    for _ in range(n_iterations):\n",
        "        _ = ffn_relu(x)\n",
        "time_relu = (time.time() - start) / n_iterations\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCICE 2: Comparaison des temps de calcul\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  - d_model: {d_model}\")\n",
        "print(f\"  - d_ff: {d_ff}\")\n",
        "print(f\"  - batch_size: {batch_size}\")\n",
        "print(f\"  - seq_len: {seq_len}\")\n",
        "print(f\"\\nRésultats (moyenne sur {n_iterations} itérations):\")\n",
        "print(f\"  - GELU: {time_gelu*1000:.3f} ms\")\n",
        "print(f\"  - ReLU: {time_relu*1000:.3f} ms\")\n",
        "print(f\"  - Ratio: {time_gelu/time_relu:.2f}x\")\n",
        "print(f\"\\nConclusion: {'GELU est plus lent' if time_gelu > time_relu else 'ReLU est plus lent'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 3: Visualiser l'Impact du Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Visualisez l'effet du dropout sur les activations\n",
        "d_model = 128\n",
        "d_ff = 512\n",
        "dropout_rates = [0.0, 0.1, 0.3, 0.5]\n",
        "\n",
        "x = torch.randn(1, 10, d_model)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"EXERCICE 3: Impact du dropout\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for dropout in dropout_rates:\n",
        "    ffn = FeedForward(d_model, d_ff, dropout=dropout)\n",
        "    ffn.train()  # Mode entraînement pour activer le dropout\n",
        "    \n",
        "    # Faire plusieurs forward passes\n",
        "    outputs = []\n",
        "    for _ in range(10):\n",
        "        with torch.no_grad():\n",
        "            out = ffn(x)\n",
        "            outputs.append(out)\n",
        "    \n",
        "    # Calculer la variance entre les passes\n",
        "    outputs_stacked = torch.stack(outputs)\n",
        "    variance = outputs_stacked.var(dim=0).mean().item()\n",
        "    \n",
        "    print(f\"\\nDropout = {dropout:.1f}:\")\n",
        "    print(f\"  Variance entre passes: {variance:.6f}\")\n",
        "    print(f\"  (Plus de dropout = plus de variance)\")\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"Observation: Le dropout augmente la variance entre les forward passes!\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Résumé et Points Clés\n",
        "\n",
        "### Ce que nous avons appris\n",
        "\n",
        "1. **Architecture du FFN:**\n",
        "   - Deux transformations linéaires avec activation non-linéaire\n",
        "   - Expansion puis projection (d_model → d_ff → d_model)\n",
        "   - Préservation des dimensions d'entrée\n",
        "\n",
        "2. **Implémentation from scratch:**\n",
        "   - Multiplication matricielle explicite avec NumPy\n",
        "   - Calcul manuel de GELU et ReLU\n",
        "   - Compréhension des transformations de dimensions\n",
        "\n",
        "3. **Implémentation PyTorch:**\n",
        "   - Utilisation de nn.Linear, nn.GELU, nn.Dropout\n",
        "   - Gestion automatique des gradients\n",
        "   - Mode train vs eval pour le dropout\n",
        "\n",
        "4. **GELU vs ReLU:**\n",
        "   - GELU est plus lisse (pas de discontinuité)\n",
        "   - GELU permet un gradient non-nul pour x < 0\n",
        "   - GELU est utilisé dans BERT et GPT\n",
        "   - ReLU est plus simple mais peut \"tuer\" des neurones\n",
        "\n",
        "5. **Validation:**\n",
        "   - Shape checks à chaque étape\n",
        "   - Vérification de la préservation des dimensions\n",
        "   - Tests avec différentes configurations\n",
        "\n",
        "### Rôle dans le Transformer\n",
        "\n",
        "Le FFN est appliqué après l'attention dans chaque TransformerBlock:\n",
        "\n",
        "```\n",
        "x → Multi-Head Attention → Add & Norm → FFN → Add & Norm → output\n",
        "```\n",
        "\n",
        "**Pourquoi le FFN est important:**\n",
        "- Ajoute de la capacité de transformation non-linéaire\n",
        "- Permet au modèle d'apprendre des représentations complexes\n",
        "- Appliqué indépendamment à chaque position (\"position-wise\")\n",
        "- Représente ~2/3 des paramètres d'un TransformerBlock\n",
        "\n",
        "### Prochaines Étapes\n",
        "\n",
        "Dans le prochain notebook, nous allons:\n",
        "1. Assembler le TransformerBlock complet\n",
        "2. Combiner Multi-Head Attention + FFN\n",
        "3. Ajouter LayerNorm et connexions résiduelles\n",
        "4. Créer un bloc transformer fonctionnel\n",
        "\n",
        "**Continuez vers:** `02_transformer_block.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Références\n",
        "\n",
        "### Articles Scientifiques\n",
        "\n",
        "1. **Attention Is All You Need** (Vaswani et al., 2017)\n",
        "   - Paper original des Transformers\n",
        "   - Décrit l'architecture FFN\n",
        "\n",
        "2. **BERT: Pre-training of Deep Bidirectional Transformers** (Devlin et al., 2018)\n",
        "   - Utilise GELU dans le FFN\n",
        "   - Démontre l'efficacité de GELU\n",
        "\n",
        "3. **Gaussian Error Linear Units (GELUs)** (Hendrycks & Gimpel, 2016)\n",
        "   - Introduction de l'activation GELU\n",
        "   - Analyse théorique et empirique\n",
        "\n",
        "### Ressources Additionnelles\n",
        "\n",
        "- PyTorch Documentation: https://pytorch.org/docs/stable/nn.html\n",
        "- The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\n",
        "- Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}

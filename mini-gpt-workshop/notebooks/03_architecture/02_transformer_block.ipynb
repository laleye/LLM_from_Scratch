{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TransformerBlock: Assembler Tous les Composants\n\n## Introduction\n\nBienvenue dans ce notebook sur le **TransformerBlock**, l'unit\u00e9 de base des architectures transformer!\n\n### Objectifs p\u00e9dagogiques\n\nDans ce notebook, vous allez:\n1. Comprendre l'architecture compl\u00e8te d'un TransformerBlock\n2. Assembler Multi-Head Attention, LayerNorm, et Feed-Forward\n3. Impl\u00e9menter les connexions r\u00e9siduelles (residual connections)\n4. Comprendre le r\u00f4le de LayerNorm et Dropout\n5. Valider la pr\u00e9servation des dimensions\n6. Comparer BERT (bidirectionnel) vs GPT (causal)\n7. Empiler plusieurs blocs pour cr\u00e9er un transformer profond\n\n### Qu'est-ce qu'un TransformerBlock?\n\nUn TransformerBlock est l'unit\u00e9 de base qui est r\u00e9p\u00e9t\u00e9e N fois dans un transformer.\n\n**Composants:**\n- **Multi-Head Attention:** Permet aux tokens de s'attendre mutuellement\n- **Layer Normalization:** Stabilise l'entra\u00eenement\n- **Feed-Forward Network:** Transformations non-lin\u00e9aires\n- **Residual Connections:** Permettent au gradient de circuler\n- **Dropout:** R\u00e9gularisation\n\n**Architecture:**\n```\nInput\n  \u2193\nMulti-Head Attention\n  \u2193\nAdd & Norm (residual + LayerNorm)\n  \u2193\nFeed-Forward Network\n  \u2193\nAdd & Norm (residual + LayerNorm)\n  \u2193\nOutput\n```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Formules Math\u00e9matiques du TransformerBlock\n\n### Architecture Compl\u00e8te\n\nUn TransformerBlock applique deux sous-couches avec connexions r\u00e9siduelles:\n\n#### Sous-Couche 1: Multi-Head Attention\n\n$x' = \\text{LayerNorm}(x + \\text{Dropout}(\\text{MultiHeadAttention}(x)))$\n\n#### Sous-Couche 2: Feed-Forward Network\n\n$\\text{output} = \\text{LayerNorm}(x' + \\text{Dropout}(\\text{FFN}(x')))$\n\n### Connexions R\u00e9siduelles (Residual Connections)\n\nLes connexions r\u00e9siduelles permettent au gradient de circuler directement:\n\n$\\text{output} = x + F(x)$\n\nO\u00f9 $F(x)$ est une transformation (attention ou FFN).\n\n**Pourquoi les residual connections?**\n- Permettent d'entra\u00eener des r\u00e9seaux tr\u00e8s profonds (>100 couches)\n- Le gradient peut circuler directement: $\\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial \\text{output}} + \\frac{\\partial L}{\\partial F(x)}$\n- Si $F(x)$ n'apporte rien, le r\u00e9seau peut l'ignorer (poids \u2192 0)\n- Introduites dans ResNet, adopt\u00e9es par tous les transformers\n\n### Layer Normalization\n\n$\\text{LayerNorm}(x) = \\gamma \\odot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta$\n\nO\u00f9:\n- $\\mu = \\frac{1}{d} \\sum_{i=1}^{d} x_i$ : Moyenne sur la dimension des features\n- $\\sigma^2 = \\frac{1}{d} \\sum_{i=1}^{d} (x_i - \\mu)^2$ : Variance sur la dimension des features\n- $\\gamma, \\beta$ : Param\u00e8tres apprenables (scale et shift)\n- $\\epsilon$ : Petit nombre pour \u00e9viter division par z\u00e9ro (1e-5)\n\n**Diff\u00e9rence avec BatchNorm:**\n- **BatchNorm:** Normalise sur le batch (dimension 0)\n- **LayerNorm:** Normalise sur les features (dimension -1)\n- **LayerNorm** fonctionne mieux pour les s\u00e9quences et petits batches\n\n**Pourquoi LayerNorm?**\n- Stabilise l'entra\u00eenement en r\u00e9duisant le \"covariate shift\"\n- Permet d'utiliser des learning rates plus \u00e9lev\u00e9s\n- R\u00e9duit la sensibilit\u00e9 \u00e0 l'initialisation\n- Acc\u00e9l\u00e8re la convergence\n\n### Dropout\n\nPendant l'entra\u00eenement, chaque neurone a une probabilit\u00e9 $p$ d'\u00eatre d\u00e9sactiv\u00e9:\n\n$\\text{Dropout}(x)_i = \\begin{cases} \n0 & \\text{avec probabilit\u00e9 } p \\\\\n\\frac{x_i}{1-p} & \\text{avec probabilit\u00e9 } 1-p\n\\end{cases}$\n\nPendant l'\u00e9valuation: $\\text{Dropout}(x) = x$ (pas de dropout)\n\n**Pourquoi le Dropout?**\n- R\u00e9gularisation: Pr\u00e9vient le surapprentissage\n- Force la redondance: Le r\u00e9seau ne peut pas d\u00e9pendre d'un seul neurone\n- Effet d'ensemble: Comme entra\u00eener plusieurs r\u00e9seaux en parall\u00e8le\n\n### Dimensions\n\n**Propri\u00e9t\u00e9 importante:** Le TransformerBlock pr\u00e9serve les dimensions!\n\n- **Input:** $(\\text{batch}, \\text{seq\\_len}, d_{\\text{model}})$\n- **Output:** $(\\text{batch}, \\text{seq\\_len}, d_{\\text{model}})$\n\nCette propri\u00e9t\u00e9 permet d'empiler plusieurs blocs s\u00e9quentiellement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Imports\nimport numpy as np\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nfrom typing import Tuple, Optional\n\n# Ajouter le chemin vers src/\nsys.path.append('../..')\n\n# Importer nos modules\nfrom src.architecture.transformer_block import (\n    TransformerBlock,\n    validate_transformer_block,\n    example_transformer_block,\n    example_stacked_transformer_blocks\n)\nfrom src.architecture.feed_forward import FeedForward\nfrom src.attention.multi_head import MultiHeadAttention\nfrom src.attention.masking import create_causal_mask\n\n# Configuration pour les visualisations\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n# Seed pour la reproductibilit\u00e9\nnp.random.seed(42)\ntorch.manual_seed(42)\n\nprint(\"\u2713 Imports r\u00e9ussis!\")\nprint(f\"\u2713 PyTorch version: {torch.__version__}\")\nprint(f\"\u2713 Device disponible: {'GPU' if torch.cuda.is_available() else 'CPU'}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Diagramme d'Architecture\n\n### Visualisation du TransformerBlock\n\nVoici un diagramme d\u00e9taill\u00e9 montrant le flux de donn\u00e9es \u00e0 travers un TransformerBlock:\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        Input (x)                             \u2502\n\u2502                  (batch, seq_len, d_model)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502                  \u2502\n                           \u25bc                  \u2502 (residual)\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Multi-Head         \u2502      \u2502\n                  \u2502 Attention          \u2502      \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                           \u2502                  \u2502\n                           \u25bc                  \u2502\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Dropout            \u2502      \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                           \u2502                  \u2502\n                           \u25bc                  \u2502\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Add (x + attn)     \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 LayerNorm          \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502 (x')\n                           \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502                  \u2502\n                           \u25bc                  \u2502 (residual)\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Feed-Forward       \u2502      \u2502\n                  \u2502 Network            \u2502      \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                           \u2502                  \u2502\n                           \u25bc                  \u2502\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Dropout            \u2502      \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n                           \u2502                  \u2502\n                           \u25bc                  \u2502\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n                  \u2502 Add (x' + ffn)     \u2502\u25c4\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                  \u2502 LayerNorm          \u2502\n                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                           \u2502\n                           \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       Output                                 \u2502\n\u2502                  (batch, seq_len, d_model)                   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Points cl\u00e9s:**\n1. **Deux sous-couches:** Attention + FFN\n2. **Deux residual connections:** Une pour chaque sous-couche\n3. **Deux LayerNorm:** Une apr\u00e8s chaque residual connection\n4. **Dropout:** Appliqu\u00e9 apr\u00e8s attention et FFN\n5. **Dimension pr\u00e9serv\u00e9e:** Input et output ont la m\u00eame shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cr\u00e9ation d'un TransformerBlock\n\n### 3.1 Instanciation du Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Param\u00e8tres\nd_model = 256\nnum_heads = 8\nd_ff = 1024  # 4 \u00d7 d_model\ndropout = 0.1\n\n# Cr\u00e9er le TransformerBlock\nblock = TransformerBlock(\n    d_model=d_model,\n    num_heads=num_heads,\n    d_ff=d_ff,\n    dropout=dropout\n)\n\nprint(\"=\" * 70)\nprint(\"TRANSFORMERBLOCK CR\u00c9\u00c9\")\nprint(\"=\" * 70)\nprint(f\"\\nConfiguration:\")\nprint(f\"  - d_model: {d_model}\")\nprint(f\"  - num_heads: {num_heads}\")\nprint(f\"  - d_ff: {d_ff}\")\nprint(f\"  - dropout: {dropout}\")\n\nprint(f\"\\nArchitecture:\")\nprint(block)\n\n# Compter les param\u00e8tres\ntotal_params = sum(p.numel() for p in block.parameters())\nprint(f\"\\nNombre total de param\u00e8tres: {total_params:,}\")\n\n# D\u00e9tail par composant\nprint(f\"\\nD\u00e9tail des param\u00e8tres:\")\nfor name, param in block.named_parameters():\n    print(f\"  {name}: {param.shape} ({param.numel():,} params)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Forward Pass Sans Masque (BERT-style)\n\nTestons le bloc avec attention bidirectionnelle (pas de masque)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cr\u00e9er un input de test\nbatch_size = 2\nseq_len = 10\nx = torch.randn(batch_size, seq_len, d_model)\n\nprint(\"=\" * 70)\nprint(\"FORWARD PASS SANS MASQUE (BERT-style)\")\nprint(\"=\" * 70)\nprint(f\"\\nInput shape: {x.shape}\")\nprint(f\"  (batch_size={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n\n# Mode \u00e9valuation (d\u00e9sactive dropout)\nblock.eval()\n\n# Forward pass\nwith torch.no_grad():\n    output = block(x, mask=None)\n\nprint(f\"\\nOutput shape: {output.shape}\")\nprint(f\"  (batch_size={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n\nprint(f\"\\n\u2713 Dimensions pr\u00e9serv\u00e9es: {x.shape == output.shape}\")\n\n# Statistiques\nprint(f\"\\nStatistiques:\")\nprint(f\"  Input  - Mean: {x.mean().item():.6f}, Std: {x.std().item():.6f}\")\nprint(f\"  Output - Mean: {output.mean().item():.6f}, Std: {output.std().item():.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Forward Pass Avec Masque Causal (GPT-style)\n\nTestons maintenant avec un masque causal pour l'attention autoregressive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\nprint(\"FORWARD PASS AVEC MASQUE CAUSAL (GPT-style)\")\nprint(\"=\" * 70)\n\n# Cr\u00e9er un masque causal\ncausal_mask = create_causal_mask(seq_len, x.device)\n\nprint(f\"\\nMasque causal ({seq_len}\u00d7{seq_len}):\")\nprint(causal_mask)\nprint(f\"\\nInterpr\u00e9tation:\")\nprint(f\"  - 1: Attention autoris\u00e9e\")\nprint(f\"  - 0: Attention bloqu\u00e9e (positions futures)\")\n\n# Forward pass avec masque\nwith torch.no_grad():\n    output_masked = block(x, mask=causal_mask)\n\nprint(f\"\\nOutput shape: {output_masked.shape}\")\nprint(f\"\u2713 Dimensions pr\u00e9serv\u00e9es: {x.shape == output_masked.shape}\")\n\n# Comparer avec et sans masque\ndiff = torch.abs(output - output_masked).mean().item()\nprint(f\"\\nDiff\u00e9rence moyenne entre BERT et GPT: {diff:.6f}\")\nprint(f\"  (Les sorties sont diff\u00e9rentes car le masque change l'attention)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Validation des Dimensions\n\n### 4.1 Shape Checks D\u00e9taill\u00e9s\n\nV\u00e9rifions que chaque \u00e9tape du forward pass pr\u00e9serve les dimensions correctement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"=\" * 70)\nprint(\"SHAPE CHECKS D\u00c9TAILL\u00c9S\")\nprint(\"=\" * 70)\n\n# Cr\u00e9er un input\nx_test = torch.randn(2, 5, d_model)\nprint(f\"\\nInput shape: {x_test.shape}\")\n\n# Acc\u00e9der aux composants internes\nblock.eval()\n\n# \u00c9tape 1: Multi-Head Attention\nwith torch.no_grad():\n    attn_output = block.attention(x_test, mask=None)\nprint(f\"\\nApr\u00e8s Multi-Head Attention: {attn_output.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\n# \u00c9tape 2: Dropout\nwith torch.no_grad():\n    attn_dropped = block.dropout(attn_output)\nprint(f\"Apr\u00e8s Dropout: {attn_dropped.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\n# \u00c9tape 3: Residual + LayerNorm\nwith torch.no_grad():\n    x_norm1 = block.norm1(x_test + attn_dropped)\nprint(f\"Apr\u00e8s Add & Norm 1: {x_norm1.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\n# \u00c9tape 4: Feed-Forward\nwith torch.no_grad():\n    ff_output = block.feed_forward(x_norm1)\nprint(f\"\\nApr\u00e8s Feed-Forward: {ff_output.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\n# \u00c9tape 5: Dropout\nwith torch.no_grad():\n    ff_dropped = block.dropout(ff_output)\nprint(f\"Apr\u00e8s Dropout: {ff_dropped.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\n# \u00c9tape 6: Residual + LayerNorm\nwith torch.no_grad():\n    final_output = block.norm2(x_norm1 + ff_dropped)\nprint(f\"Apr\u00e8s Add & Norm 2: {final_output.shape}\")\nprint(f\"  \u2713 Shape pr\u00e9serv\u00e9e\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"\u2713 Toutes les \u00e9tapes pr\u00e9servent les dimensions!\")\nprint(f\"{'='*70}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Validation Compl\u00e8te\n\nUtilisons la fonction de validation pour tester diff\u00e9rentes configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Valider avec diff\u00e9rentes configurations\nvalidate_transformer_block(\n    d_model=256,\n    num_heads=8,\n    d_ff=1024,\n    batch_size=4,\n    seq_len=16,\n    dropout=0.1\n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Comparaison BERT vs GPT\n\n### 5.1 Diff\u00e9rence Fondamentale: Le Masque\n\nLa principale diff\u00e9rence entre BERT et GPT est l'utilisation du masque causal:\n\n**BERT (Bidirectionnel):**\n- Pas de masque\n- Chaque token peut voir tous les autres tokens\n- Utilis\u00e9 pour des t\u00e2ches de compr\u00e9hension (classification, NER, QA)\n\n**GPT (Autor\u00e9gressif):**\n- Masque causal\n- Chaque token ne peut voir que les tokens pr\u00e9c\u00e9dents\n- Utilis\u00e9 pour la g\u00e9n\u00e9ration de texte\n\n### 5.2 D\u00e9monstration Visuelle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Cr\u00e9er un exemple simple\nseq_len_demo = 5\nx_demo = torch.randn(1, seq_len_demo, 128)\n\n# Cr\u00e9er deux blocs identiques\nblock_bert = TransformerBlock(d_model=128, num_heads=4, d_ff=512, dropout=0.0)\nblock_gpt = TransformerBlock(d_model=128, num_heads=4, d_ff=512, dropout=0.0)\n\n# Copier les poids pour une comparaison \u00e9quitable\nblock_gpt.load_state_dict(block_bert.state_dict())\n\nblock_bert.eval()\nblock_gpt.eval()\n\nprint(\"=\" * 70)\nprint(\"BERT vs GPT: Comparaison\")\nprint(\"=\" * 70)\n\n# BERT: Sans masque\nwith torch.no_grad():\n    output_bert = block_bert(x_demo, mask=None)\n\nprint(f\"\\nBERT (bidirectionnel):\")\nprint(f\"  - Masque: Aucun\")\nprint(f\"  - Chaque token voit: Tous les tokens (0-{seq_len_demo-1})\")\nprint(f\"  - Output shape: {output_bert.shape}\")\n\n# GPT: Avec masque causal\ncausal_mask_demo = create_causal_mask(seq_len_demo, x_demo.device)\nwith torch.no_grad():\n    output_gpt = block_gpt(x_demo, mask=causal_mask_demo)\n\nprint(f\"\\nGPT (autor\u00e9gressif):\")\nprint(f\"  - Masque: Causal (triangulaire)\")\nprint(f\"  - Token i voit: Tokens 0 \u00e0 i seulement\")\nprint(f\"  - Output shape: {output_gpt.shape}\")\n\n# Analyser les diff\u00e9rences\nprint(f\"\\nAnalyse des diff\u00e9rences:\")\nfor pos in range(seq_len_demo):\n    diff_pos = torch.abs(output_bert[0, pos] - output_gpt[0, pos]).mean().item()\n    print(f\"  Position {pos}: diff\u00e9rence moyenne = {diff_pos:.6f}\")\n\nprint(f\"\\nObservation:\")\nprint(f\"  - Les premi\u00e8res positions sont similaires\")\nprint(f\"  - Les diff\u00e9rences augmentent vers la fin\")\nprint(f\"  - Le masque causal change progressivement le comportement\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Empiler Plusieurs TransformerBlocks\n\n### 6.1 Cr\u00e9er un Stack de Blocs\n\nUn transformer complet est compos\u00e9 de plusieurs TransformerBlocks empil\u00e9s."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Configuration\nd_model = 128\nnum_heads = 4\nd_ff = 512\nnum_layers = 3  # 3 blocs empil\u00e9s\n\nprint(\"=\" * 70)\nprint(f\"STACK DE {num_layers} TRANSFORMERBLOCKS\")\nprint(\"=\" * 70)\n\n# Cr\u00e9er un stack avec nn.ModuleList\nblocks = nn.ModuleList([\n    TransformerBlock(d_model, num_heads, d_ff, dropout=0.1)\n    for _ in range(num_layers)\n])\n\n# Compter les param\u00e8tres\ntotal_params = sum(p.numel() for p in blocks.parameters())\nparams_per_block = total_params // num_layers\n\nprint(f\"\\nConfiguration:\")\nprint(f\"  - Nombre de blocs: {num_layers}\")\nprint(f\"  - d_model: {d_model}\")\nprint(f\"  - num_heads: {num_heads}\")\nprint(f\"  - d_ff: {d_ff}\")\n\nprint(f\"\\nParam\u00e8tres:\")\nprint(f\"  - Total: {total_params:,}\")\nprint(f\"  - Par bloc: {params_per_block:,}\")\n\n# Cr\u00e9er un input\nx = torch.randn(2, 8, d_model)\ncausal_mask = create_causal_mask(8, x.device)\n\nprint(f\"\\nForward pass \u00e0 travers le stack:\")\nprint(f\"  Input shape: {x.shape}\")\n\n# Passer \u00e0 travers tous les blocs\nfor i, block in enumerate(blocks):\n    block.eval()\n    with torch.no_grad():\n        x = block(x, mask=causal_mask)\n    print(f\"  Apr\u00e8s bloc {i+1}: {x.shape}\")\n\nprint(f\"\\n  Output final: {x.shape}\")\nprint(f\"  \u2713 Dimension pr\u00e9serv\u00e9e \u00e0 travers tous les blocs!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Exemple Complet avec Fonction Utilitaire"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Utiliser la fonction d'exemple\nexample_stacked_transformer_blocks()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Exercices Pratiques\n\n### Exercice 1: Exp\u00e9rimenter avec le Nombre de T\u00eates\n\nTestez diff\u00e9rents nombres de t\u00eates et observez l'impact sur les param\u00e8tres."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Compl\u00e9tez ce code\nd_model = 256\nd_ff = 1024\n\n# Testez diff\u00e9rents nombres de t\u00eates\nnum_heads_list = [1, 2, 4, 8, 16]\n\nprint(\"=\" * 70)\nprint(\"EXERCICE 1: Impact du nombre de t\u00eates\")\nprint(\"=\" * 70)\nprint(f\"\\n{'T\u00eates':>8} | {'Param\u00e8tres':>15} | {'Ratio':>10}\")\nprint(\"=\" * 70)\n\nbase_params = None\nfor num_heads in num_heads_list:\n    # TODO: Cr\u00e9ez un TransformerBlock et comptez les param\u00e8tres\n    block = TransformerBlock(d_model, num_heads, d_ff, dropout=0.0)\n    total_params = sum(p.numel() for p in block.parameters())\n    \n    if base_params is None:\n        base_params = total_params\n    \n    ratio = total_params / base_params\n    print(f\"{num_heads:>8} | {total_params:>15,} | {ratio:>10.2f}x\")\n\nprint(\"=\" * 70)\nprint(\"\\nObservation: Le nombre de param\u00e8tres ne change pas beaucoup!\")\nprint(\"Pourquoi? Les t\u00eates partagent les m\u00eames dimensions totales.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 2: Mesurer l'Impact du Dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Mesurez la variance des sorties avec diff\u00e9rents taux de dropout\nd_model = 128\nd_ff = 512\ndropout_rates = [0.0, 0.1, 0.3, 0.5]\n\nx = torch.randn(1, 10, d_model)\n\nprint(\"=\" * 70)\nprint(\"EXERCICE 2: Impact du dropout\")\nprint(\"=\" * 70)\n\nfor dropout in dropout_rates:\n    block = TransformerBlock(d_model, num_heads=4, d_ff=d_ff, dropout=dropout)\n    block.train()  # Mode entra\u00eenement pour activer le dropout\n    \n    # Faire plusieurs forward passes\n    outputs = []\n    for _ in range(20):\n        with torch.no_grad():\n            out = block(x, mask=None)\n            outputs.append(out)\n    \n    # Calculer la variance\n    outputs_stacked = torch.stack(outputs)\n    variance = outputs_stacked.var(dim=0).mean().item()\n    \n    print(f\"\\nDropout = {dropout:.1f}:\")\n    print(f\"  Variance entre passes: {variance:.6f}\")\n\nprint(f\"\\n{'='*70}\")\nprint(\"Observation: Plus de dropout = plus de variance!\")\nprint(f\"{'='*70}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercice 3: Visualiser l'Effet des Residual Connections\n\nComparez un bloc avec et sans residual connections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# TODO: Cr\u00e9ez une version sans residual connections et comparez\nclass TransformerBlockNoResidual(nn.Module):\n    \"\"\"TransformerBlock SANS residual connections (pour comparaison).\"\"\"\n    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n        super().__init__()\n        self.attention = MultiHeadAttention(d_model, num_heads, dropout=dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.feed_forward = FeedForward(d_model, d_ff, dropout=dropout)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x, mask=None):\n        # SANS residual connections\n        x = self.norm1(self.dropout(self.attention(x, mask)))\n        x = self.norm2(self.dropout(self.feed_forward(x)))\n        return x\n\n# Comparer\nd_model = 128\nx = torch.randn(1, 10, d_model)\n\nblock_with_residual = TransformerBlock(d_model, 4, 512, dropout=0.0)\nblock_no_residual = TransformerBlockNoResidual(d_model, 4, 512, dropout=0.0)\n\nblock_with_residual.eval()\nblock_no_residual.eval()\n\nwith torch.no_grad():\n    out_with = block_with_residual(x, mask=None)\n    out_without = block_no_residual(x, mask=None)\n\nprint(\"=\" * 70)\nprint(\"EXERCICE 3: Impact des Residual Connections\")\nprint(\"=\" * 70)\nprint(f\"\\nAvec residual connections:\")\nprint(f\"  Output mean: {out_with.mean().item():.6f}\")\nprint(f\"  Output std: {out_with.std().item():.6f}\")\n\nprint(f\"\\nSans residual connections:\")\nprint(f\"  Output mean: {out_without.mean().item():.6f}\")\nprint(f\"  Output std: {out_without.std().item():.6f}\")\n\nprint(f\"\\nObservation:\")\nprint(f\"  - Les residual connections stabilisent les activations\")\nprint(f\"  - Sans elles, les valeurs peuvent exploser ou dispara\u00eetre\")\nprint(f\"  - Essentielles pour entra\u00eener des r\u00e9seaux profonds!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. R\u00e9sum\u00e9 et Points Cl\u00e9s\n\n### Ce que nous avons appris\n\n1. **Architecture du TransformerBlock:**\n   - Deux sous-couches: Multi-Head Attention + Feed-Forward\n   - Residual connections apr\u00e8s chaque sous-couche\n   - LayerNorm apr\u00e8s chaque residual connection\n   - Dropout pour la r\u00e9gularisation\n\n2. **Connexions R\u00e9siduelles:**\n   - Permettent au gradient de circuler directement\n   - Essentielles pour entra\u00eener des r\u00e9seaux profonds\n   - Formule: output = x + F(x)\n\n3. **Layer Normalization:**\n   - Normalise sur la dimension des features\n   - Stabilise l'entra\u00eenement\n   - Diff\u00e9rent de BatchNorm\n\n4. **BERT vs GPT:**\n   - BERT: Pas de masque (bidirectionnel)\n   - GPT: Masque causal (autor\u00e9gressif)\n   - M\u00eame architecture, usage diff\u00e9rent\n\n5. **Empiler des Blocs:**\n   - Un transformer = Stack de N TransformerBlocks\n   - Dimension pr\u00e9serv\u00e9e \u00e0 chaque \u00e9tape\n   - Plus de blocs = Plus de capacit\u00e9\n\n### R\u00f4le dans le Transformer Complet\n\n```\nInput Embeddings\n      \u2193\nTransformerBlock 1\n      \u2193\nTransformerBlock 2\n      \u2193\n      ...\n      \u2193\nTransformerBlock N\n      \u2193\nOutput Projection\n```\n\n### Prochaines \u00c9tapes\n\nDans le prochain notebook, nous allons:\n1. Assembler le mod\u00e8le Mini-GPT complet\n2. Ajouter les embeddings d'entr\u00e9e\n3. Ajouter la projection de sortie\n4. Cr\u00e9er un mod\u00e8le de g\u00e9n\u00e9ration de texte fonctionnel\n\n**Continuez vers:** `03_complete_mini_gpt.ipynb`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. R\u00e9f\u00e9rences\n\n### Articles Scientifiques\n\n1. **Attention Is All You Need** (Vaswani et al., 2017)\n   - Paper original des Transformers\n   - D\u00e9crit l'architecture TransformerBlock\n\n2. **Deep Residual Learning for Image Recognition** (He et al., 2015)\n   - Introduction des residual connections\n   - Inspiration pour les transformers\n\n3. **Layer Normalization** (Ba et al., 2016)\n   - Introduction de LayerNorm\n   - Analyse th\u00e9orique et empirique\n\n4. **BERT: Pre-training of Deep Bidirectional Transformers** (Devlin et al., 2018)\n   - Utilisation bidirectionnelle des transformers\n\n5. **Language Models are Unsupervised Multitask Learners** (Radford et al., 2019)\n   - GPT-2: Utilisation autoregressive des transformers\n\n### Ressources Additionnelles\n\n- PyTorch Documentation: https://pytorch.org/docs/stable/nn.html\n- The Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/\n- Annotated Transformer: http://nlp.seas.harvard.edu/annotated-transformer/\n- Understanding LSTM Networks: http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
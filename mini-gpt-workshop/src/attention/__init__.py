"""Attention module - Scaled dot-product and multi-head attention."""
